{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022671ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time as time\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93447a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #visualisation\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce85991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ad1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418eaec",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9ac27c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multicore Loading Time = 1.5009870529174805\n",
      "34179\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "labelled = pd.read_csv('data/mature_labelled.csv', index_col=0, dtype='string')\n",
    "e = time.time()\n",
    "print(\"Multicore Loading Time = {}\".format(e-s))\n",
    "\n",
    "print(len(labelled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43db245a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article_date</th>\n",
       "      <th>pubmed_date</th>\n",
       "      <th>article_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_short</th>\n",
       "      <th>journal_country</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_affils</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>references_pmids</th>\n",
       "      <th>feature</th>\n",
       "      <th>include</th>\n",
       "      <th>mature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pmid, doi, title, abstract, article_date, pubmed_date, article_type, lang, journal, journal_short, journal_country, authors, author_affils, keywords, mesh_terms, references_pmids, feature, include, mature]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled[labelled.isnull().all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e936edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34179"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = labelled[['pmid', 'feature']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5e8c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34179 entries, 1 to 172538\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   pmid    34179 non-null  string\n",
      " 1   text    34179 non-null  string\n",
      "dtypes: string(2)\n",
      "memory usage: 801.1+ KB\n"
     ]
    }
   ],
   "source": [
    "selected = selected.rename(columns={\"feature\":\"text\"})\n",
    "selected.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462c141",
   "metadata": {},
   "source": [
    "## Pre-process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53d5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = selected.fillna(\"\") #handle NaN values to allow regex over all cells\n",
    "\n",
    "groups_1 = groups.applymap(lambda x:x.lower() if type(x) == str else x) #reduce all to lowercase\n",
    "\n",
    "groups_2 = groups_1.replace(r\"[\\([{})\\]]\", \"\", regex=True) #remove brackets\n",
    "\n",
    "groups_3 = groups_2.replace(\"' \", \"'\", regex=True) #remove quote+space in front of word\n",
    "\n",
    "groups_4 = groups_3.replace(\"\"\"[\\.'!?]\"\"\", \"\", regex=True) #remove punctuation\n",
    "\n",
    "groups = groups_4.replace('\"', \"\", regex=True) #remove double quote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca6231",
   "metadata": {},
   "source": [
    "## Tag Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5552fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = groups[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9106efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## CLASSES\n",
    "######################\n",
    "# NEURAL NETWORK / nn\n",
    "# SUPPORT VECTOR MACHINE / svm\n",
    "# STANDARD REGRESSIONS /reg\n",
    "# DECISION TREES / dt\n",
    "# DISCRIMINANT ANALYSIS / da\n",
    "# NAIVE BAYES / nb\n",
    "# K-NEAREST NEIGHBOUR / knn\n",
    "# \n",
    "# TRANSFER LEARNING / tl\n",
    "# FEDERATED LEARNING / fl\n",
    "# UNSUPERVISED LEARNING / unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0681097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 20276, '1': 13903})\n"
     ]
    }
   ],
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "## text\n",
    "text = ['neural net', 'deep learning', 'convolutional', 'back propagation', 'lstm', ' cnn']\n",
    "\n",
    "algo['nn_text'] = np.where(groups['text'].str.contains(\"neural net\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['nn_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['nn_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['nn_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6901da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 29687, '1': 4492})\n"
     ]
    }
   ],
   "source": [
    "## SUPPORT VECTOR MACHINE\n",
    "\n",
    "## text\n",
    "text = ['vector machine', 'support vector', 'svm', 'vector regression']\n",
    "\n",
    "algo['svm_text'] = np.where(groups['text'].str.contains(\"support vector machine\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['svm_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['svm_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['svm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a5802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32171, '1': 2008})\n"
     ]
    }
   ],
   "source": [
    "## MULTIVARIABLE REGRESSION\n",
    "\n",
    "## text\n",
    "text = ['logistic regression', 'linear regression', 'multivariable regression', 'multivariate regression',\n",
    "       'simple regression', 'univariate logistic', 'multivariate linear', 'multivariable linear', 'linear model', 'logistic model',\n",
    "        'glm', 'regularized regression', 'ridge regression', 'sparse regression', 'stepwise regression', 'kernel regression',\n",
    "       'process regression']\n",
    "\n",
    "algo['reg_text'] = np.where(groups['text'].str.contains(\"univariate regression\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['reg_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['reg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['reg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e628feb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31365, '1': 2814})\n"
     ]
    }
   ],
   "source": [
    "## DECISION TREE\n",
    "\n",
    "## text\n",
    "text = ['regression tree', 'random forest', 'ensemble tree', 'adaboost', 'xgboost', 'gradient boost']\n",
    "\n",
    "algo['dt_text'] = np.where(groups['text'].str.contains(\"decision tree\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['dt_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['dt_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['dt_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f515e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33575, '1': 604})\n"
     ]
    }
   ],
   "source": [
    "## DISCRIMINANT ANALYSIS\n",
    "\n",
    "## text\n",
    "text = ['discriminant analysis', 'linear discriminant', 'linear discrimination']\n",
    "\n",
    "algo['da_text'] = np.where(groups['text'].str.contains(\"discrimination analysis\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['da_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['da_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['da_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30f5c41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33965, '1': 214})\n"
     ]
    }
   ],
   "source": [
    "## NAIVE BAYES\n",
    "\n",
    "## text\n",
    "text = ['probabilistic classif']\n",
    "\n",
    "algo['nb_text'] = np.where(groups['text'].str.contains(\"naive bayes\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['nb_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['nb_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(algo['nb_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29971592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33372, '1': 807})\n"
     ]
    }
   ],
   "source": [
    "## TRANSFER LEARNING\n",
    "\n",
    "## text\n",
    "algo['tl_text'] = np.where(groups['text'].str.contains(\"transfer learning\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['tl_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d920132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34163, '1': 16})\n"
     ]
    }
   ],
   "source": [
    "## FEDERATED LEARNING\n",
    "\n",
    "## text\n",
    "algo['fl_text'] = np.where(groups['text'].str.contains(\"federated learning\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['fl_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "146ea1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33629, '1': 550})\n"
     ]
    }
   ],
   "source": [
    "## K-NEAREST NEIGHBOUR\n",
    "\n",
    "## text\n",
    "algo['knn_text'] = np.where(groups['text'].str.contains(\"k-nearest\"), \"1\", \"0\")\n",
    "algo['knn_text'] = np.where(groups['text'].str.contains(\"k nearest neighbour\"), \"1\", algo['knn_text'])\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['knn_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "621c52ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33576, '1': 603})\n"
     ]
    }
   ],
   "source": [
    "## UNSUPERVISED LEARNING\n",
    "\n",
    "## text\n",
    "text = ['k-means', 'means cluster', 'hierarchical cluster', 'unsupervised learning', 'unsupervised algorithm',\n",
    "       'unsupervised model', 'unsupervised method', 'latent class analysis']\n",
    "\n",
    "algo['unsup_text'] = np.where(groups['text'].str.contains(\"clustering algorithm\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    algo['unsup_text'] = np.where(groups['text'].str.contains(x), \"1\", algo['unsup_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(algo['unsup_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41bd3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMBINE\n",
    "labelled['algo_neural_net'] = np.where(algo['nn_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_support_vector'] = np.where(algo['svm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_regression'] = np.where(algo['reg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_decision_tree'] = np.where(algo['dt_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_discriminant'] = np.where(algo['da_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_naive_bayes'] = np.where(algo['nb_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_transfer'] = np.where(algo['tl_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_federated'] = np.where(algo['fl_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_k_nearest'] = np.where(algo['knn_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['algo_unsupervised'] = np.where(algo['unsup_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "#algo.to_csv('output/algo_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a87f48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article_date</th>\n",
       "      <th>pubmed_date</th>\n",
       "      <th>article_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_short</th>\n",
       "      <th>journal_country</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_affils</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>references_pmids</th>\n",
       "      <th>feature</th>\n",
       "      <th>include</th>\n",
       "      <th>mature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34688173</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104924</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.</td>\n",
       "      <td>Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']</td>\n",
       "      <td>['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']</td>\n",
       "      <td>['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688172</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104927</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.</td>\n",
       "      <td>The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']</td>\n",
       "      <td>['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']</td>\n",
       "      <td>['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34687858</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118652</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States.</td>\n",
       "      <td>Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>NeuroImage</td>\n",
       "      <td>Neuroimage</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']</td>\n",
       "      <td>['Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']</td>\n",
       "      <td>['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34687853</td>\n",
       "      <td>10.1016/j.mri.2021.10.024</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.</td>\n",
       "      <td>To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']</td>\n",
       "      <td>['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']</td>\n",
       "      <td>['Endometrial cancer', 'Radiomic machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34687850</td>\n",
       "      <td>10.1016/j.mri.2021.10.023</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.</td>\n",
       "      <td>Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']</td>\n",
       "      <td>['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']</td>\n",
       "      <td>['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pmid                               doi  \\\n",
       "1   34688173  10.1016/j.compbiomed.2021.104924   \n",
       "2   34688172  10.1016/j.compbiomed.2021.104927   \n",
       "8   34687858  10.1016/j.neuroimage.2021.118652   \n",
       "9   34687853         10.1016/j.mri.2021.10.024   \n",
       "10  34687850         10.1016/j.mri.2021.10.023   \n",
       "\n",
       "                                                                                                                                                                              title  \\\n",
       "1                                                             A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.   \n",
       "2                                                                             A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.   \n",
       "8                                                                                                                       Causal Decoding of Individual Cortical Excitability States.   \n",
       "9   Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.   \n",
       "10                                       MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                abstract  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                                                           The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8   Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "\n",
       "   article_date pubmed_date     article_type lang  \\\n",
       "1    2021-10-06  2021-10-24  Journal Article  eng   \n",
       "2    2021-10-11  2021-10-24  Journal Article  eng   \n",
       "8    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "9    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "10   2021-10-20  2021-10-24  Journal Article  eng   \n",
       "\n",
       "                              journal       journal_short journal_country  \\\n",
       "1   Computers in biology and medicine     Comput Biol Med   United States   \n",
       "2   Computers in biology and medicine     Comput Biol Med   United States   \n",
       "8                          NeuroImage          Neuroimage   United States   \n",
       "9          Magnetic resonance imaging  Magn Reson Imaging     Netherlands   \n",
       "10         Magnetic resonance imaging  Magn Reson Imaging     Netherlands   \n",
       "\n",
       "                                                                                                                                                                                                               authors  \\\n",
       "1                                                                                             ['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']   \n",
       "2                                                                                                                                                  ['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']   \n",
       "8                                                                                                                                             ['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']   \n",
       "9   ['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']   \n",
       "10                                                                                                                                          ['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         author_affils  \\\n",
       "1   ['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']   \n",
       "\n",
       "                                                                                                                         keywords  \\\n",
       "1           ['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']   \n",
       "2   ['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']   \n",
       "8                                             ['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']   \n",
       "9                                                                             ['Endometrial cancer', 'Radiomic machine learning']   \n",
       "10                              ['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']   \n",
       "\n",
       "   mesh_terms references_pmids  \\\n",
       "1        <NA>             <NA>   \n",
       "2        <NA>             <NA>   \n",
       "8        <NA>             <NA>   \n",
       "9        <NA>             <NA>   \n",
       "10       <NA>             <NA>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             feature  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                 A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8   Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "\n",
       "   include mature algo_neural_net algo_support_vector algo_regression  \\\n",
       "1      1.0    1.0               1                   0               0   \n",
       "2      1.0    0.0               0                   0               0   \n",
       "8      1.0    0.0               0                   0               0   \n",
       "9      1.0    1.0               0                   0               0   \n",
       "10     1.0    0.0               0                   0               0   \n",
       "\n",
       "   algo_decision_tree algo_discriminant algo_naive_bayes algo_transfer  \\\n",
       "1                   0                 0                0             0   \n",
       "2                   0                 0                0             1   \n",
       "8                   0                 0                0             0   \n",
       "9                   0                 0                0             0   \n",
       "10                  0                 0                0             0   \n",
       "\n",
       "   algo_federated algo_k_nearest algo_unsupervised  \n",
       "1               0              0                 0  \n",
       "2               0              0                 0  \n",
       "8               0              0                 0  \n",
       "9               0              0                 0  \n",
       "10              0              0                 0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27a66f",
   "metadata": {},
   "source": [
    "## Tag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dd88371",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = groups[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66f82d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## CLASSES\n",
    "######################\n",
    "# BIO_MARKER / bio\n",
    "# GENOMIC / gene\n",
    "# IMAGING / imaging\n",
    "    ### XR / xr\n",
    "    ### CT / ct\n",
    "    ### MRI / mri\n",
    "# ECHO / echo\n",
    "# US / us\n",
    "# MAMMOGRAM / mamm\n",
    "# OCT / oct\n",
    "# EEG / eeg\n",
    "# ECG / ecg\n",
    "# EMG / emg\n",
    "# DERMASCOPY / derm\n",
    "# CELLULAR_PATH / histo\n",
    "# ENDOSCOPY / endo\n",
    "#\n",
    "# NATURAL_LANGUAGE / nlp\n",
    "# EHR RECORDS / ehr\n",
    "#\n",
    "# WEARABLE_SENSORS / sensor\n",
    "# SMARTPHONE / phone\n",
    "# PATIENT REPORTED / prom\n",
    "# DIGITAL STETH / sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97ccefab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32828, '1': 1351})\n"
     ]
    }
   ],
   "source": [
    "## XR\n",
    "\n",
    "## text\n",
    "text = ['xr', 'x-ray', 'radiograph']\n",
    "\n",
    "feat['xr_text'] = np.where(groups['text'].str.contains(\"cxr\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['xr_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['xr_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['xr_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd75e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31198, '1': 2981})\n"
     ]
    }
   ],
   "source": [
    "## CT\n",
    "\n",
    "## text\n",
    "text = ['computed tomograph', 'axial tomograph', 'ct scan', 'ct image', 'ct slice', ' ct ', ' ct-',\n",
    "       'tomography scan', 'computerised tomograph', 'computerized tomograph', 'assisted tomograph']\n",
    "\n",
    "feat['ct_text'] = np.where(groups['text'].str.contains(\"cat scan\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['ct_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['ct_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##exclude\n",
    "\n",
    "feat['ct_text'] = np.where(groups['text'].str.contains(\"optical coherence\"), \"0\", feat['ct_text']) #exclude oct\n",
    "feat['ct_text'] = np.where(groups['text'].str.contains(\"coherence tomograph\"), \"0\", feat['ct_text']) #exclude oct\n",
    "        \n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['ct_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb3b9535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 30726, '1': 3453})\n"
     ]
    }
   ],
   "source": [
    "## MRI\n",
    "\n",
    "## text\n",
    "text = ['magnetic resonance']\n",
    "\n",
    "feat['mri_text'] = np.where(groups['text'].str.contains(\" mri\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['mri_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['mri_text'] ) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['mri_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d00d6e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33988, '1': 191})\n"
     ]
    }
   ],
   "source": [
    "## ECHO\n",
    "\n",
    "## text\n",
    "text = ['echo-cardio', 'echokardio', 'cardiac echo']\n",
    "\n",
    "feat['echo_text'] = np.where(groups['text'].str.contains(\"echocardio\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['echo_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['echo_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['echo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1af88f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33003, '1': 1176})\n"
     ]
    }
   ],
   "source": [
    "## US\n",
    "\n",
    "## text\n",
    "text = ['sonography', 'ultra-sound', 'ultrasonograph', 'doppler']\n",
    "\n",
    "feat['us_text'] = np.where(groups['text'].str.contains(\"ultrasound\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['us_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['us_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['us_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e10bf909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33072, '1': 1107})\n"
     ]
    }
   ],
   "source": [
    "## ECG\n",
    "\n",
    "## text\n",
    "text = [' ecg', ' ekg', 'electrokardio', 'electro-cardio', 'holter monitor', 'cardiac monitor']\n",
    "\n",
    "feat['ecg_text'] = np.where(groups['text'].str.contains(\"electrocardio\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['ecg_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['ecg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['ecg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf6f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32309, '1': 1870})\n"
     ]
    }
   ],
   "source": [
    "## EEG\n",
    "\n",
    "## text\n",
    "text = [' eeg']\n",
    "\n",
    "feat['eeg_text'] = np.where(groups['text'].str.contains(\"electroenc\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['eeg_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['eeg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['eeg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "566f93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33656, '1': 523})\n"
     ]
    }
   ],
   "source": [
    "## EMG\n",
    "\n",
    "## text\n",
    "text = ['myoelectric', 'electro-myo']\n",
    "\n",
    "feat['emg_text'] = np.where(groups['text'].str.contains(\"electromyo\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['emg_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['emg_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['emg_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "170697e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74683</th>\n",
       "      <td>deep learning for electromyographic hand gesture signal classification using transfer learning in recent years, deep learning algorithms have become increasingly more prominent for their unparalleled ability to automatically learn discriminant features from large amounts of data however, within the field of electromyography-based gesture recognition, deep learning algorithms are seldom employed as they require an unreasonable amount of effort from a single person, to generate tens of thousands of examples this papers hypothesis is that general, informative features can be learned from the large amounts of data generated by aggregating the signals of multiple users, thus reducing the recording burden while enhancing gesture recognition consequently, this paper proposes applying transfer learning on aggregated data from multiple users while leveraging the capacity of deep learning algorithms to learn discriminant features from large datasets two datasets comprised 19 and 17 able-bodied participants, respectively the first one is employed for pre-training, were recorded for this work, using the myo armband a third myo armband dataset was taken from the ninapro database and is comprised ten able-bodied participants three different deep learning networks employing three different modalities as input raw emg, spectrograms, and continuous wavelet transform cwt are tested on the second and third dataset the proposed transfer learning scheme is shown to systematically and significantly enhance the performance for all three networks on the two datasets, achieving an offline accuracy of 9831% for 7 gestures over 17 participants for the cwt-based convnet and 6898% for 18 gestures over 10 participants for the raw emg-based convnet finally, a use-case study employing eight able-bodied participants suggests that real-time feedback allows users to adapt their muscle activation strategy which reduces the degradation in accuracy normally experienced over time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30529</th>\n",
       "      <td>feature extraction of surface electromyography using wavelet weighted permutation entropy for hand movement recognition the feature extraction of surface electromyography semg signals has been an important aspect of myoelectric prosthesis control to improve the practicability of myoelectric prosthetic hands, we proposed a feature extraction method for semg signals that uses wavelet weighted permutation entropy wwpe first, wavelet transform was used to decompose and preprocess semg signals collected from the relevant muscles of the upper limbs to obtain the wavelet sub-bands in each frequency segment then, the weighted permutation entropies wpes of the wavelet sub-bands were extracted to construct wwpe feature set lastly, the wwpe feature set was used as input to a support vector machine svm classifier and a backpropagation neural network bpnn classifier to recognize seven hand movements experimental results show that the proposed method exhibits remarkable recognition accuracy that is superior to those of single sub-band feature set and commonly used time-domain feature set the maximum recognition accuracy rate is 100% for hand movements, and the average recognition accuracy rates of svm and bpnn are 100% and 98%, respectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36064</th>\n",
       "      <td>estimating knee joint load using acoustic emissions during ambulation quantifying joint load in activities of daily life could lead to improvements in mobility for numerous people; however, current methods for assessing joint load are unsuitable for ubiquitous settings the aim of this study is to demonstrate that joint acoustic emissions contain information to estimate this internal joint load in a potentially wearable implementation eleven healthy, able-bodied individuals performed ambulation tasks under varying speed, incline, and loading conditions while joint acoustic emissions and essential gait measures-electromyography, ground reaction forces, and motion capture trajectories-were collected the gait measures were synthesized using a neuromuscular model to estimate internal joint contact force which was the target variable for subject-specific machine learning models xgboost trained based on spectral, temporal, cepstral, and amplitude-based features of the joint acoustic emissions the model using joint acoustic emissions significantly outperformed p &lt; 005 the best estimate without the sounds, the subject-specific average load mae = 031 ± 012 bw, for both seen mae = 008 ± 001 bw and unseen mae = 021 ± 005 bw conditions this demonstrates that joint acoustic emissions contain information that correlates to internal joint contact force and that information is consistent such that unique cases can be estimated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13659</th>\n",
       "      <td>evaluation of three machine learning algorithms for the automatic classification of emg patterns in gait disorders gait disorders are common in neurodegenerative diseases and distinguishing between seemingly similar kinematic patterns associated with different pathological entities is a challenge even for the experienced clinician ultimately, muscle activity underlies the generation of kinematic patterns therefore, one possible way to address this problem may be to differentiate gait disorders by analyzing intrinsic features of muscle activations patterns here, we examined whether it is possible to differentiate electromyography emg gait patterns of healthy subjects and patients with different gait disorders using machine learning techniques nineteen healthy volunteers 9 male, 10 female, age 282 ± 62 years and 18 patients with gait disorders 10 male, 8 female, age 662 ± 147 years resulting from different neurological diseases walked down a hallway 10 times at a convenient pace while their muscle activity was recorded via surface emg electrodes attached to 5 muscles of each leg 10 channels in total gait disorders were classified as predominantly hypokinetic &lt;i&gt;n&lt;/i&gt; = 12 or ataxic &lt;i&gt;n&lt;/i&gt; = 6 gait by two experienced raters based on video recordings three different classification methods convolutional neural network-cnn, support vector machine-svm, k-nearest neighbors-knn were used to automatically classify emg patterns according to the underlying gait disorder and differentiate patients and healthy participants using a leave-one-out approach for training and evaluating the classifiers, the automatic classification of normal and abnormal emg patterns during gait 2 classes: healthy and patient was possible with a high degree of accuracy using cnn accuracy 919%, but not svm accuracy 676% or knn accuracy 487% for classification of hypokinetic vs ataxic vs normal gait 3 classes best results were again obtained for cnn accuracy 838% while svm and knn performed worse accuracy svm 514%, knn 324% these results suggest that machine learning methods are useful for distinguishing individuals with gait disorders from healthy controls and may help classification with respect to the underlying disorder even when classifiers are trained on comparably small cohorts in our study, cnn achieved higher accuracy than svm and knn and may constitute a promising method for further investigation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147165</th>\n",
       "      <td>support vector machine-based classification scheme for myoelectric control applied to upper limb this paper proposes and evaluates the application of support vector machine svm to classify upper limb motions using myoelectric signals it explores the optimum configuration of svm-based myoelectric control, by suggesting an advantageous data segmentation technique, feature set, model selection approach for svm, and postprocessing methods this work presents a method to adjust svm parameters before classification, and examines overlapped segmentation and majority voting as two techniques to improve controller performance a svm, as the core of classification in myoelectric control, is compared with two commonly used classifiers: linear discriminant analysis lda and multilayer perceptron mlp neural networks it demonstrates exceptional accuracy, robust performance, and low computational load the entropy of the output of the classifier is also examined as an online index to evaluate the correctness of classification; this can be used by online training for long-term myoelectric control operations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "74683                                                                                                                                                                                                                                                                                                                                                                                                                                                        deep learning for electromyographic hand gesture signal classification using transfer learning in recent years, deep learning algorithms have become increasingly more prominent for their unparalleled ability to automatically learn discriminant features from large amounts of data however, within the field of electromyography-based gesture recognition, deep learning algorithms are seldom employed as they require an unreasonable amount of effort from a single person, to generate tens of thousands of examples this papers hypothesis is that general, informative features can be learned from the large amounts of data generated by aggregating the signals of multiple users, thus reducing the recording burden while enhancing gesture recognition consequently, this paper proposes applying transfer learning on aggregated data from multiple users while leveraging the capacity of deep learning algorithms to learn discriminant features from large datasets two datasets comprised 19 and 17 able-bodied participants, respectively the first one is employed for pre-training, were recorded for this work, using the myo armband a third myo armband dataset was taken from the ninapro database and is comprised ten able-bodied participants three different deep learning networks employing three different modalities as input raw emg, spectrograms, and continuous wavelet transform cwt are tested on the second and third dataset the proposed transfer learning scheme is shown to systematically and significantly enhance the performance for all three networks on the two datasets, achieving an offline accuracy of 9831% for 7 gestures over 17 participants for the cwt-based convnet and 6898% for 18 gestures over 10 participants for the raw emg-based convnet finally, a use-case study employing eight able-bodied participants suggests that real-time feedback allows users to adapt their muscle activation strategy which reduces the degradation in accuracy normally experienced over time   \n",
       "30529                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 feature extraction of surface electromyography using wavelet weighted permutation entropy for hand movement recognition the feature extraction of surface electromyography semg signals has been an important aspect of myoelectric prosthesis control to improve the practicability of myoelectric prosthetic hands, we proposed a feature extraction method for semg signals that uses wavelet weighted permutation entropy wwpe first, wavelet transform was used to decompose and preprocess semg signals collected from the relevant muscles of the upper limbs to obtain the wavelet sub-bands in each frequency segment then, the weighted permutation entropies wpes of the wavelet sub-bands were extracted to construct wwpe feature set lastly, the wwpe feature set was used as input to a support vector machine svm classifier and a backpropagation neural network bpnn classifier to recognize seven hand movements experimental results show that the proposed method exhibits remarkable recognition accuracy that is superior to those of single sub-band feature set and commonly used time-domain feature set the maximum recognition accuracy rate is 100% for hand movements, and the average recognition accuracy rates of svm and bpnn are 100% and 98%, respectively   \n",
       "36064                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      estimating knee joint load using acoustic emissions during ambulation quantifying joint load in activities of daily life could lead to improvements in mobility for numerous people; however, current methods for assessing joint load are unsuitable for ubiquitous settings the aim of this study is to demonstrate that joint acoustic emissions contain information to estimate this internal joint load in a potentially wearable implementation eleven healthy, able-bodied individuals performed ambulation tasks under varying speed, incline, and loading conditions while joint acoustic emissions and essential gait measures-electromyography, ground reaction forces, and motion capture trajectories-were collected the gait measures were synthesized using a neuromuscular model to estimate internal joint contact force which was the target variable for subject-specific machine learning models xgboost trained based on spectral, temporal, cepstral, and amplitude-based features of the joint acoustic emissions the model using joint acoustic emissions significantly outperformed p < 005 the best estimate without the sounds, the subject-specific average load mae = 031 ± 012 bw, for both seen mae = 008 ± 001 bw and unseen mae = 021 ± 005 bw conditions this demonstrates that joint acoustic emissions contain information that correlates to internal joint contact force and that information is consistent such that unique cases can be estimated   \n",
       "13659   evaluation of three machine learning algorithms for the automatic classification of emg patterns in gait disorders gait disorders are common in neurodegenerative diseases and distinguishing between seemingly similar kinematic patterns associated with different pathological entities is a challenge even for the experienced clinician ultimately, muscle activity underlies the generation of kinematic patterns therefore, one possible way to address this problem may be to differentiate gait disorders by analyzing intrinsic features of muscle activations patterns here, we examined whether it is possible to differentiate electromyography emg gait patterns of healthy subjects and patients with different gait disorders using machine learning techniques nineteen healthy volunteers 9 male, 10 female, age 282 ± 62 years and 18 patients with gait disorders 10 male, 8 female, age 662 ± 147 years resulting from different neurological diseases walked down a hallway 10 times at a convenient pace while their muscle activity was recorded via surface emg electrodes attached to 5 muscles of each leg 10 channels in total gait disorders were classified as predominantly hypokinetic <i>n</i> = 12 or ataxic <i>n</i> = 6 gait by two experienced raters based on video recordings three different classification methods convolutional neural network-cnn, support vector machine-svm, k-nearest neighbors-knn were used to automatically classify emg patterns according to the underlying gait disorder and differentiate patients and healthy participants using a leave-one-out approach for training and evaluating the classifiers, the automatic classification of normal and abnormal emg patterns during gait 2 classes: healthy and patient was possible with a high degree of accuracy using cnn accuracy 919%, but not svm accuracy 676% or knn accuracy 487% for classification of hypokinetic vs ataxic vs normal gait 3 classes best results were again obtained for cnn accuracy 838% while svm and knn performed worse accuracy svm 514%, knn 324% these results suggest that machine learning methods are useful for distinguishing individuals with gait disorders from healthy controls and may help classification with respect to the underlying disorder even when classifiers are trained on comparably small cohorts in our study, cnn achieved higher accuracy than svm and knn and may constitute a promising method for further investigation   \n",
       "147165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              support vector machine-based classification scheme for myoelectric control applied to upper limb this paper proposes and evaluates the application of support vector machine svm to classify upper limb motions using myoelectric signals it explores the optimum configuration of svm-based myoelectric control, by suggesting an advantageous data segmentation technique, feature set, model selection approach for svm, and postprocessing methods this work presents a method to adjust svm parameters before classification, and examines overlapped segmentation and majority voting as two techniques to improve controller performance a svm, as the core of classification in myoelectric control, is compared with two commonly used classifiers: linear discriminant analysis lda and multilayer perceptron mlp neural networks it demonstrates exceptional accuracy, robust performance, and low computational load the entropy of the output of the classifier is also examined as an online index to evaluate the correctness of classification; this can be used by online training for long-term myoelectric control operations   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \n",
       "74683        0       0        0         0       0        0        0        1  \n",
       "30529        0       0        0         0       0        0        0        1  \n",
       "36064        0       0        0         0       0        0        0        1  \n",
       "13659        0       0        0         0       0        0        0        1  \n",
       "147165       0       0        0         0       0        0        0        1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['emg_text']==\"1\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50c69805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32009, '1': 2170})\n"
     ]
    }
   ],
   "source": [
    "## CELLULAR PATHOLOGY\n",
    "\n",
    "## text\n",
    "text = ['histopath', 'histology', 'histochem', 'immunohist', 'cytolog', 'cytochem', 'cellular path', 'microscopy',\n",
    "       'smear', 'cytometry', 'hematoxylin', 'specimens', 'stain', 'tissue sample', 'tissue section', 'brushing']\n",
    "\n",
    "feat['histo_text'] = np.where(groups['text'].str.contains(\"histologic\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['histo_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['histo_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['histo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f53163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33245, '1': 934})\n"
     ]
    }
   ],
   "source": [
    "## OCT / retinal\n",
    "\n",
    "## text\n",
    "text = ['coherence tomog', ' oct ', 'retinal photo', 'retinal imag', 'retinal tomograph',\n",
    "        'laser ophth', 'fundus imag', 'fundus phot', 'fundal imag', 'fundal phot']\n",
    "\n",
    "feat['oct_text'] = np.where(groups['text'].str.contains(\"optical coherence\"), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['oct_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['oct_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output    \n",
    "print('text counts:')\n",
    "print(Counter(feat['oct_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "615c8ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166783</th>\n",
       "      <td>comparing neural networks and linear discriminant functions for glaucoma detection using confocal scanning laser ophthalmoscopy of the optic disc to determine whether neural network techniques can improve differentiation between glaucomatous and nonglaucomatous eyes, using the optic disc topography parameters of the heidelberg retina tomograph hrt; heidelberg engineering, heidelberg, germany</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51496</th>\n",
       "      <td>deep learning segmentation for optical coherence tomography measurements of the lower tear meniscus the tear meniscus contains most of the tear fluid and therefore is a good indicator for the state of the tear film previously, we used a custom-built optical coherence tomography oct system to study the lower tear meniscus by automatically segmenting the image data with a thresholding-based segmentation algorithm tbsa in this report, we investigate whether the results of this image segmentation algorithm are suitable to train a neural network in order to obtain similar or better segmentation results with shorter processing times considering the class imbalance problem, we compare two approaches, one directly segmenting the tear meniscus dsa, the other first localizing the region of interest and then segmenting within the higher resolution image section lsa a total of 6658 images labeled by the tbsa were used to train deep convolutional neural networks with supervised learning five-fold cross-validation reveals a sensitivity of 9636% and 9643%, a specificity of 9998% and 9986% and a jaccard index of 9324% and 9316% for the dsa and lsa, respectively average segmentation times are up to 228 times faster than the tbsa additionally, we report the behavior of the dsa and lsa in cases challenging for the tbsa and further test the applicability to measurements acquired with a commercially available oct system the application of deep learning for the segmentation of the tear meniscus provides a powerful tool for the assessment of the tear film, supporting studies for the investigation of the pathophysiology of dry eye-related diseases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36645</th>\n",
       "      <td>probability distribution guided optic disc and cup segmentation from fundus images in this paper, we proposed and validated a probability distribution guided network for segmenting optic disc od and optic cup oc from fundus images uncertainty is inevitable in deep learning, as induced by different sensors, insufficient samples, and inaccurate labeling since the input data and the corresponding ground truth label may be inaccurate, they may actually follow some potential distribution in this study, a variational autoencoder vae based network was proposed to estimate the joint distribution of the input image and the corresponding segmentation both the ground truth segmentation and the predicted segmentation, making the segmentation network learn not only pixel-wise information but also semantic probability distribution moreover, we designed a building block, namely the dilated inception block dib, for a better generalization of the model and a more effective extraction of multi-scale features the proposed method was compared to several existing state-of-the-art methods superior segmentation performance has been observed over two datasets origa and refuge, with the mean dice overlap coefficients being 9657% and 9581% for od and 8846% and 8891% for oc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33977</th>\n",
       "      <td>joint optic disc and cup segmentation based on residual multi-scale fully convolutional neural network glaucoma is the leading cause of irreversible blindness, but its early symptoms are not obvious and are easily overlooked, so early screening for glaucoma is particularly important the cup to disc ratio is an important indicator for clinical glaucoma screening, and accurate segmentation of the optic cup and disc is the key to calculating the cup to disc ratio in this paper, a full convolutional neural network with residual multi-scale convolution module was proposed for the optic cup and disc segmentation first, the fundus image was contrast enhanced and polar transformation was introduced subsequently, w-net was used as the backbone network, which replaced the standard convolution unit with the residual multi-scale full convolution module, the input port was added to the image pyramid to construct the multi-scale input, and the side output layer was used as the early classifier to generate the local prediction output finally, a new multi-tag loss function was proposed to guide network segmentation the mean intersection over union of the optic cup and disc segmentation in the refuge dataset was 0904 0 and 0955 3 respectively, and the overlapping error was 0178 0 and 0066 5 respectively the results show that this method not only realizes the joint segmentation of cup and disc, but also improves the segmentation accuracy effectively, which could be helpful for the promotion of large-scale early glaucoma screening</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76356</th>\n",
       "      <td>from machine to machine: an oct-trained deep learning algorithm for objective quantification of glaucomatous damage in fundus photographs previous approaches using deep learning dl algorithms to classify glaucomatous damage on fundus photographs have been limited by the requirement for human labeling of a reference training set we propose a new approach using quantitative spectral-domain sd oct data to train a dl algorithm to quantify glaucomatous structural damage on optic disc photographs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90544</th>\n",
       "      <td>an ensemble deep learning based approach for red lesion detection in fundus images diabetic retinopathy dr is one of the leading causes of preventable blindness in the world its earliest sign are red lesions, a general term that groups both microaneurysms mas and hemorrhages hes in daily clinical practice, these lesions are manually detected by physicians using fundus photographs however, this task is tedious and time consuming, and requires an intensive effort due to the small size of the lesions and their lack of contrast computer-assisted diagnosis of dr based on red lesion detection is being actively explored due to its improvement effects both in clinicians consistency and accuracy moreover, it provides comprehensive feedback that is easy to assess by the physicians several methods for detecting red lesions have been proposed in the literature, most of them based on characterizing lesion candidates using hand crafted features, and classifying them into true or false positive detections deep learning based approaches, by contrast, are scarce in this domain due to the high expense of annotating the lesions manually</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6202</th>\n",
       "      <td>region of interest-based predictive algorithm for subretinal hemorrhage detection using faster r-cnn macular edema me is an essential sort of macular issue caused due to the storing of fluid underneath the macula age-related macular degeneration amd and diabetic macular edema dme are the two customary visual contaminations that can lead to fragmentary or complete vision loss this paper proposes a deep learning-based predictive algorithm that can be used to detect the presence of a subretinal hemorrhage region convolutional neural network r-cnn and faster r-cnn are used to develop the predictive algorithm that can improve the classification accuracy this method initially detects the presence of subretinal hemorrhage, and it then segments the region of interest roi by a semantic segmentation process the segmented roi is applied to a predictive algorithm which is derived from the fast region convolutional neural network algorithm, that can categorize the subretinal hemorrhage as responsive or non-responsive the dataset, provided by a medical institution, comprised of optical coherence tomography oct images of both pre- and post-treatment images, was used for training the proposed faster region convolutional neural network faster r-cnn we also used the kaggle dataset for performance comparison with the traditional methods that are derived from the convolutional neural network cnn algorithm the evaluation results using the kaggle dataset and the hospital images provide an average sensitivity, selectivity, and accuracy of 853%, 8964%, and 9348% respectively further, the proposed method provides a time complexity in testing as 264s, which is less than the traditional schemes like cnn, r-cnn, and fast r-cnn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29448</th>\n",
       "      <td>classification of retinal images based on convolutional neural network automatic detection of maculopathy disease is a very important step to achieve high-accuracy results for the early discovery of the disease to help ophthalmologists to treat patients manual detection of diabetic maculopathy needs much effort and time from ophthalmologists detection of exudates from retinal images is applied for the maculopathy disease diagnosis the first proposed framework in this paper for retinal image classification begins with fuzzy preprocessing in order to improve the original image to enhance the contrast between the objects and the background after that, image segmentation is performed through binarization of the image to extract both blood vessels and the optic disc and then remove them from the original image a gradient process is performed on the retinal image after this removal process for discrimination between normal and abnormal cases histogram of the gradients is estimated, and consequently the cumulative histogram of gradients is obtained and compared with a threshold cumulative histogram at certain bins to determine the threshold cumulative histogram, cumulative histograms of images with exudates and images without exudates are obtained and averaged for each type, and the threshold cumulative histogram is set as the average of both cumulative histograms certain histogram bins are selected and thresholded according to the estimated threshold cumulative histogram, and the results are used for retinal image classification in the second framework in this paper, a convolutional neural network cnn is utilized to classify normal and abnormal cases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25415</th>\n",
       "      <td>automatic prediction of treatment outcomes in patients with diabetic macular edema using ensemble machine learning this study aimed to predict the treatment outcomes in patients with diabetic macular edema dme after 3 monthly anti-vascular endothelial growth factor vegf injections using machine learning ml based on pretreatment optical coherence tomography oct images and clinical variables</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55944</th>\n",
       "      <td>deep learning based sub-retinal fluid segmentation in central serous chorioretinopathy optical coherence tomography scans development of an automated sub-retinal fluid segmentation technique from optical coherence tomography oct scans is faced with challenges such as noise and motion artifacts present in oct images, variation in size, shape and location of fluid pockets within the retina the ability of a fully convolutional neural network to automatically learn significant low level features to differentiate subtle spatial variations makes it suitable for retinal fluid segmentation task hence, a fully convolutional neural network has been proposed in this work for the automatic segmentation of sub-retinal fluid in oct scans of central serous chorioretinopathy csc pathology the proposed method has been evaluated on a dataset of 15 oct volumes and an average dice rate, precision and recall of 091, 093 and 089 respectively has been achieved over the test set</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26890</th>\n",
       "      <td>early detection of diabetic retinopathy based on deep learning and ultra-wide-field fundus images visually impaired and blind people due to diabetic retinopathy were 26 million in 2015 and estimated to be 32 million in 2020 globally though the incidence of diabetic retinopathy is expected to decrease for high-income countries, detection and treatment of it in the early stages are crucial for low-income and middle-income countries due to the recent advancement of deep learning technologies, researchers showed that automated screening and grading of diabetic retinopathy are efficient in saving time and workforce however, most automatic systems utilize conventional fundus photography, despite ultra-wide-field fundus photography provides up to 82% of the retinal surface in this study, we present a diabetic retinopathy detection system based on ultra-wide-field fundus photography and deep learning in experiments, we show that the use of early treatment diabetic retinopathy study 7-standard field image extracted from ultra-wide-field fundus photography outperforms that of the optic disc and macula centered image in a statistical sense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81623</th>\n",
       "      <td>amplitude-scan classification using artificial neural networks optical coherence tomography oct images semi-transparent tissues noninvasively relying on backscatter and interferometry to calculate spatial relationships, oct shares similarities with other pulse-echo modalities there is considerable interest in using machine learning techniques for automated image classification, particularly among ophthalmologists who rely heavily on diagnostic oct artificial neural networks ann consist of interconnected nodes and can be employed as classifiers after training on large datasets conventionally, oct scans are rendered as 2d or 3d human-readable images of which the smallest depth-resolved unit is the amplitude-scan reflectivity-function profile which is difficult for humans to interpret we set out to determine whether amplitude-scan reflectivity-function profiles representing disease signatures could be distinguished and classified by a feed-forward ann our classifier achieved high accuracies after training on only 24 eyes, with evidence of good generalization on unseen data the repertoire of our classifier can now be expanded to include rare and unseen diseases and can be extended to other disciplines and industries</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144047</th>\n",
       "      <td>automatic identification of diabetic maculopathy stages using fundus images diabetes mellitus is a major cause of visual impairment and blindness twenty years after the onset of diabetes, almost all patients with type 1 diabetes and over 60% of patients with type 2 diabetes will have some degree of retinopathy prolonged diabetes retinopathy leads to maculopathy, which impairs the normal vision depending on the severity of damage of the macula this paper presents a computer-based intelligent system for the identification of clinically significant maculopathy, non-clinically significant maculopathy and normal fundus eye images features are extracted from these raw fundus images which are then fed to the classifier our protocol uses feed-forward architecture in an artificial neural network classifier for classification of different stages three different kinds of eye disease conditions were tested in 350 subjects we demonstrated a sensitivity of more than 95% for these classifiers with a specificity of 100%, and results are very promising our systems are ready to run clinically on large amounts of datasets</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>analysis of transfer learning for select retinal disease classification to analyze the effect of transfer learning tl for classification of diabetic retinopathy dr by fundus photography and select retinal diseases by spectral-domain optical coherence tomography sd-oct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110435</th>\n",
       "      <td>a novel image recuperation approach for diagnosing and ranking retinopathy disease level using diabetic fundus image retinal fundus images are widely used in diagnosing and providing treatment for several eye diseases prior works using retinal fundus images detected the presence of exudation with the aid of publicly available dataset using extensive segmentation process though it was proved to be computationally efficient, it failed to create a diabetic retinopathy feature selection system for transparently diagnosing the disease state also the diagnosis of diseases did not employ machine learning methods to categorize candidate fundus images into true positive and true negative ratio several candidate fundus images did not include more detailed feature selection technique for diabetic retinopathy to apply machine learning methods and classify the candidate fundus images on the basis of sliding window a method called, diabetic fundus image recuperation dfir is designed in this paper the initial phase of dfir method select the feature of optic cup in digital retinal fundus images based on sliding window approach with this, the disease state for diabetic retinopathy is assessed the feature selection in dfir method uses collection of sliding windows to obtain the features based on the histogram value the histogram based feature selection with the aid of group sparsity non-overlapping function provides more detailed information of features using support vector model in the second phase, the dfir method based on spiral basis function effectively ranks the diabetic retinopathy diseases the ranking of disease level for each candidate set provides a much promising result for developing practically automated diabetic retinopathy diagnosis system experimental work on digital fundus images using the dfir method performs research on the factors such as sensitivity, specificity rate, ranking efficiency and feature selection time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29342</th>\n",
       "      <td>development and validation of a deep learning system to screen vision-threatening conditions in high myopia using optical coherence tomography images to apply deep learning technology to develop an artificial intelligence ai system that can identify vision-threatening conditions in high myopia patients based on optical coherence tomography oct macular images</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66594</th>\n",
       "      <td>automated classification platform for the identification of otitis media using optical coherence tomography the diagnosis and treatment of otitis media om, a common childhood infection, is a significant burden on the healthcare system diagnosis relies on observer experience via otoscopy, although for non-specialists or inexperienced users, accurate diagnosis can be difficult in past studies, optical coherence tomography oct has been used to quantitatively characterize disease states of om, although with the involvement of experts to interpret and correlate image-based indicators of infection with clinical information in this paper, a flexible and comprehensive framework is presented that automatically extracts features from oct images, classifies data, and presents clinically relevant results in a user-friendly platform suitable for point-of-care and primary care settings this framework was used to test the discrimination between oct images of normal controls, ears with biofilms, and ears with biofilms and middle ear fluid effusion predicted future performance of this classification platform returned promising results 90%+ accuracy in various initial tests with integration into patient healthcare workflow, users of all levels of medical experience may be able to collect oct data and accurately identify the presence of middle ear fluid and/or biofilms</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77520</th>\n",
       "      <td>a deep learning model for the detection of both advanced and early glaucoma using fundus photography to build a deep learning model to diagnose glaucoma using fundus photography</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15846</th>\n",
       "      <td>assessment of generative adversarial networks for synthetic anterior segment optical coherence tomography images in closed-angle detection to develop generative adversarial networks gans that synthesize realistic anterior segment optical coherence tomography as-oct images and evaluate deep learning dl models that are trained on real and synthetic datasets for detecting angle closure</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66609</th>\n",
       "      <td>pivotal trial of an autonomous ai-based diagnostic system for detection of diabetic retinopathy in primary care offices artificial intelligence ai has long promised to increase healthcare affordability, quality and accessibility but fda, until recently, had never authorized an autonomous ai diagnostic system this pivotal trial of an ai system to detect diabetic retinopathy dr in people with diabetes enrolled 900 subjects, with no history of dr at primary care clinics, by comparing to wisconsin fundus photograph reading center fprc widefield stereoscopic photography and macular optical coherence tomography oct, by fprc certified photographers, and fprc grading of early treatment diabetic retinopathy study severity scale etdrs and diabetic macular edema dme more than mild dr mtmdr was defined as etdrs level 35 or higher, and/or dme, in at least one eye ai system operators underwent a standardized training protocol before study start median age was 59 years range, 22-84 years; among participants, 475% of participants were male; 161% were hispanic, 833% not hispanic; 286% african american and 634% were not; 198 238% had mtmdr the ai system exceeded all pre-specified superiority endpoints at sensitivity of 872% 95% ci, 818-912% &gt;85%, specificity of 907% 95% ci, 883-927% &gt;825%, and imageability rate of 961% 95% ci, 946-973%, demonstrating ais ability to bring specialty-level diagnostics to primary care settings based on these results, fda authorized the system for use by health care providers to detect more than mild dr and diabetic macular edema, making it, the first fda authorized autonomous ai diagnostic system in any field of medicine, with the potential to help prevent vision loss in thousands of people with diabetes annually clinicaltrialsgov nct02963441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "166783                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      comparing neural networks and linear discriminant functions for glaucoma detection using confocal scanning laser ophthalmoscopy of the optic disc to determine whether neural network techniques can improve differentiation between glaucomatous and nonglaucomatous eyes, using the optic disc topography parameters of the heidelberg retina tomograph hrt; heidelberg engineering, heidelberg, germany   \n",
       "51496                                                                                                                                                                                                                                                                                                              deep learning segmentation for optical coherence tomography measurements of the lower tear meniscus the tear meniscus contains most of the tear fluid and therefore is a good indicator for the state of the tear film previously, we used a custom-built optical coherence tomography oct system to study the lower tear meniscus by automatically segmenting the image data with a thresholding-based segmentation algorithm tbsa in this report, we investigate whether the results of this image segmentation algorithm are suitable to train a neural network in order to obtain similar or better segmentation results with shorter processing times considering the class imbalance problem, we compare two approaches, one directly segmenting the tear meniscus dsa, the other first localizing the region of interest and then segmenting within the higher resolution image section lsa a total of 6658 images labeled by the tbsa were used to train deep convolutional neural networks with supervised learning five-fold cross-validation reveals a sensitivity of 9636% and 9643%, a specificity of 9998% and 9986% and a jaccard index of 9324% and 9316% for the dsa and lsa, respectively average segmentation times are up to 228 times faster than the tbsa additionally, we report the behavior of the dsa and lsa in cases challenging for the tbsa and further test the applicability to measurements acquired with a commercially available oct system the application of deep learning for the segmentation of the tear meniscus provides a powerful tool for the assessment of the tear film, supporting studies for the investigation of the pathophysiology of dry eye-related diseases   \n",
       "36645                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              probability distribution guided optic disc and cup segmentation from fundus images in this paper, we proposed and validated a probability distribution guided network for segmenting optic disc od and optic cup oc from fundus images uncertainty is inevitable in deep learning, as induced by different sensors, insufficient samples, and inaccurate labeling since the input data and the corresponding ground truth label may be inaccurate, they may actually follow some potential distribution in this study, a variational autoencoder vae based network was proposed to estimate the joint distribution of the input image and the corresponding segmentation both the ground truth segmentation and the predicted segmentation, making the segmentation network learn not only pixel-wise information but also semantic probability distribution moreover, we designed a building block, namely the dilated inception block dib, for a better generalization of the model and a more effective extraction of multi-scale features the proposed method was compared to several existing state-of-the-art methods superior segmentation performance has been observed over two datasets origa and refuge, with the mean dice overlap coefficients being 9657% and 9581% for od and 8846% and 8891% for oc   \n",
       "33977                                                                                                                                                                                                                                                                                                                                                                                                                                joint optic disc and cup segmentation based on residual multi-scale fully convolutional neural network glaucoma is the leading cause of irreversible blindness, but its early symptoms are not obvious and are easily overlooked, so early screening for glaucoma is particularly important the cup to disc ratio is an important indicator for clinical glaucoma screening, and accurate segmentation of the optic cup and disc is the key to calculating the cup to disc ratio in this paper, a full convolutional neural network with residual multi-scale convolution module was proposed for the optic cup and disc segmentation first, the fundus image was contrast enhanced and polar transformation was introduced subsequently, w-net was used as the backbone network, which replaced the standard convolution unit with the residual multi-scale full convolution module, the input port was added to the image pyramid to construct the multi-scale input, and the side output layer was used as the early classifier to generate the local prediction output finally, a new multi-tag loss function was proposed to guide network segmentation the mean intersection over union of the optic cup and disc segmentation in the refuge dataset was 0904 0 and 0955 3 respectively, and the overlapping error was 0178 0 and 0066 5 respectively the results show that this method not only realizes the joint segmentation of cup and disc, but also improves the segmentation accuracy effectively, which could be helpful for the promotion of large-scale early glaucoma screening   \n",
       "76356                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  from machine to machine: an oct-trained deep learning algorithm for objective quantification of glaucomatous damage in fundus photographs previous approaches using deep learning dl algorithms to classify glaucomatous damage on fundus photographs have been limited by the requirement for human labeling of a reference training set we propose a new approach using quantitative spectral-domain sd oct data to train a dl algorithm to quantify glaucomatous structural damage on optic disc photographs   \n",
       "90544                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  an ensemble deep learning based approach for red lesion detection in fundus images diabetic retinopathy dr is one of the leading causes of preventable blindness in the world its earliest sign are red lesions, a general term that groups both microaneurysms mas and hemorrhages hes in daily clinical practice, these lesions are manually detected by physicians using fundus photographs however, this task is tedious and time consuming, and requires an intensive effort due to the small size of the lesions and their lack of contrast computer-assisted diagnosis of dr based on red lesion detection is being actively explored due to its improvement effects both in clinicians consistency and accuracy moreover, it provides comprehensive feedback that is easy to assess by the physicians several methods for detecting red lesions have been proposed in the literature, most of them based on characterizing lesion candidates using hand crafted features, and classifying them into true or false positive detections deep learning based approaches, by contrast, are scarce in this domain due to the high expense of annotating the lesions manually   \n",
       "6202                                                                                                                                                                                                                                  region of interest-based predictive algorithm for subretinal hemorrhage detection using faster r-cnn macular edema me is an essential sort of macular issue caused due to the storing of fluid underneath the macula age-related macular degeneration amd and diabetic macular edema dme are the two customary visual contaminations that can lead to fragmentary or complete vision loss this paper proposes a deep learning-based predictive algorithm that can be used to detect the presence of a subretinal hemorrhage region convolutional neural network r-cnn and faster r-cnn are used to develop the predictive algorithm that can improve the classification accuracy this method initially detects the presence of subretinal hemorrhage, and it then segments the region of interest roi by a semantic segmentation process the segmented roi is applied to a predictive algorithm which is derived from the fast region convolutional neural network algorithm, that can categorize the subretinal hemorrhage as responsive or non-responsive the dataset, provided by a medical institution, comprised of optical coherence tomography oct images of both pre- and post-treatment images, was used for training the proposed faster region convolutional neural network faster r-cnn we also used the kaggle dataset for performance comparison with the traditional methods that are derived from the convolutional neural network cnn algorithm the evaluation results using the kaggle dataset and the hospital images provide an average sensitivity, selectivity, and accuracy of 853%, 8964%, and 9348% respectively further, the proposed method provides a time complexity in testing as 264s, which is less than the traditional schemes like cnn, r-cnn, and fast r-cnn   \n",
       "29448                                                                                                                                                                                                                                                                                         classification of retinal images based on convolutional neural network automatic detection of maculopathy disease is a very important step to achieve high-accuracy results for the early discovery of the disease to help ophthalmologists to treat patients manual detection of diabetic maculopathy needs much effort and time from ophthalmologists detection of exudates from retinal images is applied for the maculopathy disease diagnosis the first proposed framework in this paper for retinal image classification begins with fuzzy preprocessing in order to improve the original image to enhance the contrast between the objects and the background after that, image segmentation is performed through binarization of the image to extract both blood vessels and the optic disc and then remove them from the original image a gradient process is performed on the retinal image after this removal process for discrimination between normal and abnormal cases histogram of the gradients is estimated, and consequently the cumulative histogram of gradients is obtained and compared with a threshold cumulative histogram at certain bins to determine the threshold cumulative histogram, cumulative histograms of images with exudates and images without exudates are obtained and averaged for each type, and the threshold cumulative histogram is set as the average of both cumulative histograms certain histogram bins are selected and thresholded according to the estimated threshold cumulative histogram, and the results are used for retinal image classification in the second framework in this paper, a convolutional neural network cnn is utilized to classify normal and abnormal cases   \n",
       "25415                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         automatic prediction of treatment outcomes in patients with diabetic macular edema using ensemble machine learning this study aimed to predict the treatment outcomes in patients with diabetic macular edema dme after 3 monthly anti-vascular endothelial growth factor vegf injections using machine learning ml based on pretreatment optical coherence tomography oct images and clinical variables   \n",
       "55944                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        deep learning based sub-retinal fluid segmentation in central serous chorioretinopathy optical coherence tomography scans development of an automated sub-retinal fluid segmentation technique from optical coherence tomography oct scans is faced with challenges such as noise and motion artifacts present in oct images, variation in size, shape and location of fluid pockets within the retina the ability of a fully convolutional neural network to automatically learn significant low level features to differentiate subtle spatial variations makes it suitable for retinal fluid segmentation task hence, a fully convolutional neural network has been proposed in this work for the automatic segmentation of sub-retinal fluid in oct scans of central serous chorioretinopathy csc pathology the proposed method has been evaluated on a dataset of 15 oct volumes and an average dice rate, precision and recall of 091, 093 and 089 respectively has been achieved over the test set   \n",
       "26890                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       early detection of diabetic retinopathy based on deep learning and ultra-wide-field fundus images visually impaired and blind people due to diabetic retinopathy were 26 million in 2015 and estimated to be 32 million in 2020 globally though the incidence of diabetic retinopathy is expected to decrease for high-income countries, detection and treatment of it in the early stages are crucial for low-income and middle-income countries due to the recent advancement of deep learning technologies, researchers showed that automated screening and grading of diabetic retinopathy are efficient in saving time and workforce however, most automatic systems utilize conventional fundus photography, despite ultra-wide-field fundus photography provides up to 82% of the retinal surface in this study, we present a diabetic retinopathy detection system based on ultra-wide-field fundus photography and deep learning in experiments, we show that the use of early treatment diabetic retinopathy study 7-standard field image extracted from ultra-wide-field fundus photography outperforms that of the optic disc and macula centered image in a statistical sense   \n",
       "81623                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  amplitude-scan classification using artificial neural networks optical coherence tomography oct images semi-transparent tissues noninvasively relying on backscatter and interferometry to calculate spatial relationships, oct shares similarities with other pulse-echo modalities there is considerable interest in using machine learning techniques for automated image classification, particularly among ophthalmologists who rely heavily on diagnostic oct artificial neural networks ann consist of interconnected nodes and can be employed as classifiers after training on large datasets conventionally, oct scans are rendered as 2d or 3d human-readable images of which the smallest depth-resolved unit is the amplitude-scan reflectivity-function profile which is difficult for humans to interpret we set out to determine whether amplitude-scan reflectivity-function profiles representing disease signatures could be distinguished and classified by a feed-forward ann our classifier achieved high accuracies after training on only 24 eyes, with evidence of good generalization on unseen data the repertoire of our classifier can now be expanded to include rare and unseen diseases and can be extended to other disciplines and industries   \n",
       "144047                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                automatic identification of diabetic maculopathy stages using fundus images diabetes mellitus is a major cause of visual impairment and blindness twenty years after the onset of diabetes, almost all patients with type 1 diabetes and over 60% of patients with type 2 diabetes will have some degree of retinopathy prolonged diabetes retinopathy leads to maculopathy, which impairs the normal vision depending on the severity of damage of the macula this paper presents a computer-based intelligent system for the identification of clinically significant maculopathy, non-clinically significant maculopathy and normal fundus eye images features are extracted from these raw fundus images which are then fed to the classifier our protocol uses feed-forward architecture in an artificial neural network classifier for classification of different stages three different kinds of eye disease conditions were tested in 350 subjects we demonstrated a sensitivity of more than 95% for these classifiers with a specificity of 100%, and results are very promising our systems are ready to run clinically on large amounts of datasets   \n",
       "6812                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      analysis of transfer learning for select retinal disease classification to analyze the effect of transfer learning tl for classification of diabetic retinopathy dr by fundus photography and select retinal diseases by spectral-domain optical coherence tomography sd-oct   \n",
       "110435  a novel image recuperation approach for diagnosing and ranking retinopathy disease level using diabetic fundus image retinal fundus images are widely used in diagnosing and providing treatment for several eye diseases prior works using retinal fundus images detected the presence of exudation with the aid of publicly available dataset using extensive segmentation process though it was proved to be computationally efficient, it failed to create a diabetic retinopathy feature selection system for transparently diagnosing the disease state also the diagnosis of diseases did not employ machine learning methods to categorize candidate fundus images into true positive and true negative ratio several candidate fundus images did not include more detailed feature selection technique for diabetic retinopathy to apply machine learning methods and classify the candidate fundus images on the basis of sliding window a method called, diabetic fundus image recuperation dfir is designed in this paper the initial phase of dfir method select the feature of optic cup in digital retinal fundus images based on sliding window approach with this, the disease state for diabetic retinopathy is assessed the feature selection in dfir method uses collection of sliding windows to obtain the features based on the histogram value the histogram based feature selection with the aid of group sparsity non-overlapping function provides more detailed information of features using support vector model in the second phase, the dfir method based on spiral basis function effectively ranks the diabetic retinopathy diseases the ranking of disease level for each candidate set provides a much promising result for developing practically automated diabetic retinopathy diagnosis system experimental work on digital fundus images using the dfir method performs research on the factors such as sensitivity, specificity rate, ranking efficiency and feature selection time    \n",
       "29342                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         development and validation of a deep learning system to screen vision-threatening conditions in high myopia using optical coherence tomography images to apply deep learning technology to develop an artificial intelligence ai system that can identify vision-threatening conditions in high myopia patients based on optical coherence tomography oct macular images   \n",
       "66594                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     automated classification platform for the identification of otitis media using optical coherence tomography the diagnosis and treatment of otitis media om, a common childhood infection, is a significant burden on the healthcare system diagnosis relies on observer experience via otoscopy, although for non-specialists or inexperienced users, accurate diagnosis can be difficult in past studies, optical coherence tomography oct has been used to quantitatively characterize disease states of om, although with the involvement of experts to interpret and correlate image-based indicators of infection with clinical information in this paper, a flexible and comprehensive framework is presented that automatically extracts features from oct images, classifies data, and presents clinically relevant results in a user-friendly platform suitable for point-of-care and primary care settings this framework was used to test the discrimination between oct images of normal controls, ears with biofilms, and ears with biofilms and middle ear fluid effusion predicted future performance of this classification platform returned promising results 90%+ accuracy in various initial tests with integration into patient healthcare workflow, users of all levels of medical experience may be able to collect oct data and accurately identify the presence of middle ear fluid and/or biofilms   \n",
       "77520                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                a deep learning model for the detection of both advanced and early glaucoma using fundus photography to build a deep learning model to diagnose glaucoma using fundus photography   \n",
       "15846                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                assessment of generative adversarial networks for synthetic anterior segment optical coherence tomography images in closed-angle detection to develop generative adversarial networks gans that synthesize realistic anterior segment optical coherence tomography as-oct images and evaluate deep learning dl models that are trained on real and synthetic datasets for detecting angle closure   \n",
       "66609                                                                                                                                                                         pivotal trial of an autonomous ai-based diagnostic system for detection of diabetic retinopathy in primary care offices artificial intelligence ai has long promised to increase healthcare affordability, quality and accessibility but fda, until recently, had never authorized an autonomous ai diagnostic system this pivotal trial of an ai system to detect diabetic retinopathy dr in people with diabetes enrolled 900 subjects, with no history of dr at primary care clinics, by comparing to wisconsin fundus photograph reading center fprc widefield stereoscopic photography and macular optical coherence tomography oct, by fprc certified photographers, and fprc grading of early treatment diabetic retinopathy study severity scale etdrs and diabetic macular edema dme more than mild dr mtmdr was defined as etdrs level 35 or higher, and/or dme, in at least one eye ai system operators underwent a standardized training protocol before study start median age was 59 years range, 22-84 years; among participants, 475% of participants were male; 161% were hispanic, 833% not hispanic; 286% african american and 634% were not; 198 238% had mtmdr the ai system exceeded all pre-specified superiority endpoints at sensitivity of 872% 95% ci, 818-912% >85%, specificity of 907% 95% ci, 883-927% >825%, and imageability rate of 961% 95% ci, 946-973%, demonstrating ais ability to bring specialty-level diagnostics to primary care settings based on these results, fda authorized the system for use by health care providers to detect more than mild dr and diabetic macular edema, making it, the first fda authorized autonomous ai diagnostic system in any field of medicine, with the potential to help prevent vision loss in thousands of people with diabetes annually clinicaltrialsgov nct02963441   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "166783       0       0        0         0       0        0        0        0   \n",
       "51496        0       0        0         0       0        0        0        0   \n",
       "36645        0       0        0         0       0        0        0        0   \n",
       "33977        0       0        0         0       0        0        0        0   \n",
       "76356        0       0        0         0       0        0        0        0   \n",
       "90544        0       0        0         0       0        0        0        0   \n",
       "6202         0       0        0         0       0        0        0        0   \n",
       "29448        0       0        0         0       0        0        0        0   \n",
       "25415        0       0        0         0       0        0        0        0   \n",
       "55944        0       0        0         0       0        0        0        0   \n",
       "26890        0       0        0         0       0        0        0        0   \n",
       "81623        0       0        0         0       0        0        0        0   \n",
       "144047       0       0        0         0       0        0        0        0   \n",
       "6812         0       0        0         0       0        0        0        0   \n",
       "110435       0       0        0         0       0        0        0        0   \n",
       "29342        0       0        0         0       0        0        0        0   \n",
       "66594        0       0        0         0       0        0        0        0   \n",
       "77520        0       0        0         0       0        0        0        0   \n",
       "15846        0       0        0         0       0        0        0        0   \n",
       "66609        0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text  \n",
       "166783          0        1  \n",
       "51496           0        1  \n",
       "36645           0        1  \n",
       "33977           0        1  \n",
       "76356           0        1  \n",
       "90544           0        1  \n",
       "6202            0        1  \n",
       "29448           0        1  \n",
       "25415           0        1  \n",
       "55944           0        1  \n",
       "26890           0        1  \n",
       "81623           0        1  \n",
       "144047          0        1  \n",
       "6812            0        1  \n",
       "110435          0        1  \n",
       "29342           0        1  \n",
       "66594           0        1  \n",
       "77520           0        1  \n",
       "15846           0        1  \n",
       "66609           0        1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['oct_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da73cf19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33611, '1': 568})\n"
     ]
    }
   ],
   "source": [
    "## MAMMOGRAM\n",
    "\n",
    "## text\n",
    "feat['mamm_text'] = np.where(groups['text'].str.contains(\"mammog\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['mamm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e86ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33576, '1': 603})\n"
     ]
    }
   ],
   "source": [
    "## FIBREOPTIC ENDOSCOPY\n",
    "\n",
    "## text\n",
    "text = ['colonoscop', 'endoscop', 'bronchoscop', 'fiberoptic', 'fiber-optic', 'fiberscop', 'fibrescop',\n",
    "       'cystoscop', 'enteroscop', 'hysteroscop']\n",
    "\n",
    "feat['endo_text'] = np.where(groups['text'].str.contains('endoscopy'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['endo_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['endo_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['endo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "563a4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33876, '1': 303})\n"
     ]
    }
   ],
   "source": [
    "## DERMATOLOGY IMAGES\n",
    "\n",
    "## text\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"dermoscop\"), \"1\", \"0\")\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"dermascop\"), \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"skin cancer\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"skin cancer\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"dermat\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"dermat\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"melanoma\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"melanoma\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"skin lesion\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"skin lesion\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"image\")) &\n",
    "                             (groups['text'].str.contains(\"rash\")) , \"1\", feat['derm_text'])\n",
    "feat['derm_text'] = np.where((groups['text'].str.contains(\"photo\")) &\n",
    "                             (groups['text'].str.contains(\"rash\")) , \"1\", feat['derm_text'])\n",
    "\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"histo\"), \"0\", feat['derm_text']) # exclude histological studies\n",
    "feat['derm_text'] = np.where(groups['text'].str.contains(\"microsc\"), \"0\", feat['derm_text']) # exclude microscopy\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['derm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6aaf3f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "      <th>mamm_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>derm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161554</th>\n",
       "      <td>automatic lesion boundary detection in dermoscopy images using gradient vector flow snakes malignant melanoma has a good prognosis if treated early dermoscopy images of pigmented lesions are most commonly taken at x 10 magnification under lighting at a low angle of incidence while the skin is immersed in oil under a glass plate accurate skin lesion segmentation from the background skin is important because some of the features anticipated to be used for diagnosis deal with shape of the lesion and others deal with the color of the lesion compared with the color of the surrounding skin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132026</th>\n",
       "      <td>computer-aided pattern classification system for dermoscopy images computer-aided pattern classification of melanoma and other pigmented skin lesions is one of the most important tasks for clinical diagnosis to differentiate between benign and malignant lesions, the extraction of color, architectural order, symmetry of pattern and homogeneity cash is a challenging task</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136351</th>\n",
       "      <td>skin lesion segmentation using an improved snake model accurate identification of lesion borders is an important task in the analysis of dermoscopy images since the extraction of skin lesion borders provides important cues for accurate diagnosis snakes have been used for segmenting a variety of medical imagery including dermoscopy, however, due to the compromise of internal and external energy forces they can lead to under- or over-segmentation problems in this paper, we introduce a mean shift based gradient vector flow gvf snake algorithm that drives the internal/external energies towards the correct direction the proposed segmentation method incorporates a mean shift operation within the standard gvf cost function experimental results on a large set of diverse dermoscopy images demonstrate that the presented method accurately determines skin lesion borders in dermoscopy images</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83254</th>\n",
       "      <td>learning to detect blue-white structures in dermoscopy images with weak supervision we propose a novel approach to identify one of the most significant dermoscopic criteria in the diagnosis of cutaneous melanoma: the blue-white structure bws in this paper, we achieve this goal in a multiple instance learning mil framework using only image-level labels indicating whether the feature is present or not to this aim, each image is represented as a bag of nonoverlapping regions, where each region may or may not be identified as an instance of bws a probabilistic graphical model is trained in mil fashion to predict the bag image labels as output, we predict the classification label for the image ie, the presence or absence of bws in each image and we also localize the feature in the image experiments are conducted on a challenging dataset with results outperforming state-of-the-art techniques, with bws detection besting competing methods in terms of performance this study provides an improvement on the scope of modeling for computerized image analysis of skin lesions in particular, it propounds a framework for identification of dermoscopic local features from weakly labeled data</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85384</th>\n",
       "      <td>dense deconvolution net: multi path fusion and dense deconvolution for high resolution skin lesion segmentation dermoscopy imaging has been a routine examination approach for skin lesion diagnosis accurate segmentation is the first step for automatic dermoscopy image assessment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73042</th>\n",
       "      <td>a convolutional neural network trained with dermoscopic images performed on par with 145 dermatologists in a clinical melanoma image classification task recent studies have demonstrated the use of convolutional neural networks cnns to classify images of melanoma with accuracies comparable to those achieved by board-certified dermatologists however, the performance of a cnn exclusively trained with dermoscopic images in a clinical image classification task in direct competition with a large number of dermatologists has not been measured to date this study compares the performance of a convolutional neuronal network trained with dermoscopic images exclusively for identifying melanoma in clinical photographs with the manual grading of the same images by dermatologists</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27979</th>\n",
       "      <td>robustness of convolutional neural networks in recognition of pigmented skin lesions a basic requirement for artificial intelligence ai-based image analysis systems, which are to be integrated into clinical practice, is a high robustness minor changes in how those images are acquired, for example, during routine skin cancer screening, should not change the diagnosis of such assistance systems</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>non-melanoma skin cancer diagnosis: a comparison between dermoscopic and smartphone images by unified visual and sonification deep learning algorithms non-melanoma skin cancer nmsc is the most frequent keratinocyte-origin skin tumor it is confirmed that dermoscopy of nmsc confers a diagnostic advantage as compared to visual face-to-face assessment covid-19 restrictions diagnostics by telemedicine photos, which are analogous to visual inspection, displaced part of in-person visits this study evaluated by a dual convolutional neural network cnn performance metrics in dermoscopic di versus smartphone-captured images si and tested if artificial intelligence narrows the proclaimed gap in diagnostic accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54350</th>\n",
       "      <td>multiple skin lesions diagnostics via integrated deep convolutional networks for segmentation and classification computer automated diagnosis of various skin lesions through medical dermoscopy images remains a challenging task</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>data augmentation using adversarial image-to-image translation for the segmentation of mobile-acquired dermatological images dermoscopic images allow the detailed examination of subsurface characteristics of the skin, which led to creating several substantial databases of diverse skin lesions however, the dermoscope is not an easily accessible tool in some regions a less expensive alternative could be acquiring medium resolution clinical macroscopic images of skin lesions however, the limited volume of macroscopic images available, especially mobile-acquired, hinders developing a clinical mobile-based deep learning approach in this work, we present a technique to efficiently utilize the sizable number of dermoscopic images to improve the segmentation capacity of macroscopic skin lesion images a cycle-consistent adversarial network is used to translate the image between the two distinct domains created by the different image acquisition devices a visual inspection was performed on several databases for qualitative evaluation of the results, based on the disappearance and appearance of intrinsic dermoscopic and macroscopic features moreover, the fréchet inception distance was used as a quantitative metric the quantitative segmentation results are demonstrated on the available macroscopic segmentation databases, smartskins and dermofit image library, yielding test set thresholded jaccard index of 8513% and 7430% these results establish a new state-of-the-art performance in the smartskins database</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134504</th>\n",
       "      <td>generalizing common tasks in automated skin lesion diagnosis we present a general model using supervised learning and map estimation that is capable of performing many common tasks in automated skin lesion diagnosis we apply our model to segment skin lesions, detect occluding hair, and identify the dermoscopic structure pigment network quantitative results are presented for segmentation and hair detection and are competitive when compared to other specialized methods additionally, we leverage the probabilistic nature of the model to produce receiver operating characteristic curves, show compelling visualizations of pigment networks, and provide confidence intervals on segmentations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15617</th>\n",
       "      <td>the utrack framework for segmenting and measuring dermatological ulcers through telemedicine chronic dermatological ulcers cause great discomfort to patients, and while monitoring the size of wounds over time provides significant clues about the healing evolution and the clinical condition of patients, the lack of practical applications in existing studies impairs usersaccess to appropriate treatment and diagnosis methods we propose the utrack framework to help with the acquisition of photos, the segmentation and measurement of wounds, the storage of photos and symptoms, and the visualization of the evolution of ulcer healing utrack-app is a mobile app for the framework, which processes images taken by standard mobile device cameras without specialized equipment and stores all data locally the user manually delineates the regions of the wound and the measurement object, and the tool uses the proposed utrack-seg segmentation method to segment them utrack-app also allows users to manually input a unit of measurement centimeter or inch in the image to improve the wound area estimation experiments show that utrack-seg outperforms its state-of-the-art competitors in ulcer segmentation tasks, improving f-measure by up to 825% when compared to superpixel-based approaches and up to 19% when compared to deep learning ones the method is unsupervised, and it semi-automatically segments real-world images with 09 of f-measure, on average the automatic measurement outperformed the manual process in three out of five different rulers utrack-app takes at most 30 s to perform all evaluation steps over high-resolution images, thus being well-suited to analyze ulcers using standard mobile devices</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44924</th>\n",
       "      <td>the dermoscopic inverse approach significantly improves the accuracy of human readers for lentigo maligna diagnosis a recently introduced dermoscopic method for the diagnosis of early lentigo maligna lm is based on the absence of prevalent patterns of pigmented actinic keratosis and solar lentigo/flat seborrheic keratosis we term this the inverse approach</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>the study of usefulness of a set of fractal parameters to build classes of disease units based on images of pigmented skin lesions dermatoscopic images are also increasingly used to train artificial neural networks for the future to provide fully automatic diagnostic systems capable of determining the type of pigmented skin lesion therefore, fractal analysis was used in this study to measure the irregularity of pigmented skin lesion surfaces this paper presents selected results from individual stages of preliminary processing of the dermatoscopic image on pigmented skin lesion, in which fractal analysis was used and referred to the effectiveness of classification by fuzzy or statistical methods classification of the first unsupervised stage was performed using the method of analysis of scatter graphs and the fuzzy method using the kohonen network the results of the kohonen network learning process with an input vector consisting of eight elements prove that neuronal activation requires a larger learning set with greater differentiation for the same training conditions, the final results are at a higher level and can be classified as weaker statistics of factor analysis were proposed, allowing for the reduction in variables, and the directions of further studies were indicated</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21211</th>\n",
       "      <td>reducing the impact of confounding factors on skin cancer classification via image segmentation: technical model study studies have shown that artificial intelligence achieves similar or better performance than dermatologists in specific dermoscopic image classification tasks however, artificial intelligence is susceptible to the influence of confounding factors within images eg, skin markings, which can lead to false diagnoses of cancerous skin lesions image segmentation can remove lesion-adjacent confounding factors but greatly change the image representation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109133</th>\n",
       "      <td>validation of a skin-lesion image-matching algorithm based on computer vision technology melanoma incidence is increasing globally, but consistently accurate skin-lesion classification methods remain elusive we developed a simple software system to classify potentially all types of skin lesions in the current study, we evaluated the systems ability to identify melanomas with a diameter of 10 mm or larger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36651</th>\n",
       "      <td>deep convolutional neural network ensembles for multi-classification of skin lesions from dermoscopic and clinical images in this paper, we consider the problem of classifying skin lesions into multiple classes using both dermoscopic and clinical images different convolutional neural network architectures are considered for this task and a novel ensemble scheme is proposed, which makes use of a progressive transfer learning strategy the proposed approach is tested over a dataset of 4000 images containing both dermoscopic and clinical examples and it is shown to achieve an average specificity of 933% and an average sensitivity of 799% in discriminating skin lesions belonging to four different classes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128698</th>\n",
       "      <td>analysis of clinical and dermoscopic features for basal cell carcinoma neural network classification basal cell carcinoma bcc is the most commonly diagnosed cancer in the usa in this research, we examine four different feature categories used for diagnostic decisions, including patient personal profile patient age, gender, etc, general exam lesion size and location, common dermoscopic blue-gray ovoids, leaf-structure dirt trails, etc, and specific dermoscopic lesion white/pink areas, semitranslucency, etc specific dermoscopic features are more restricted versions of the common dermoscopic features</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71268</th>\n",
       "      <td>deep learning outperformed 136 of 157 dermatologists in a head-to-head dermoscopic melanoma image classification task recent studies have successfully demonstrated the use of deep-learning algorithms for dermatologist-level classification of suspicious lesions by the use of excessive proprietary image databases and limited numbers of dermatologists for the first time, the performance of a deep-learning algorithm trained by open-source images exclusively is compared to a large number of dermatologists covering all levels within the clinical hierarchy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19583</th>\n",
       "      <td>predicting the clinical management of skin lesions using deep learning automated machine learning approaches to skin lesion diagnosis from images are approaching dermatologist-level performance however, current machine learning approaches that suggest management decisions rely on predicting the underlying skin condition to infer a management decision without considering the variability of management decisions that may exist within a single condition we present the first work to explore image-based prediction of clinical management decisions directly without explicitly predicting the diagnosis in particular, we use clinical and dermoscopic images of skin lesions along with patient metadata from the interactive atlas of dermoscopy dataset 1011 cases; 20 disease labels; 3 management decisions and demonstrate that predicting management labels directly is more accurate than predicting the diagnosis and then inferring the management decision formula: see text and formula: see text improvement in overall accuracy and auroc respectively, statistically significant at formula: see text directly predicting management decisions also considerably reduces the over-excision rate as compared to management decisions inferred from diagnosis predictions 2456% fewer cases wrongly predicted to be excised furthermore, we show that training a model to also simultaneously predict the seven-point criteria and the diagnosis of skin lesions yields an even higher accuracy improvements of formula: see text and formula: see text in overall accuracy and auroc respectively of management predictions finally, we demonstrate our models generalizability by evaluating on the publicly available mclass-d dataset and show that our model agrees with the clinical management recommendations of 157 dermatologists as much as they agree amongst each other</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  \\\n",
       "161554                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     automatic lesion boundary detection in dermoscopy images using gradient vector flow snakes malignant melanoma has a good prognosis if treated early dermoscopy images of pigmented lesions are most commonly taken at x 10 magnification under lighting at a low angle of incidence while the skin is immersed in oil under a glass plate accurate skin lesion segmentation from the background skin is important because some of the features anticipated to be used for diagnosis deal with shape of the lesion and others deal with the color of the lesion compared with the color of the surrounding skin   \n",
       "132026                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                computer-aided pattern classification system for dermoscopy images computer-aided pattern classification of melanoma and other pigmented skin lesions is one of the most important tasks for clinical diagnosis to differentiate between benign and malignant lesions, the extraction of color, architectural order, symmetry of pattern and homogeneity cash is a challenging task   \n",
       "136351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        skin lesion segmentation using an improved snake model accurate identification of lesion borders is an important task in the analysis of dermoscopy images since the extraction of skin lesion borders provides important cues for accurate diagnosis snakes have been used for segmenting a variety of medical imagery including dermoscopy, however, due to the compromise of internal and external energy forces they can lead to under- or over-segmentation problems in this paper, we introduce a mean shift based gradient vector flow gvf snake algorithm that drives the internal/external energies towards the correct direction the proposed segmentation method incorporates a mean shift operation within the standard gvf cost function experimental results on a large set of diverse dermoscopy images demonstrate that the presented method accurately determines skin lesion borders in dermoscopy images   \n",
       "83254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              learning to detect blue-white structures in dermoscopy images with weak supervision we propose a novel approach to identify one of the most significant dermoscopic criteria in the diagnosis of cutaneous melanoma: the blue-white structure bws in this paper, we achieve this goal in a multiple instance learning mil framework using only image-level labels indicating whether the feature is present or not to this aim, each image is represented as a bag of nonoverlapping regions, where each region may or may not be identified as an instance of bws a probabilistic graphical model is trained in mil fashion to predict the bag image labels as output, we predict the classification label for the image ie, the presence or absence of bws in each image and we also localize the feature in the image experiments are conducted on a challenging dataset with results outperforming state-of-the-art techniques, with bws detection besting competing methods in terms of performance this study provides an improvement on the scope of modeling for computerized image analysis of skin lesions in particular, it propounds a framework for identification of dermoscopic local features from weakly labeled data   \n",
       "85384                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              dense deconvolution net: multi path fusion and dense deconvolution for high resolution skin lesion segmentation dermoscopy imaging has been a routine examination approach for skin lesion diagnosis accurate segmentation is the first step for automatic dermoscopy image assessment   \n",
       "73042                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             a convolutional neural network trained with dermoscopic images performed on par with 145 dermatologists in a clinical melanoma image classification task recent studies have demonstrated the use of convolutional neural networks cnns to classify images of melanoma with accuracies comparable to those achieved by board-certified dermatologists however, the performance of a cnn exclusively trained with dermoscopic images in a clinical image classification task in direct competition with a large number of dermatologists has not been measured to date this study compares the performance of a convolutional neuronal network trained with dermoscopic images exclusively for identifying melanoma in clinical photographs with the manual grading of the same images by dermatologists   \n",
       "27979                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         robustness of convolutional neural networks in recognition of pigmented skin lesions a basic requirement for artificial intelligence ai-based image analysis systems, which are to be integrated into clinical practice, is a high robustness minor changes in how those images are acquired, for example, during routine skin cancer screening, should not change the diagnosis of such assistance systems   \n",
       "3162                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              non-melanoma skin cancer diagnosis: a comparison between dermoscopic and smartphone images by unified visual and sonification deep learning algorithms non-melanoma skin cancer nmsc is the most frequent keratinocyte-origin skin tumor it is confirmed that dermoscopy of nmsc confers a diagnostic advantage as compared to visual face-to-face assessment covid-19 restrictions diagnostics by telemedicine photos, which are analogous to visual inspection, displaced part of in-person visits this study evaluated by a dual convolutional neural network cnn performance metrics in dermoscopic di versus smartphone-captured images si and tested if artificial intelligence narrows the proclaimed gap in diagnostic accuracy   \n",
       "54350                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  multiple skin lesions diagnostics via integrated deep convolutional networks for segmentation and classification computer automated diagnosis of various skin lesions through medical dermoscopy images remains a challenging task   \n",
       "5207                                                                                                                                                                                                                                                                                                                                       data augmentation using adversarial image-to-image translation for the segmentation of mobile-acquired dermatological images dermoscopic images allow the detailed examination of subsurface characteristics of the skin, which led to creating several substantial databases of diverse skin lesions however, the dermoscope is not an easily accessible tool in some regions a less expensive alternative could be acquiring medium resolution clinical macroscopic images of skin lesions however, the limited volume of macroscopic images available, especially mobile-acquired, hinders developing a clinical mobile-based deep learning approach in this work, we present a technique to efficiently utilize the sizable number of dermoscopic images to improve the segmentation capacity of macroscopic skin lesion images a cycle-consistent adversarial network is used to translate the image between the two distinct domains created by the different image acquisition devices a visual inspection was performed on several databases for qualitative evaluation of the results, based on the disappearance and appearance of intrinsic dermoscopic and macroscopic features moreover, the fréchet inception distance was used as a quantitative metric the quantitative segmentation results are demonstrated on the available macroscopic segmentation databases, smartskins and dermofit image library, yielding test set thresholded jaccard index of 8513% and 7430% these results establish a new state-of-the-art performance in the smartskins database   \n",
       "134504                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 generalizing common tasks in automated skin lesion diagnosis we present a general model using supervised learning and map estimation that is capable of performing many common tasks in automated skin lesion diagnosis we apply our model to segment skin lesions, detect occluding hair, and identify the dermoscopic structure pigment network quantitative results are presented for segmentation and hair detection and are competitive when compared to other specialized methods additionally, we leverage the probabilistic nature of the model to produce receiver operating characteristic curves, show compelling visualizations of pigment networks, and provide confidence intervals on segmentations   \n",
       "15617                                                                                                                                          the utrack framework for segmenting and measuring dermatological ulcers through telemedicine chronic dermatological ulcers cause great discomfort to patients, and while monitoring the size of wounds over time provides significant clues about the healing evolution and the clinical condition of patients, the lack of practical applications in existing studies impairs usersaccess to appropriate treatment and diagnosis methods we propose the utrack framework to help with the acquisition of photos, the segmentation and measurement of wounds, the storage of photos and symptoms, and the visualization of the evolution of ulcer healing utrack-app is a mobile app for the framework, which processes images taken by standard mobile device cameras without specialized equipment and stores all data locally the user manually delineates the regions of the wound and the measurement object, and the tool uses the proposed utrack-seg segmentation method to segment them utrack-app also allows users to manually input a unit of measurement centimeter or inch in the image to improve the wound area estimation experiments show that utrack-seg outperforms its state-of-the-art competitors in ulcer segmentation tasks, improving f-measure by up to 825% when compared to superpixel-based approaches and up to 19% when compared to deep learning ones the method is unsupervised, and it semi-automatically segments real-world images with 09 of f-measure, on average the automatic measurement outperformed the manual process in three out of five different rulers utrack-app takes at most 30 s to perform all evaluation steps over high-resolution images, thus being well-suited to analyze ulcers using standard mobile devices   \n",
       "44924                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               the dermoscopic inverse approach significantly improves the accuracy of human readers for lentigo maligna diagnosis a recently introduced dermoscopic method for the diagnosis of early lentigo maligna lm is based on the absence of prevalent patterns of pigmented actinic keratosis and solar lentigo/flat seborrheic keratosis we term this the inverse approach   \n",
       "199                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      the study of usefulness of a set of fractal parameters to build classes of disease units based on images of pigmented skin lesions dermatoscopic images are also increasingly used to train artificial neural networks for the future to provide fully automatic diagnostic systems capable of determining the type of pigmented skin lesion therefore, fractal analysis was used in this study to measure the irregularity of pigmented skin lesion surfaces this paper presents selected results from individual stages of preliminary processing of the dermatoscopic image on pigmented skin lesion, in which fractal analysis was used and referred to the effectiveness of classification by fuzzy or statistical methods classification of the first unsupervised stage was performed using the method of analysis of scatter graphs and the fuzzy method using the kohonen network the results of the kohonen network learning process with an input vector consisting of eight elements prove that neuronal activation requires a larger learning set with greater differentiation for the same training conditions, the final results are at a higher level and can be classified as weaker statistics of factor analysis were proposed, allowing for the reduction in variables, and the directions of further studies were indicated   \n",
       "21211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reducing the impact of confounding factors on skin cancer classification via image segmentation: technical model study studies have shown that artificial intelligence achieves similar or better performance than dermatologists in specific dermoscopic image classification tasks however, artificial intelligence is susceptible to the influence of confounding factors within images eg, skin markings, which can lead to false diagnoses of cancerous skin lesions image segmentation can remove lesion-adjacent confounding factors but greatly change the image representation   \n",
       "109133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            validation of a skin-lesion image-matching algorithm based on computer vision technology melanoma incidence is increasing globally, but consistently accurate skin-lesion classification methods remain elusive we developed a simple software system to classify potentially all types of skin lesions in the current study, we evaluated the systems ability to identify melanomas with a diameter of 10 mm or larger   \n",
       "36651                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                deep convolutional neural network ensembles for multi-classification of skin lesions from dermoscopic and clinical images in this paper, we consider the problem of classifying skin lesions into multiple classes using both dermoscopic and clinical images different convolutional neural network architectures are considered for this task and a novel ensemble scheme is proposed, which makes use of a progressive transfer learning strategy the proposed approach is tested over a dataset of 4000 images containing both dermoscopic and clinical examples and it is shown to achieve an average specificity of 933% and an average sensitivity of 799% in discriminating skin lesions belonging to four different classes   \n",
       "128698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       analysis of clinical and dermoscopic features for basal cell carcinoma neural network classification basal cell carcinoma bcc is the most commonly diagnosed cancer in the usa in this research, we examine four different feature categories used for diagnostic decisions, including patient personal profile patient age, gender, etc, general exam lesion size and location, common dermoscopic blue-gray ovoids, leaf-structure dirt trails, etc, and specific dermoscopic lesion white/pink areas, semitranslucency, etc specific dermoscopic features are more restricted versions of the common dermoscopic features   \n",
       "71268                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         deep learning outperformed 136 of 157 dermatologists in a head-to-head dermoscopic melanoma image classification task recent studies have successfully demonstrated the use of deep-learning algorithms for dermatologist-level classification of suspicious lesions by the use of excessive proprietary image databases and limited numbers of dermatologists for the first time, the performance of a deep-learning algorithm trained by open-source images exclusively is compared to a large number of dermatologists covering all levels within the clinical hierarchy   \n",
       "19583   predicting the clinical management of skin lesions using deep learning automated machine learning approaches to skin lesion diagnosis from images are approaching dermatologist-level performance however, current machine learning approaches that suggest management decisions rely on predicting the underlying skin condition to infer a management decision without considering the variability of management decisions that may exist within a single condition we present the first work to explore image-based prediction of clinical management decisions directly without explicitly predicting the diagnosis in particular, we use clinical and dermoscopic images of skin lesions along with patient metadata from the interactive atlas of dermoscopy dataset 1011 cases; 20 disease labels; 3 management decisions and demonstrate that predicting management labels directly is more accurate than predicting the diagnosis and then inferring the management decision formula: see text and formula: see text improvement in overall accuracy and auroc respectively, statistically significant at formula: see text directly predicting management decisions also considerably reduces the over-excision rate as compared to management decisions inferred from diagnosis predictions 2456% fewer cases wrongly predicted to be excised furthermore, we show that training a model to also simultaneously predict the seven-point criteria and the diagnosis of skin lesions yields an even higher accuracy improvements of formula: see text and formula: see text in overall accuracy and auroc respectively of management predictions finally, we demonstrate our models generalizability by evaluating on the publicly available mclass-d dataset and show that our model agrees with the clinical management recommendations of 157 dermatologists as much as they agree amongst each other   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "161554       0       0        0         0       0        0        0        0   \n",
       "132026       0       0        0         0       0        0        0        0   \n",
       "136351       0       0        0         0       0        0        0        0   \n",
       "83254        0       0        0         0       0        0        0        0   \n",
       "85384        0       0        0         0       0        0        0        0   \n",
       "73042        0       0        0         0       0        0        0        0   \n",
       "27979        0       0        0         0       0        0        0        0   \n",
       "3162         0       0        0         0       0        0        0        0   \n",
       "54350        0       0        0         0       0        0        0        0   \n",
       "5207         0       0        0         0       0        0        0        0   \n",
       "134504       0       0        0         0       0        0        0        0   \n",
       "15617        0       0        0         0       0        0        0        0   \n",
       "44924        0       0        0         0       0        0        0        0   \n",
       "199          0       0        0         0       0        0        0        0   \n",
       "21211        0       0        0         0       0        0        0        0   \n",
       "109133       0       0        0         0       0        0        0        0   \n",
       "36651        0       0        0         0       0        0        0        0   \n",
       "128698       0       0        0         0       0        0        0        0   \n",
       "71268        0       0        0         0       0        0        0        0   \n",
       "19583        0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text mamm_text endo_text derm_text  \n",
       "161554          0        0         0         0         1  \n",
       "132026          0        0         0         0         1  \n",
       "136351          0        0         0         0         1  \n",
       "83254           0        0         0         0         1  \n",
       "85384           0        0         0         0         1  \n",
       "73042           0        0         0         0         1  \n",
       "27979           0        0         0         0         1  \n",
       "3162            0        0         0         0         1  \n",
       "54350           0        0         0         0         1  \n",
       "5207            0        0         0         0         1  \n",
       "134504          0        0         0         0         1  \n",
       "15617           0        0         0         0         1  \n",
       "44924           0        0         0         0         1  \n",
       "199             0        0         0         0         1  \n",
       "21211           0        0         0         0         1  \n",
       "109133          0        0         0         0         1  \n",
       "36651           0        0         0         0         1  \n",
       "128698          0        0         0         0         1  \n",
       "71268           0        0         0         0         1  \n",
       "19583           0        0         0         0         1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['derm_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8503ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32439, '1': 1740})\n"
     ]
    }
   ],
   "source": [
    "## GENOMIC\n",
    "\n",
    "## text\n",
    "text = ['candidate gene', 'prognostic gene', ' gene ', ' genes ', ' dna ', ' rna ']\n",
    "\n",
    "feat['gene_text'] = np.where(groups['text'].str.contains('genomic'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['gene_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['gene_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['gene_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a231099f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "      <th>mamm_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>gene_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26797</th>\n",
       "      <td>prediction and interpretation of cancer survival using graph convolution neural networks the survival rate of cancer has increased significantly during the past two decades for breast, prostate, testicular, and colon cancer, while the brain and pancreatic cancers have a much lower median survival rate that has not improved much over the last forty years this has imposed the challenge of finding gene markers for early cancer detection and treatment strategies different methods including regression-based cox-ph, artificial neural networks, and recently deep learning algorithms have been proposed to predict the survival rate for cancers we established in this work a novel graph convolution neural network gcnn approach called surv_gcnn to predict the survival rate for 13 different cancer types using the tcga dataset for each cancer type, 6 surv_gcnn models with graphs generated by correlation analysis, genemania database, and correlation + genemania were trained with and without clinical data to predict the risk score rs the performance of the 6 surv_gcnn models was compared with two other existing models, cox-ph and cox-nnet the results showed that cox-ph has the worst performance among 8 tested models across the 13 cancer types while surv_gcnn models with clinical data reported the best overall performance, outperforming other competing models in 7 out of 13 cancer types including blca, brca, coad, lusc, sarc, stad, and ucec a novel network-based interpretation of surv_gcnn was also proposed to identify potential gene markers for breast cancer the signatures learned by the nodes in the hidden layer of surv_gcnn were identified and were linked to potential gene markers by network modularization the identified gene markers for breast cancer have been compared to a total of 213 gene markers from three widely cited lists for breast cancer survival analysis about 57% of gene markers obtained by surv_gcnn with correlation + genemania graph either overlap or directly interact with the 213 genes, confirming the effectiveness of the identified markers by surv_gcnn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29326</th>\n",
       "      <td>a genomic-pathologic annotated risk model to predict recurrence in early-stage lung adenocarcinoma recommendations for adjuvant therapy after surgical resection of lung adenocarcinoma luad are based solely on tnm classification but are agnostic to genomic and high-risk clinicopathologic factors creation of a prediction model that integrates tumor genomic and clinicopathologic factors may better identify patients at risk for recurrence</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114138</th>\n",
       "      <td>a new 25d representation for lymph node detection using random sets of deep convolutional neural network observations automated lymph node ln detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in computed tomography ct and to their varying sizes, poses, shapes and sparsely distributed locations state-of-the-art studies show the performance range of 529% sensitivity at 31 false-positives per volume fp/vol, or 609% at 61 fp/vol for mediastinal ln, by one-shot boosting on 3d haar features in this paper, we first operate a preliminary candidate generation stage, towards -100% sensitivity at the cost of high fp levels -40 per patient, to harvest volumes of interest voi our 25d approach consequently decomposes any 3d voi by resampling 2d reformatted orthogonal views n times, via scale, random translations, and rotations with respect to the voi centroid coordinates these random views are then used to train a deep convolutional neural network cnn classifier in testing, the cnn is employed to assign ln probabilities for all n random views that can be simply averaged as a set to compute the final classification probability per voi we validate the approach on two datasets: 90 ct volumes with 388 mediastinal lns and 86 patients with 595 abdominal lns we achieve sensitivities of 70%/83% at 3 fp/vol and 84%/90% at 6 fp/vol in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51360</th>\n",
       "      <td>transfer learning with convolutional neural networks for cancer survival prediction using gene-expression data precision medicine in oncology aims at obtaining data from heterogeneous sources to have a precise estimation of a given patients state and prognosis with the purpose of advancing to personalized medicine framework, accurate diagnoses allow prescription of more effective treatments adapted to the specificities of each individual case in the last years, next-generation sequencing has impelled cancer research by providing physicians with an overwhelming amount of gene-expression data from rna-seq high-throughput platforms in this scenario, data mining and machine learning techniques have widely contribute to gene-expression data analysis by supplying computational models to supporting decision-making on real-world data nevertheless, existing public gene-expression databases are characterized by the unfavorable imbalance between the huge number of genes in the order of tenths of thousands and the small number of samples in the order of a few hundreds available despite diverse feature selection and extraction strategies have been traditionally applied to surpass derived over-fitting issues, the efficacy of standard machine learning pipelines is far from being satisfactory for the prediction of relevant clinical outcomes like follow-up end-points or patients survival using the public pan-cancer dataset, in this study we pre-train convolutional neural network architectures for survival prediction on a subset composed of thousands of gene-expression samples from thirty-one tumor types the resulting architectures are subsequently fine-tuned to predict lung cancer progression-free interval the application of convolutional networks to gene-expression data has many limitations, derived from the unstructured nature of these data in this work we propose a methodology to rearrange rna-seq data by transforming rna-seq samples into gene-expression images, from which convolutional networks can extract high-level features as an additional objective, we investigate whether leveraging the information extracted from other tumor-type samples contributes to the extraction of high-level features that improve lung cancer progression prediction, compared to other machine learning approaches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44111</th>\n",
       "      <td>comparison of unsupervised machine-learning methods to identify metabolomic signatures in patients with localized breast cancer genomics and transcriptomics have led to the widely-used molecular classification of breast cancer bc however, heterogeneous biological behaviors persist within breast cancer subtypes metabolomics is a rapidly-expanding field of study dedicated to cellular metabolisms affected by the environment the aim of this study was to compare metabolomic signatures of bc obtained by 5 different unsupervised machine learning ml methods fifty-two consecutive patients with bc with an indication for adjuvant chemotherapy between 2013 and 2016 were retrospectively included we performed metabolomic profiling of tumor resection samples using liquid chromatography-mass spectrometry here, four hundred and forty-nine identified metabolites were selected for further analysis clusters obtained using 5 unsupervised ml methods pca k-means, sparse k-means, spectral clustering, simlr and k-sparse were compared in terms of clinical and biological characteristics with an optimal partitioning parameter k = 3, the five methods identified three prognosis groups of patients favorable, intermediate, unfavorable with different clinical and biological profiles simlr and k-sparse methods were the most effective techniques in terms of clustering &lt;i&gt;in-silico&lt;/i&gt; survival analysis revealed a significant difference for 5-year predicted os between the 3 clusters further pathway analysis using the 449 selected metabolites showed significant differences in amino acid and glucose metabolism between bc histologic subtypes our results provide proof-of-concept for the use of unsupervised ml metabolomics enabling stratification and personalized management of bc patients the design of novel computational methods incorporating ml and bioinformatics techniques should make available tools particularly suited to improving the outcome of cancer treatment and reducing cancer-related mortalities</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77046</th>\n",
       "      <td>laplacian regularized low-rank representation for cancer samples clustering cancer samples clustering based on biomolecular data has been becoming an important tool for cancer classification the recognition of cancer types is of great importance for cancer treatment in this paper, in order to improve the accuracy of cancer recognition, we propose to use laplacian regularized low-rank representation llrr to cluster the cancer samples based on genomic data in llrr method, the high-dimensional genomic data are approximately treated as samples extracted from a combination of several low-rank subspaces the purpose of llrr method is to seek the lowest-rank representation matrix based on a dictionary because a laplacian regularization based on manifold is introduced into llrr, compared to the low-rank representation lrr method, besides capturing the global geometric structure, llrr can capture the intrinsic local structure of high-dimensional observation data well and what is more, in llrr, the original data themselves are selected as a dictionary, so the lowest-rank representation is actually a similar expression between the samples therefore, corresponding to the low-rank representation matrix, the samples with high similarity are considered to come from the same subspace and are grouped into a class the experiment results on real genomic data illustrate that llrr method, compared with lrr and mllrr, is more robust to noise and has a better ability to learn the inherent subspace structure of data, and achieves remarkable performance in the clustering of cancer samples</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27449</th>\n",
       "      <td>updates in using a molecular classifier to identify usual interstitial pneumonia in conventional transbronchial lung biopsy samples &lt;b&gt;a molecular classifier using a machine-learning algorithm based on genomic data could provide an objective method to aid clinicians and multidisciplinary teams to establish the diagnosis of ipf in less-invasive transbronchial lung biopsy samples&lt;/b&gt; https://bitly/2qldwim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163389</th>\n",
       "      <td>tree-structured supervised learning and the genetics of hypertension this paper is about an algorithm, flextree, for general supervised learning it extends the binary tree-structured approach classification and regression trees, cart although it differs greatly in its selection and combination of predictors it is particularly applicable to assessing interactions: gene by gene and gene by environment as they bear on complex disease one model for predisposition to complex disease involves many genes of them, most are pure noise; each of the values that is not the prevalent genotype for the minority of genes that contribute to the signal carries a score scores add individuals with scores above an unknown threshold are predisposed to the disease for the additive score problem and simulated data, flextree has cross-validated risk better than many cutting-edge technologies to which it was compared when small fractions of candidate genes carry the signal for the model where only a precise list of aberrant genotypes is predisposing, there is not a systematic pattern of absolute superiority; however, overall, flextree seems better than the other technologies we tried the algorithm on data from 563 chinese women, 206 hypotensive, 357 hypertensive, with information on ethnicity, menopausal status, insulin-resistant status, and 21 loci flextree and logic regression appear better than the others in terms of bayes risk however, the differences are not significant in the usual statistical sense</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20388</th>\n",
       "      <td>gated graph attention network for cancer prediction with its increasing incidence, cancer has become one of the main causes of worldwide mortality in this work, we mainly propose a novel attention-based neural network model named gated graph attention network ggat for cancer prediction, where a gating mechanism gm is introduced to work with the attention mechanism am, to break through the previous works limitation of 1-hop neighbourhood reasoning in this way, our ggat is capable of fully mining the potential correlation between related samples, helping for improving the cancer prediction accuracy additionally, to simplify the datasets, we propose a hybrid feature selection algorithm to strictly select gene features, which significantly reduces training time without affecting prediction accuracy to the best of our knowledge, our proposed ggat achieves the state-of-the-art results in cancer prediction task on lihc, luad, kirc compared to other traditional machine learning methods and neural network models, and improves the accuracy by 1% to 2% on cora dataset, compared to the state-of-the-art graph neural network methods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49609</th>\n",
       "      <td>a novel microrna signature for pathological grading in lung adenocarcinoma based on tcga and geo data lung adenocarcinoma luad is one of the most common types of lung cancer and its poor prognosis largely depends on the tumor pathological stage critical roles of micrornas mirnas have been reported in the tumorigenesis and progression of lung cancer however, whether the differential expression pattern of mirnas could be used to distinguish early‑stage stage i from mid‑late‑stage stages ii‑iv luad tumors is still unclear in this study, clinical information and mirna expression profiles of patients with luad were downloaded from the cancer genome atlas tcga and gene expression omnibus databases tcga‑luad n=470 dataset was used for model training and validation, and the gse62182 n=94 and gse83527 n=36 datasets were used as external independent test datasets the diagnostic model was created through mirna feature selection followed by svm classifier and was confirmed by 5‑fold cross‑validation a receiver operating characteristic curve was calculated to evaluate the accuracy and robustness of the model using the dx score and libsvm tool, a 16‑mirna signature that could distinguish luad pathological stages was identified the area under the curve rates were 062 95% confidence interval ci: 056‑067, 066 95% ci: 054‑076 and 063 95% ci: 043‑082 in tcga‑luad internal validation dataset, the gse62182 external validation dataset, and the gse83527 external validation dataset, respectively kyoto encyclopedia of genes and genomes and gene ontology enrichment analyses suggested that the target genes of the 16‑mirna signature were mainly involved in metabolic pathways the present findings demonstrate that a 16‑mirna signature could serve as a promising diagnostic biomarker for pathological staging in luad</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8571</th>\n",
       "      <td>machine learning predicts treatment sensitivity in multiple myeloma based on molecular and clinical information coupled with drug response providing treatment sensitivity stratification at the time of cancer diagnosis allows better allocation of patients to alternative treatment options despite many clinical and biological risk markers having been associated with variable survival in cancer, assessing the interplay of these markers through machine learning ml algorithms still remains to be fully explored here, we present a multi learning training approach mult combining supervised, unsupervised and self-supervised learning algorithms, to examine the predictive value of heterogeneous treatment outcomes for multiple myeloma mm we show that gene expression values improve the treatment sensitivity prediction and recapitulates genetic abnormalities detected by fluorescence in situ hybridization fish testing mult performance was assessed by cross-validation experiments, in which it predicted treatment sensitivity with 6870% of auc finally, simulations showed numerical evidences that in average 1707% of patients could get better response to a different treatment at the first line</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47192</th>\n",
       "      <td>metabolomics, machine learning and immunohistochemistry to predict succinate dehydrogenase mutational status in phaeochromocytomas and paragangliomas phaeochromocytomas and paragangliomas ppgls are rare neuroendocrine tumours with a hereditary background in over one-third of patients mutations in succinate dehydrogenase sdh genes increase the risk for ppgls and several other tumours mutations in subunit b sdhb in particular are a risk factor for metastatic disease, further highlighting the importance of identifying sdhx mutations for patient management genetic variants of unknown significance, where implications for the patient and family members are unclear, are a problem for interpretation for such cases, reliable methods for evaluating protein functionality are required immunohistochemistry for sdhb sdhb-ihc is the method of choice but does not assess functionality at the enzymatic level liquid chromatography-mass spectrometry-based measurements of metabolite precursors and products of enzymatic reactions provide an alternative method here, we compare sdhb-ihc with metabolite profiling in 189 tumours from 187 ppgl patients besides evaluating succinate:fumarate ratios sfrs, machine learning algorithms were developed to establish predictive models for interpreting metabolite data metabolite profiling showed higher diagnostic specificity compared to sdhb-ihc 992% versus 925%, p = 0021, whereas sensitivity was comparable application of machine learning algorithms to metabolite profiles improved predictive ability over that of the sfr, in particular for hard-to-interpret cases of head and neck paragangliomas auc 09821 versus 09613, p = 0044 importantly, the combination of metabolite profiling with sdhb-ihc has complementary utility, as sdhb-ihc correctly classified all but one of the false negatives from metabolite profiling strategies, while metabolite profiling correctly classified all but one of the false negatives/positives from sdhb-ihc from 186 tumours with confirmed status of sdhx variant pathogenicity, the combination of the two methods resulted in 185 correct predictions, highlighting the benefits of both strategies for patient management © 2020 the authors the journal of pathology published by john wiley &amp; sons ltd on behalf of pathological society of great britain and ireland</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110856</th>\n",
       "      <td>fuzzy logic selection as a new reliable tool to identify molecular grade signatures in breast cancer--the innodiag study personalized medicine has become a priority in breast cancer patient management in addition to the routinely used clinicopathological characteristics, clinicians will have to face an increasing amount of data derived from tumor molecular profiling the aims of this study were to develop a new gene selection method based on a fuzzy logic selection and classification algorithm, and to validate the gene signatures obtained on breast cancer patient cohorts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109108</th>\n",
       "      <td>a comprehensive computer-aided polyp detection system for colonoscopy videos computer-aided detection cad can help colonoscopists reduce their polyp miss-rate, but existing cad systems are handicapped by using either shape, texture, or temporal information for detecting polyps, achieving limited sensitivity and specificity to overcome this limitation, our key contribution of this paper is to fuse all possible polyp features by exploiting the strengths of each feature while minimizing its weaknesses our new cad system has two stages, where the first stage builds on the robustness of shape features to reliably generate a set of candidates with a high sensitivity, while the second stage utilizes the high discriminative power of the computationally expensive features to effectively reduce false positives specifically, we employ a unique edge classifier and an original voting scheme to capture geometric features of polyps in context and then harness the power of convolutional neural networks in a novel score fusion approach to extract and combine shape, color, texture, and temporal information of the candidates our experimental results based on froc curves and a new analysis of polyp detection latency demonstrate a superiority over the state-of-the-art where our system yields a lower polyp detection latency and achieves a significantly higher sensitivity while generating dramatically fewer false positives this performance improvement is attributed to our reliable candidate generation and effective false positive reduction methods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44411</th>\n",
       "      <td>arrhythmic gut microbiome signatures predict risk of type 2 diabetes lifestyle, obesity, and the gut microbiome are important risk factors for metabolic disorders we demonstrate in 1,976 subjects of a german population cohort kora that specific microbiota members show 24-h oscillations in their relative abundance and identified 13 taxa with disrupted rhythmicity in type 2 diabetes t2d cross-validated prediction models based on this signature similarly classified t2d in an independent cohort focus, disruption of microbial oscillation and the model for t2d classification was confirmed in 1,363 subjects this arrhythmic risk signature was able to predict t2d in 699 kora subjects 5 years after initial sampling, being most effective in combination with bmi shotgun metagenomic analysis functionally linked 26 metabolic pathways to the diurnal oscillation of gut bacteria thus, a cohort-specific risk pattern of arrhythmic taxa enables classification and prediction of t2d, suggesting a functional link between circadian rhythms and the microbiome in metabolic diseases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73347</th>\n",
       "      <td>time to treatment prediction in chronic lymphocytic leukemia based on new transcriptional patterns chronic lymphocytic leukemia cll is the most frequent lymphoproliferative syndrome in western countries cll evolution is frequently indolent, and treatment is mostly reserved for those patients with signs or symptoms of disease progression in this work, we used rna sequencing data from the international cancer genome consortium cll cohort to determine new gene expression patterns that correlate with clinical evolutionwe determined that a 290-gene expression signature, in addition to immunoglobulin heavy chain variable region &lt;i&gt;ighv&lt;/i&gt; mutation status, stratifies patients into four groups with notably different time to first treatment this finding was confirmed in an independent cohort similarly, we present a machine learning algorithm that predicts the need for treatment within the first 5 years following diagnosis using expression data from 2,198 genes this predictor achieved 90% precision and 89% accuracy when classifying independent cll cases our findings indicate that cll progression risk largely correlates with particular transcriptomic patterns and paves the way for the identification of high-risk patients who might benefit from prompt therapy following diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71454</th>\n",
       "      <td>epigenetic classifiers for precision diagnosis of brain tumors dna methylation profiling has proven to be a powerful analytical tool, which can accurately identify the tissue of origin of a wide range of benign and malignant neoplasms using microarray-based profiling and supervised machine learning algorithms, we and other groups have recently unraveled dna methylation signatures capable of aiding the histomolecular diagnosis of different tumor types we have explored the methylomes of metastatic brain tumors from patients with lung cancer, breast cancer, and cutaneous melanoma and primary brain neoplasms to build epigenetic classifiers our brain metastasis methylation brainmeth classifier has the ability to determine the type of brain tumor, the origin of the metastases, and the clinical-therapeutic subtype for patients with breast cancer brain metastases to facilitate the translation of these epigenetic classifiers into clinical practice, we selected and validated the most informative genomic regions utilizing quantitative methylation-specific polymerase chain reaction qmsp we believe that the refinement, expansion, integration, and clinical validation of brainmeth and other recently developed epigenetic classifiers will significantly contribute to the development of more comprehensive and accurate systems for the personalized management of patients with brain metastases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133385</th>\n",
       "      <td>survival ensembles by the sum of pairwise differences with application to lung cancer microarray studies lung cancer is among the most common cancers in the united states, in terms of incidence and mortality in 2009, it is estimated that more than 150,000 deaths will result from lung cancer alone genetic information is an extremely valuable data source in characterizing the personal nature of cancer over the past several years, investigators have conducted numerous association studies where intensive genetic data is collected on relatively few patients compared to the numbers of gene predictors, with one scientific goal being to identify genetic features associated with cancer recurrence or survival in this note, we propose high-dimensional survival analysis through a new application of boosting, a powerful tool in machine learning our approach is based on an accelerated lifetime model and minimizing the sum of pairwise differences in residuals we apply our method to a recent microarray study of lung adenocarcinoma and find that our ensemble is composed of 19 genes while a proportional hazards ph ensemble is composed of nine genes, a proper subset of the 19-gene panel in one of our simulation scenarios, we demonstrate that ph boosting in a misspecified model tends to underfit and ignore moderately-sized covariate effects, on average diagnostic analyses suggest that the ph assumption is not satisfied in the microarray data and may explain, in part, the discrepancy in the sets of active coefficients our simulation studies and comparative data analyses demonstrate how statistical learning by ph models alone is insufficient</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102203</th>\n",
       "      <td>phenotypic characterization of breast invasive carcinoma via transferable tissue morphometric patterns learned from glioblastoma multiforme quantitative analysis of whole slide images wsis in a large cohort may provide predictive models of clinical outcome however, the performance of the existing techniques is hindered as a result of large technical variations eg, fixation, staining and biological heterogeneities eg, cell type, cell state that are always present in a large cohort although unsupervised feature learning provides a promising way in learning pertinent features without human intervention, its capability can be greatly limited due to the lack of well-curated examples in this paper, we explored the transferability of knowledge acquired from a well-curated glioblastoma multiforme gbm dataset through its application to the representation and characterization of tissue histology from the cancer genome atlas tcga breast invasive carcinoma brca cohort our experimental results reveals two major phenotypic subtypes with statistically significantly different survival curves further differential expression analysis of these two subtypes indicates enrichment of genes regulated by nf-kb in response to tnf and genes up-regulated in response to ifng</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54325</th>\n",
       "      <td>deepbts: prediction of recurrence-free survival of non-small cell lung cancer using a time-binned deep neural network accurate prediction of non-small cell lung cancer nsclc prognosis after surgery remains challenging the cox proportional hazard ph model is widely used, however, there are some limitations associated with it in this study, we developed novel neural network models called binned time survival analysis deepbts models using 30 clinico-pathological features of surgically resected nsclc patients training cohort, n = 1,022; external validation cohort, n = 298 we employed the root-mean-square error in the supervised learning model, s- deepbts or negative log-likelihood in the semi-unsupervised learning model, su-deepbts as the loss function the su-deepbts algorithm achieved better performance c-index = 07306; auc = 07677 than the other models cox ph: c-index = 07048 and auc = 07390; s-deepbts: c-index = 07126 and auc = 07420 the top 14 features were selected using su-deepbts model as a selector and could distinguish the low- and high-risk groups in the training cohort p = 186 × 10&lt;sup&gt;-11&lt;/sup&gt; and validation cohort p = 104 × 10&lt;sup&gt;-10&lt;/sup&gt; when trained with the optimal feature set for each model, the su-deepbts model could predict the prognoses of nsclc better than the traditional model, especially in stage i patients follow-up studies using combined radiological, pathological imaging, and genomic data to enhance the performance of our model are ongoing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
       "26797                                                                                                                                                                                                                                               prediction and interpretation of cancer survival using graph convolution neural networks the survival rate of cancer has increased significantly during the past two decades for breast, prostate, testicular, and colon cancer, while the brain and pancreatic cancers have a much lower median survival rate that has not improved much over the last forty years this has imposed the challenge of finding gene markers for early cancer detection and treatment strategies different methods including regression-based cox-ph, artificial neural networks, and recently deep learning algorithms have been proposed to predict the survival rate for cancers we established in this work a novel graph convolution neural network gcnn approach called surv_gcnn to predict the survival rate for 13 different cancer types using the tcga dataset for each cancer type, 6 surv_gcnn models with graphs generated by correlation analysis, genemania database, and correlation + genemania were trained with and without clinical data to predict the risk score rs the performance of the 6 surv_gcnn models was compared with two other existing models, cox-ph and cox-nnet the results showed that cox-ph has the worst performance among 8 tested models across the 13 cancer types while surv_gcnn models with clinical data reported the best overall performance, outperforming other competing models in 7 out of 13 cancer types including blca, brca, coad, lusc, sarc, stad, and ucec a novel network-based interpretation of surv_gcnn was also proposed to identify potential gene markers for breast cancer the signatures learned by the nodes in the hidden layer of surv_gcnn were identified and were linked to potential gene markers by network modularization the identified gene markers for breast cancer have been compared to a total of 213 gene markers from three widely cited lists for breast cancer survival analysis about 57% of gene markers obtained by surv_gcnn with correlation + genemania graph either overlap or directly interact with the 213 genes, confirming the effectiveness of the identified markers by surv_gcnn   \n",
       "29326                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  a genomic-pathologic annotated risk model to predict recurrence in early-stage lung adenocarcinoma recommendations for adjuvant therapy after surgical resection of lung adenocarcinoma luad are based solely on tnm classification but are agnostic to genomic and high-risk clinicopathologic factors creation of a prediction model that integrates tumor genomic and clinicopathologic factors may better identify patients at risk for recurrence   \n",
       "114138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  a new 25d representation for lymph node detection using random sets of deep convolutional neural network observations automated lymph node ln detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in computed tomography ct and to their varying sizes, poses, shapes and sparsely distributed locations state-of-the-art studies show the performance range of 529% sensitivity at 31 false-positives per volume fp/vol, or 609% at 61 fp/vol for mediastinal ln, by one-shot boosting on 3d haar features in this paper, we first operate a preliminary candidate generation stage, towards -100% sensitivity at the cost of high fp levels -40 per patient, to harvest volumes of interest voi our 25d approach consequently decomposes any 3d voi by resampling 2d reformatted orthogonal views n times, via scale, random translations, and rotations with respect to the voi centroid coordinates these random views are then used to train a deep convolutional neural network cnn classifier in testing, the cnn is employed to assign ln probabilities for all n random views that can be simply averaged as a set to compute the final classification probability per voi we validate the approach on two datasets: 90 ct volumes with 388 mediastinal lns and 86 patients with 595 abdominal lns we achieve sensitivities of 70%/83% at 3 fp/vol and 84%/90% at 6 fp/vol in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work   \n",
       "51360              transfer learning with convolutional neural networks for cancer survival prediction using gene-expression data precision medicine in oncology aims at obtaining data from heterogeneous sources to have a precise estimation of a given patients state and prognosis with the purpose of advancing to personalized medicine framework, accurate diagnoses allow prescription of more effective treatments adapted to the specificities of each individual case in the last years, next-generation sequencing has impelled cancer research by providing physicians with an overwhelming amount of gene-expression data from rna-seq high-throughput platforms in this scenario, data mining and machine learning techniques have widely contribute to gene-expression data analysis by supplying computational models to supporting decision-making on real-world data nevertheless, existing public gene-expression databases are characterized by the unfavorable imbalance between the huge number of genes in the order of tenths of thousands and the small number of samples in the order of a few hundreds available despite diverse feature selection and extraction strategies have been traditionally applied to surpass derived over-fitting issues, the efficacy of standard machine learning pipelines is far from being satisfactory for the prediction of relevant clinical outcomes like follow-up end-points or patients survival using the public pan-cancer dataset, in this study we pre-train convolutional neural network architectures for survival prediction on a subset composed of thousands of gene-expression samples from thirty-one tumor types the resulting architectures are subsequently fine-tuned to predict lung cancer progression-free interval the application of convolutional networks to gene-expression data has many limitations, derived from the unstructured nature of these data in this work we propose a methodology to rearrange rna-seq data by transforming rna-seq samples into gene-expression images, from which convolutional networks can extract high-level features as an additional objective, we investigate whether leveraging the information extracted from other tumor-type samples contributes to the extraction of high-level features that improve lung cancer progression prediction, compared to other machine learning approaches   \n",
       "44111                                                                                                                                                                                                                                                                                                                                        comparison of unsupervised machine-learning methods to identify metabolomic signatures in patients with localized breast cancer genomics and transcriptomics have led to the widely-used molecular classification of breast cancer bc however, heterogeneous biological behaviors persist within breast cancer subtypes metabolomics is a rapidly-expanding field of study dedicated to cellular metabolisms affected by the environment the aim of this study was to compare metabolomic signatures of bc obtained by 5 different unsupervised machine learning ml methods fifty-two consecutive patients with bc with an indication for adjuvant chemotherapy between 2013 and 2016 were retrospectively included we performed metabolomic profiling of tumor resection samples using liquid chromatography-mass spectrometry here, four hundred and forty-nine identified metabolites were selected for further analysis clusters obtained using 5 unsupervised ml methods pca k-means, sparse k-means, spectral clustering, simlr and k-sparse were compared in terms of clinical and biological characteristics with an optimal partitioning parameter k = 3, the five methods identified three prognosis groups of patients favorable, intermediate, unfavorable with different clinical and biological profiles simlr and k-sparse methods were the most effective techniques in terms of clustering <i>in-silico</i> survival analysis revealed a significant difference for 5-year predicted os between the 3 clusters further pathway analysis using the 449 selected metabolites showed significant differences in amino acid and glucose metabolism between bc histologic subtypes our results provide proof-of-concept for the use of unsupervised ml metabolomics enabling stratification and personalized management of bc patients the design of novel computational methods incorporating ml and bioinformatics techniques should make available tools particularly suited to improving the outcome of cancer treatment and reducing cancer-related mortalities   \n",
       "77046                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   laplacian regularized low-rank representation for cancer samples clustering cancer samples clustering based on biomolecular data has been becoming an important tool for cancer classification the recognition of cancer types is of great importance for cancer treatment in this paper, in order to improve the accuracy of cancer recognition, we propose to use laplacian regularized low-rank representation llrr to cluster the cancer samples based on genomic data in llrr method, the high-dimensional genomic data are approximately treated as samples extracted from a combination of several low-rank subspaces the purpose of llrr method is to seek the lowest-rank representation matrix based on a dictionary because a laplacian regularization based on manifold is introduced into llrr, compared to the low-rank representation lrr method, besides capturing the global geometric structure, llrr can capture the intrinsic local structure of high-dimensional observation data well and what is more, in llrr, the original data themselves are selected as a dictionary, so the lowest-rank representation is actually a similar expression between the samples therefore, corresponding to the low-rank representation matrix, the samples with high similarity are considered to come from the same subspace and are grouped into a class the experiment results on real genomic data illustrate that llrr method, compared with lrr and mllrr, is more robust to noise and has a better ability to learn the inherent subspace structure of data, and achieves remarkable performance in the clustering of cancer samples   \n",
       "27449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  updates in using a molecular classifier to identify usual interstitial pneumonia in conventional transbronchial lung biopsy samples <b>a molecular classifier using a machine-learning algorithm based on genomic data could provide an objective method to aid clinicians and multidisciplinary teams to establish the diagnosis of ipf in less-invasive transbronchial lung biopsy samples</b> https://bitly/2qldwim   \n",
       "163389                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       tree-structured supervised learning and the genetics of hypertension this paper is about an algorithm, flextree, for general supervised learning it extends the binary tree-structured approach classification and regression trees, cart although it differs greatly in its selection and combination of predictors it is particularly applicable to assessing interactions: gene by gene and gene by environment as they bear on complex disease one model for predisposition to complex disease involves many genes of them, most are pure noise; each of the values that is not the prevalent genotype for the minority of genes that contribute to the signal carries a score scores add individuals with scores above an unknown threshold are predisposed to the disease for the additive score problem and simulated data, flextree has cross-validated risk better than many cutting-edge technologies to which it was compared when small fractions of candidate genes carry the signal for the model where only a precise list of aberrant genotypes is predisposing, there is not a systematic pattern of absolute superiority; however, overall, flextree seems better than the other technologies we tried the algorithm on data from 563 chinese women, 206 hypotensive, 357 hypertensive, with information on ethnicity, menopausal status, insulin-resistant status, and 21 loci flextree and logic regression appear better than the others in terms of bayes risk however, the differences are not significant in the usual statistical sense   \n",
       "20388                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        gated graph attention network for cancer prediction with its increasing incidence, cancer has become one of the main causes of worldwide mortality in this work, we mainly propose a novel attention-based neural network model named gated graph attention network ggat for cancer prediction, where a gating mechanism gm is introduced to work with the attention mechanism am, to break through the previous works limitation of 1-hop neighbourhood reasoning in this way, our ggat is capable of fully mining the potential correlation between related samples, helping for improving the cancer prediction accuracy additionally, to simplify the datasets, we propose a hybrid feature selection algorithm to strictly select gene features, which significantly reduces training time without affecting prediction accuracy to the best of our knowledge, our proposed ggat achieves the state-of-the-art results in cancer prediction task on lihc, luad, kirc compared to other traditional machine learning methods and neural network models, and improves the accuracy by 1% to 2% on cora dataset, compared to the state-of-the-art graph neural network methods   \n",
       "49609                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 a novel microrna signature for pathological grading in lung adenocarcinoma based on tcga and geo data lung adenocarcinoma luad is one of the most common types of lung cancer and its poor prognosis largely depends on the tumor pathological stage critical roles of micrornas mirnas have been reported in the tumorigenesis and progression of lung cancer however, whether the differential expression pattern of mirnas could be used to distinguish early‑stage stage i from mid‑late‑stage stages ii‑iv luad tumors is still unclear in this study, clinical information and mirna expression profiles of patients with luad were downloaded from the cancer genome atlas tcga and gene expression omnibus databases tcga‑luad n=470 dataset was used for model training and validation, and the gse62182 n=94 and gse83527 n=36 datasets were used as external independent test datasets the diagnostic model was created through mirna feature selection followed by svm classifier and was confirmed by 5‑fold cross‑validation a receiver operating characteristic curve was calculated to evaluate the accuracy and robustness of the model using the dx score and libsvm tool, a 16‑mirna signature that could distinguish luad pathological stages was identified the area under the curve rates were 062 95% confidence interval ci: 056‑067, 066 95% ci: 054‑076 and 063 95% ci: 043‑082 in tcga‑luad internal validation dataset, the gse62182 external validation dataset, and the gse83527 external validation dataset, respectively kyoto encyclopedia of genes and genomes and gene ontology enrichment analyses suggested that the target genes of the 16‑mirna signature were mainly involved in metabolic pathways the present findings demonstrate that a 16‑mirna signature could serve as a promising diagnostic biomarker for pathological staging in luad   \n",
       "8571                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  machine learning predicts treatment sensitivity in multiple myeloma based on molecular and clinical information coupled with drug response providing treatment sensitivity stratification at the time of cancer diagnosis allows better allocation of patients to alternative treatment options despite many clinical and biological risk markers having been associated with variable survival in cancer, assessing the interplay of these markers through machine learning ml algorithms still remains to be fully explored here, we present a multi learning training approach mult combining supervised, unsupervised and self-supervised learning algorithms, to examine the predictive value of heterogeneous treatment outcomes for multiple myeloma mm we show that gene expression values improve the treatment sensitivity prediction and recapitulates genetic abnormalities detected by fluorescence in situ hybridization fish testing mult performance was assessed by cross-validation experiments, in which it predicted treatment sensitivity with 6870% of auc finally, simulations showed numerical evidences that in average 1707% of patients could get better response to a different treatment at the first line   \n",
       "47192   metabolomics, machine learning and immunohistochemistry to predict succinate dehydrogenase mutational status in phaeochromocytomas and paragangliomas phaeochromocytomas and paragangliomas ppgls are rare neuroendocrine tumours with a hereditary background in over one-third of patients mutations in succinate dehydrogenase sdh genes increase the risk for ppgls and several other tumours mutations in subunit b sdhb in particular are a risk factor for metastatic disease, further highlighting the importance of identifying sdhx mutations for patient management genetic variants of unknown significance, where implications for the patient and family members are unclear, are a problem for interpretation for such cases, reliable methods for evaluating protein functionality are required immunohistochemistry for sdhb sdhb-ihc is the method of choice but does not assess functionality at the enzymatic level liquid chromatography-mass spectrometry-based measurements of metabolite precursors and products of enzymatic reactions provide an alternative method here, we compare sdhb-ihc with metabolite profiling in 189 tumours from 187 ppgl patients besides evaluating succinate:fumarate ratios sfrs, machine learning algorithms were developed to establish predictive models for interpreting metabolite data metabolite profiling showed higher diagnostic specificity compared to sdhb-ihc 992% versus 925%, p = 0021, whereas sensitivity was comparable application of machine learning algorithms to metabolite profiles improved predictive ability over that of the sfr, in particular for hard-to-interpret cases of head and neck paragangliomas auc 09821 versus 09613, p = 0044 importantly, the combination of metabolite profiling with sdhb-ihc has complementary utility, as sdhb-ihc correctly classified all but one of the false negatives from metabolite profiling strategies, while metabolite profiling correctly classified all but one of the false negatives/positives from sdhb-ihc from 186 tumours with confirmed status of sdhx variant pathogenicity, the combination of the two methods resulted in 185 correct predictions, highlighting the benefits of both strategies for patient management © 2020 the authors the journal of pathology published by john wiley & sons ltd on behalf of pathological society of great britain and ireland   \n",
       "110856                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       fuzzy logic selection as a new reliable tool to identify molecular grade signatures in breast cancer--the innodiag study personalized medicine has become a priority in breast cancer patient management in addition to the routinely used clinicopathological characteristics, clinicians will have to face an increasing amount of data derived from tumor molecular profiling the aims of this study were to develop a new gene selection method based on a fuzzy logic selection and classification algorithm, and to validate the gene signatures obtained on breast cancer patient cohorts   \n",
       "109108                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a comprehensive computer-aided polyp detection system for colonoscopy videos computer-aided detection cad can help colonoscopists reduce their polyp miss-rate, but existing cad systems are handicapped by using either shape, texture, or temporal information for detecting polyps, achieving limited sensitivity and specificity to overcome this limitation, our key contribution of this paper is to fuse all possible polyp features by exploiting the strengths of each feature while minimizing its weaknesses our new cad system has two stages, where the first stage builds on the robustness of shape features to reliably generate a set of candidates with a high sensitivity, while the second stage utilizes the high discriminative power of the computationally expensive features to effectively reduce false positives specifically, we employ a unique edge classifier and an original voting scheme to capture geometric features of polyps in context and then harness the power of convolutional neural networks in a novel score fusion approach to extract and combine shape, color, texture, and temporal information of the candidates our experimental results based on froc curves and a new analysis of polyp detection latency demonstrate a superiority over the state-of-the-art where our system yields a lower polyp detection latency and achieves a significantly higher sensitivity while generating dramatically fewer false positives this performance improvement is attributed to our reliable candidate generation and effective false positive reduction methods   \n",
       "44411                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        arrhythmic gut microbiome signatures predict risk of type 2 diabetes lifestyle, obesity, and the gut microbiome are important risk factors for metabolic disorders we demonstrate in 1,976 subjects of a german population cohort kora that specific microbiota members show 24-h oscillations in their relative abundance and identified 13 taxa with disrupted rhythmicity in type 2 diabetes t2d cross-validated prediction models based on this signature similarly classified t2d in an independent cohort focus, disruption of microbial oscillation and the model for t2d classification was confirmed in 1,363 subjects this arrhythmic risk signature was able to predict t2d in 699 kora subjects 5 years after initial sampling, being most effective in combination with bmi shotgun metagenomic analysis functionally linked 26 metabolic pathways to the diurnal oscillation of gut bacteria thus, a cohort-specific risk pattern of arrhythmic taxa enables classification and prediction of t2d, suggesting a functional link between circadian rhythms and the microbiome in metabolic diseases   \n",
       "73347                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                time to treatment prediction in chronic lymphocytic leukemia based on new transcriptional patterns chronic lymphocytic leukemia cll is the most frequent lymphoproliferative syndrome in western countries cll evolution is frequently indolent, and treatment is mostly reserved for those patients with signs or symptoms of disease progression in this work, we used rna sequencing data from the international cancer genome consortium cll cohort to determine new gene expression patterns that correlate with clinical evolutionwe determined that a 290-gene expression signature, in addition to immunoglobulin heavy chain variable region <i>ighv</i> mutation status, stratifies patients into four groups with notably different time to first treatment this finding was confirmed in an independent cohort similarly, we present a machine learning algorithm that predicts the need for treatment within the first 5 years following diagnosis using expression data from 2,198 genes this predictor achieved 90% precision and 89% accuracy when classifying independent cll cases our findings indicate that cll progression risk largely correlates with particular transcriptomic patterns and paves the way for the identification of high-risk patients who might benefit from prompt therapy following diagnosis   \n",
       "71454                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      epigenetic classifiers for precision diagnosis of brain tumors dna methylation profiling has proven to be a powerful analytical tool, which can accurately identify the tissue of origin of a wide range of benign and malignant neoplasms using microarray-based profiling and supervised machine learning algorithms, we and other groups have recently unraveled dna methylation signatures capable of aiding the histomolecular diagnosis of different tumor types we have explored the methylomes of metastatic brain tumors from patients with lung cancer, breast cancer, and cutaneous melanoma and primary brain neoplasms to build epigenetic classifiers our brain metastasis methylation brainmeth classifier has the ability to determine the type of brain tumor, the origin of the metastases, and the clinical-therapeutic subtype for patients with breast cancer brain metastases to facilitate the translation of these epigenetic classifiers into clinical practice, we selected and validated the most informative genomic regions utilizing quantitative methylation-specific polymerase chain reaction qmsp we believe that the refinement, expansion, integration, and clinical validation of brainmeth and other recently developed epigenetic classifiers will significantly contribute to the development of more comprehensive and accurate systems for the personalized management of patients with brain metastases   \n",
       "133385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        survival ensembles by the sum of pairwise differences with application to lung cancer microarray studies lung cancer is among the most common cancers in the united states, in terms of incidence and mortality in 2009, it is estimated that more than 150,000 deaths will result from lung cancer alone genetic information is an extremely valuable data source in characterizing the personal nature of cancer over the past several years, investigators have conducted numerous association studies where intensive genetic data is collected on relatively few patients compared to the numbers of gene predictors, with one scientific goal being to identify genetic features associated with cancer recurrence or survival in this note, we propose high-dimensional survival analysis through a new application of boosting, a powerful tool in machine learning our approach is based on an accelerated lifetime model and minimizing the sum of pairwise differences in residuals we apply our method to a recent microarray study of lung adenocarcinoma and find that our ensemble is composed of 19 genes while a proportional hazards ph ensemble is composed of nine genes, a proper subset of the 19-gene panel in one of our simulation scenarios, we demonstrate that ph boosting in a misspecified model tends to underfit and ignore moderately-sized covariate effects, on average diagnostic analyses suggest that the ph assumption is not satisfied in the microarray data and may explain, in part, the discrepancy in the sets of active coefficients our simulation studies and comparative data analyses demonstrate how statistical learning by ph models alone is insufficient   \n",
       "102203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     phenotypic characterization of breast invasive carcinoma via transferable tissue morphometric patterns learned from glioblastoma multiforme quantitative analysis of whole slide images wsis in a large cohort may provide predictive models of clinical outcome however, the performance of the existing techniques is hindered as a result of large technical variations eg, fixation, staining and biological heterogeneities eg, cell type, cell state that are always present in a large cohort although unsupervised feature learning provides a promising way in learning pertinent features without human intervention, its capability can be greatly limited due to the lack of well-curated examples in this paper, we explored the transferability of knowledge acquired from a well-curated glioblastoma multiforme gbm dataset through its application to the representation and characterization of tissue histology from the cancer genome atlas tcga breast invasive carcinoma brca cohort our experimental results reveals two major phenotypic subtypes with statistically significantly different survival curves further differential expression analysis of these two subtypes indicates enrichment of genes regulated by nf-kb in response to tnf and genes up-regulated in response to ifng   \n",
       "54325                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        deepbts: prediction of recurrence-free survival of non-small cell lung cancer using a time-binned deep neural network accurate prediction of non-small cell lung cancer nsclc prognosis after surgery remains challenging the cox proportional hazard ph model is widely used, however, there are some limitations associated with it in this study, we developed novel neural network models called binned time survival analysis deepbts models using 30 clinico-pathological features of surgically resected nsclc patients training cohort, n = 1,022; external validation cohort, n = 298 we employed the root-mean-square error in the supervised learning model, s- deepbts or negative log-likelihood in the semi-unsupervised learning model, su-deepbts as the loss function the su-deepbts algorithm achieved better performance c-index = 07306; auc = 07677 than the other models cox ph: c-index = 07048 and auc = 07390; s-deepbts: c-index = 07126 and auc = 07420 the top 14 features were selected using su-deepbts model as a selector and could distinguish the low- and high-risk groups in the training cohort p = 186 × 10<sup>-11</sup> and validation cohort p = 104 × 10<sup>-10</sup> when trained with the optimal feature set for each model, the su-deepbts model could predict the prognoses of nsclc better than the traditional model, especially in stage i patients follow-up studies using combined radiological, pathological imaging, and genomic data to enhance the performance of our model are ongoing   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "26797        0       0        0         0       0        0        0        0   \n",
       "29326        0       0        0         0       0        0        0        0   \n",
       "114138       0       1        0         0       0        0        0        0   \n",
       "51360        0       0        0         0       0        0        0        0   \n",
       "44111        0       0        0         0       0        0        0        0   \n",
       "77046        0       0        0         0       0        0        0        0   \n",
       "27449        0       0        0         0       0        0        0        0   \n",
       "163389       0       0        0         0       0        0        0        0   \n",
       "20388        0       0        0         0       0        0        0        0   \n",
       "49609        0       0        0         0       0        0        0        0   \n",
       "8571         0       0        0         0       0        0        0        0   \n",
       "47192        0       0        0         0       0        0        0        0   \n",
       "110856       0       0        0         0       0        0        0        0   \n",
       "109108       0       0        0         0       0        0        0        0   \n",
       "44411        0       0        0         0       0        0        0        0   \n",
       "73347        0       0        0         0       0        0        0        0   \n",
       "71454        0       0        0         0       0        0        0        0   \n",
       "133385       0       0        0         0       0        0        0        0   \n",
       "102203       0       0        0         0       0        0        0        0   \n",
       "54325        0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text mamm_text endo_text derm_text gene_text  \n",
       "26797           0        0         0         0         0         1  \n",
       "29326           0        0         0         0         0         1  \n",
       "114138          0        0         0         0         0         1  \n",
       "51360           0        0         0         0         0         1  \n",
       "44111           1        0         0         0         0         1  \n",
       "77046           0        0         0         0         0         1  \n",
       "27449           0        0         0         0         0         1  \n",
       "163389          0        0         0         0         0         1  \n",
       "20388           0        0         0         0         0         1  \n",
       "49609           0        0         0         0         0         1  \n",
       "8571            0        0         0         0         0         1  \n",
       "47192           1        0         0         0         0         1  \n",
       "110856          0        0         0         0         0         1  \n",
       "109108          0        0         0         1         0         1  \n",
       "44411           0        0         0         0         0         1  \n",
       "73347           0        0         0         0         0         1  \n",
       "71454           0        0         0         0         0         1  \n",
       "133385          0        0         0         0         0         1  \n",
       "102203          1        0         0         0         0         1  \n",
       "54325           0        0         0         0         0         1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['gene_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4da41bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31589, '1': 2590})\n"
     ]
    }
   ],
   "source": [
    "## PROTEINOMICS/BIOMARKERS\n",
    "\n",
    "## text\n",
    "text = ['proteinomic', 'immunoglob', 'cytokine', 'biomarker', 'tumor marker', 'tumour marker', 'inflammatory marker',\n",
    "       'peptide', 'interferon', 'laboratory test', 'blood test']\n",
    "\n",
    "feat['bio_text'] = np.where(groups['text'].str.contains('serum marker'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['bio_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['bio_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['bio_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "311dfd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33668, '1': 511})\n"
     ]
    }
   ],
   "source": [
    "## NATURAL LANGUAGE PROCESSING\n",
    "\n",
    "## text\n",
    "feat['nlp_text'] = np.where(groups['text'].str.contains(\"natural language\"), \"1\", \"0\")\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['nlp_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30976541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31583, '1': 2596})\n"
     ]
    }
   ],
   "source": [
    "## EHR RECORDS\n",
    "\n",
    "## text\n",
    "text = ['electronic health', 'health record', 'electronic record', 'patient record', 'medical record',\n",
    "        'care record', 'patient registry', 'research registr', 'clinical note', 'patient note', 'patient data',\n",
    "        'care data', 'care note', 'medical data', 'clinical data', 'hospital data', 'hospital note', 'admission note',\n",
    "        'physiological data', 'observational data', 'patient features', 'patient observations', 'patient history',\n",
    "        'medical history', 'care history']\n",
    "\n",
    "feat['ehr_text'] = np.where(groups['text'].str.contains('snomed'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['ehr_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['ehr_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['ehr_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a97eb46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33731, '1': 448})\n"
     ]
    }
   ],
   "source": [
    "## WEARABLE_SENSORS\n",
    "\n",
    "## text\n",
    "text = ['wearable sensor', 'smartwatch', 'internet of thing', 'sensor device', 'smart sensor', 'fitbit', 'fitness band',\n",
    "       'activity tracker', 'fitness tracker']\n",
    "\n",
    "feat['sensor_text'] = np.where(groups['text'].str.contains('smart watch'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['sensor_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['sensor_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['sensor_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebde5f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34139, '1': 40})\n"
     ]
    }
   ],
   "source": [
    "## PROM\n",
    "\n",
    "## text\n",
    "feat['prom_text'] = np.where(groups['text'].str.contains(\"patient reported outcome\"), \"1\", \"0\")\n",
    "feat['prom_text'] = np.where(groups['text'].str.contains(\"patient-reported outcome\"), \"1\", feat['prom_text'])\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['prom_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d94cdf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33877, '1': 302})\n"
     ]
    }
   ],
   "source": [
    "## SMARTPHONE\n",
    "\n",
    "## text\n",
    "feat['phone_text'] = np.where(groups['text'].str.contains(\"smartphone\"), \"1\", \"0\")\n",
    "feat['phone_text'] = np.where(groups['text'].str.contains(\"iphone\"), \"1\", feat['phone_text'])\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(feat['phone_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f54fa182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34000, '1': 179})\n"
     ]
    }
   ],
   "source": [
    "#### DIGITAL STETH / sound\n",
    "\n",
    "## text\n",
    "text = ['heart sound', 'heart murmur', 'breath sound', 'auscultat', 'phonocardio', 'digital steth']\n",
    "\n",
    "feat['sound_text'] = np.where(groups['text'].str.contains('electronic steth'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    feat['sound_text'] = np.where(groups['text'].str.contains(x), \"1\", feat['sound_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"valve\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"murmur\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"resp\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "feat['sound_text'] = np.where((groups['text'].str.contains(\"breath\")) &\n",
    "                             (groups['text'].str.contains(\"stethoscope\")) , \"1\", feat['sound_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(feat['sound_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b868b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>xr_text</th>\n",
       "      <th>ct_text</th>\n",
       "      <th>mri_text</th>\n",
       "      <th>echo_text</th>\n",
       "      <th>us_text</th>\n",
       "      <th>ecg_text</th>\n",
       "      <th>eeg_text</th>\n",
       "      <th>emg_text</th>\n",
       "      <th>histo_text</th>\n",
       "      <th>oct_text</th>\n",
       "      <th>mamm_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>gene_text</th>\n",
       "      <th>bio_text</th>\n",
       "      <th>nlp_text</th>\n",
       "      <th>ehr_text</th>\n",
       "      <th>sensor_text</th>\n",
       "      <th>prom_text</th>\n",
       "      <th>phone_text</th>\n",
       "      <th>sound_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83646</th>\n",
       "      <td>structural risk evaluation of a deep neural network and a markov model in extracting medical information from phonocardiography this paper presents a method for exploring structural risk of any artificial intelligence-based method in bioinformatics, the a-test method this method provides a way to not only quantitate the structural risk associated with a classification method, but provides a graphical representation to compare the learning capacity of different classification methods two different methods, deep time growing neural network dtgnn and hidden markov model hmm, are selected as two classification methods for comparison time series of heart sound signals are employed as the case study where the classifiers are trained to learn the disease-related changes results showed that the dtgnn offers a superior performance both in terms of the capacity and the structural risk the a-test method can be especially employed in comparing the learning methods with small data size</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76809</th>\n",
       "      <td>artificial intelligence-assisted auscultation of heart murmurs: validation by virtual clinical trial artificial intelligence ai has potential to improve the accuracy of screening for valvular and congenital heart disease by auscultation however, despite recent advances in signal processing and classification algorithms focused on heart sounds, clinical acceptance of this technology has been limited, in part due to lack of objective performance data we hypothesized that a heart murmur detection algorithm could be quantitatively and objectively evaluated by virtual clinical trial all cases from the johns hopkins cardiac auscultatory recording database card with either a pathologic murmur, an innocent murmur or no murmur were selected the test algorithm, developed independently of card, analyzed each recording using an automated batch processing protocol 3180 heart sound recordings from 603 outpatient visits were selected from card algorithm estimation of heart rate was similar to gold standard sensitivity and specificity for detection of pathologic cases were 93% ci 90-95% and 81% ci 75-85%, respectively, with accuracy 88% ci 85-91% performance varied according to algorithm certainty measure, age of patient, heart rate, murmur intensity, location of recording on the chest and pathologic diagnosis this is the first reported comprehensive and objective evaluation of an ai-based murmur detection algorithm to our knowledge the test algorithm performed well in this virtual clinical trial this strategy can be used to efficiently compare performance of other algorithms against the same dataset and improve understanding of the potential clinical usefulness of ai-assisted auscultation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134904</th>\n",
       "      <td>comparative classification of thrombotic formations on bileaflet mechanical heart valves by phonographic analysis haemodynamic performance of bileaflet mechanical heart valves can be severely affected by the formation of thrombotic deposits hence, early detection of thrombi is fundamental for a prompt diagnosis and adequate therapy this article aims at designing a novel diagnostic and prognostic tool able to detect valvular thrombosis at early stages of formation, ie, before the appearance of critical symptoms in patients who can be effectively treated by pharmacological therapy, preventing re-operation this approach relies on the acquisition of the acoustic signals produced by mechanical heart valves in the closing phase; the corresponding power spectra are then analysed by means of artificial neural networks trained to identify the presence of thrombi and classify their occurrence five commercial bileaflet mechanical heart valves were investigated in vitro in a sheffield pulse duplicator; for each valve six functional conditions were considered, each corresponding to a risk class for patients one normofunctioning and five thrombosed: they have been simulated by placing artificial deposits of increasing weight and different shape on the valve leaflet and on the annular housing; the case of one completely blocked leaflet was also investigated these six functional conditions represent risk classes: they were examined under various hydrodynamic regimes the acoustic signals produced by the valves were acquired by means of a phonocardiographic apparatus, then analysed and classified the ability to detect and classify thrombotic formations on mechanical valve leaflet would allow ranking patients by assigning them to one of the six risk classes, helping clinicians in establish adequate therapeutic approaches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133324</th>\n",
       "      <td>non-invasive algorithm for bowel motility estimation using a back-propagation neural network model of bowel sounds radiological scoring methods such as colon transit time ctt have been widely used for the assessment of bowel motility however, these radiograph-based methods need cumbersome radiological instruments and their frequent exposure to radiation therefore, a non-invasive estimation algorithm of bowel motility, based on a back-propagation neural network bpnn model of bowel sounds bs obtained by an auscultation, was devised</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728</th>\n",
       "      <td>detection of subclinical rheumatic heart disease in children using a deep learning algorithm on digital stethoscope: a study protocol rheumatic heart diseases rhds contribute significant morbidity and mortality globally to reduce the burden of rhd, timely initiation of secondary prophylaxis is important the objectives of this study are to determine the frequency of subclinical rhd and to train a deep learning dl algorithm using waveform data from the digital auscultatory stethoscope das in predicting subclinical rhd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59081</th>\n",
       "      <td>towards classifying non-segmented heart sound records using instantaneous frequency based features heart sound and its recorded signal which is known as phonocardiograph pcg are one of the most important biosignals that can be used to diagnose cardiac diseases alongside electrocardiogram ecg over the past few years, the use of pcg signals has become more widespread and researchers pay their attention to it and aim to provide an automated heart sound analysis and classification system that supports medical professionals in their decision in this paper, a new method for heart sound features extraction for the classification of non-segmented signals using instantaneous frequency was proposed the method has two major phases: the first phase is to estimate the instantaneous frequency of the recorded signal; the second phase is to extract a set of eleven features from the estimated instantaneous frequency the method was tested into two different datasets, one for binary classification normal and abnormal and the other for multi-classification five classes to ensure the robustness of the extracted features the overall accuracy, sensitivity, specificity, and precision for binary classification and multi-classification were all above 95% using both random forest and knn classifiers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160364</th>\n",
       "      <td>automatic wheeze detection based on auditory modelling automatic wheeze detection has several potential benefits compared with reliance on human auscultation: it is experience independent, an automated historical record can easily be kept, and it allows quantification of wheeze severity previous attempts to detect wheezes automatically have had partial success but have not been reliable enough to become widely accepted as a useful tool in this paper an improved algorithm for automatic wheeze detection based on auditory modelling is developed, called the frequency- and duration-dependent threshold algorithm the mean frequency and duration of each wheeze component are obtained automatically the detected wheezes are marked on a spectrogram in the new algorithm, the concept of a frequency- and duration-dependent threshold for wheeze detection is introduced another departure from previous work is that the threshold is based not on global power but on power corresponding to a particular frequency range the algorithm has been tested on 36 subjects, 11 of whom exhibited characteristics of wheeze the results show a marked improvement in the accuracy of wheeze detection when compared with previous algorithms</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39299</th>\n",
       "      <td>a preliminary investigation of whether hrca signals can differentiate between swallows from healthy people and swallows from people with neurodegenerative diseases high-resolution cervical auscultation hrca is an emerging method for non-invasively assessing swallowing by using acoustic signals from a contact microphone, vibratory signals from an accelerometer, and advanced signal processing and machine learning techniques hrca has differentiated between safe and unsafe swallows, predicted components of the modified barium swallow impairment profile, and predicted kinematic events of swallowing such as hyoid bone displacement, laryngeal vestibular closure, and upper esophageal sphincter opening with a high degree of accuracy however, hrca has not been used to characterize swallow function in specific patient populations this study investigated the ability of hrca to differentiate between swallows from healthy people and people with neurodegenerative diseases we hypothesized that hrca would differentiate between swallows from healthy people and people with neurodegenerative diseases with a high degree of accuracy we analyzed 170 swallows from 20 patients with neurodegenerative diseases and 170 swallows from 51 healthy age-matched adults who underwent concurrent video fluoroscopy with non-invasive neck sensors we used a linear mixed model and several supervised machine learning classifiers that use hrca signal features and a leave-one-out procedure to differentiate between swallows twenty-two hrca signal features were statistically significant p &lt; 005 for predicting whether swallows were from healthy people or from patients with neurodegenerative diseases using the hrca signal features alone, logistic regression and decision trees classified swallows between the two groups with 99% accuracy, 100% sensitivity, and 99% specificity this provides preliminary research evidence that hrca can differentiate swallow function between healthy and patient populations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78084</th>\n",
       "      <td>deep unsupervised representation learning for abnormal heart sound classification given the world-wide prevalence of heart disease, the robust and automatic detection of abnormal heart sounds could have profound effects on patient care and outcomes in this regard, a comparison of conventional and state-of-theart deep learning based computer audition paradigms for the audio classification task of normal, mild abnormalities, and moderate/severe abnormalities as present in phonocardiogram recordings, is presented herein in particular, we explore the suitability of deep feature representations as learnt by sequence to sequence autoencoders based on the audeep toolkit key results, gained on the new heart sounds shenzhen corpus, indicate that a fused combination of deep unsupervised features is well suited to the three-way classification problem, achieving our highest unweighted average recall of 479% on the test partition</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70210</th>\n",
       "      <td>classifying heart sounds using images of motifs, mfcc and temporal features cardiovascular disease is the leading cause of death in the world, and its early detection is a key to improving long-term health outcomes the auscultation of the heart is still an important method in the medical process because it is very simple and cheap to detect possible heart anomalies at an early stage, an automatic method enabling cardiac health low-cost screening for the general population would be highly valuable by analyzing the phonocardiogram signals, it is possible to perform cardiac diagnosis and find possible anomalies at an early-term therefore, the development of intelligent and automated analysis tools of the phonocardiogram is very relevant in this work, we use simultaneously collected electrocardiograms and phonocardiograms from the physionet challenge database with the main objective of determining whether a phonocardiogram corresponds to a normal or abnormal physiological state our main contribution is the methodological combination of time domain features and frequency domain features of phonocardiogram signals to improve cardiac disease automatic classification this novel approach is developed using both features first, the phonocardiogram signals are segmented with an algorithm based on a logistic regression hidden semi-markov model, which uses electrocardiogram signals as a reference then, two groups of features from the time and frequency domain are extracted from the phonocardiogram segments one group is based on motifs and the other on mel-frequency cepstral coefficients after that, we combine these features into a two-dimensional time-frequency heat map representation lastly, a binary classifier is applied to both groups of features to learn a model that discriminates between normal and abnormal phonocardiogram signals in the experiments, three classification algorithms are used: support vector machines, convolutional neural network, and random forest the best results are achieved when both time and mel-frequency cepstral coefficients features are considered using a support vector machines with a radial kernel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136466</th>\n",
       "      <td>heart murmur classification with feature selection heart sounds entail crucial heart function information in conditions of heart abnormalities, such as valve dysfunctions and rapid blood flow, additional sounds are heard in regular heart sounds, which can be employed in pathology diagnosis these additional sounds, or so-called murmurs, show different characteristics with respect to cardiovascular heart diseases, namely heart valve disorders in this paper, we present a method of heart murmur classification composed by three basic steps: feature extraction, feature selection, and classification using a nonlinear classifier a new set of 17 features extracted in the time, frequency and in the state space domain is suggested the features applied for murmur classification are selected using the floating sequential forward method sffs using this approach, the original set of 17 features is reduced to 10 features the classification results achieved using the proposed method are compared on a common database with the classification results obtained using the feature sets proposed in two well-known state of the art methods for murmur classification the achieved results suggest that the proposed method achieves slightly better results using a smaller feature set</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50601</th>\n",
       "      <td>automatic heart sound classification from segmented/unsegmented phonocardiogram signals using time and frequency features heart abnormality detection using heart sound signals phonocardiogram pcg has been an active research area for the last few decades in this paper, automatic heart sound classification using segmented and unsegmented pcg signals is presented</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160892</th>\n",
       "      <td>a classifier based on the artificial neural network approach for cardiologic auscultation in pediatrics this research work was aimed at developing a reliable screening device for diagnosis of heart murmurs in pediatrics this is a significant problem in pediatric cardiology because of the high rate of incidence of heart murmurs in this population reportedly 77-95%, of which only a small fraction arises from congenital heart disease the screening devices currently available eg chest x-ray, electrocardiogram, etc suffer from poor sensitivity and specificity in detecting congenital heart disease thus, patients with heart murmurs today are frequently assessed by consultation as well with advanced imaging techniques the most prominent among these is echocardiography however, echocardiography is expensive and is usually only available in healthcare centers in major cities thus, for patients being evaluated with a heart murmur, developing a more accurate screening device is vital to efforts in reducing health care costs</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33927</th>\n",
       "      <td>clinical research of a continuous auscultation recorder based on artificial intelligence &lt;b&gt;objective:&lt;/b&gt; to investigate the feasibility and clinical significance of a continuous auscultation recorder of bowel sounds based on artificial intelligence in monitoring the bowel sounds &lt;b&gt;methods:&lt;/b&gt; from november 1,2018 to august 12,2019, a continuous auscultation recorder of bowel sounds was applied to monitor the perioperative bowel sounds of 31 patients undergoing colorectal surgery, in order to discovery underlying rules which might be used to guide clinical practice &lt;b&gt;results:&lt;/b&gt; after the operation, the bowel sounds continued to exist for 18±08 h, and then gradually weakened or disappeared, and recovered gradually after 112±35 h the first exhaust and the first defecation were detected at the time of 227±58 h and 287±69 h after surgery, respectively the bowel sounds rate increased after eating, and decreased significantly after exhaust/defecation &lt;b&gt;conclusions:&lt;/b&gt; the continuous auscultation recorder of bowel sounds based on artificial intelligence was safe and effective, which can afford help to clinical evaluation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168760</th>\n",
       "      <td>artificial neural network-based method of screening heart murmurs in children early recognition of heart disease is an important goal in pediatrics efforts in developing an inexpensive screening device that can assist in the differentiation between innocent and pathological heart murmurs have met with limited success artificial neural networks anns are valuable tools used in complex pattern recognition and classification tasks the aim of the present study was to train an ann to distinguish between innocent and pathological murmurs effectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56229</th>\n",
       "      <td>gated recurrent unit-based heart sound analysis for heart failure screening heart failure hf is a type of cardiovascular disease caused by abnormal cardiac structure and function early screening of hf has important implication for treatment in a timely manner heart sound hs conveys relevant information related to hf; this study is therefore based on the analysis of hs signals the objective is to develop an efficient tool to identify subjects of normal, hf with preserved ejection fraction and hf with reduced ejection fraction automatically</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167028</th>\n",
       "      <td>classifying coronary dysfunction using neural networks through cardiovascular auscultation the paper applies artificial neural networks anns to the analysis of heart sound abnormalities through auscultation audio auscultation samples of 16 different coronary abnormalities were collected data pre-processing included down-sampling of the auscultated data and use of the fast fourier transform fft and the levinson-durbin autoregression algorithms for feature extraction and efficient data encoding these data were used in the training of a multi-layer perceptron mlp and radial basis function rbf neural network to develop a classification mechanism capable of distinguishing between different heart sound abnormalities the mlp and rbf networks attained classification accuracies of 84% and 88%, respectively the application of anns to the analysis of respiratory auscultation and consequently the development of a combined cardio-respiratory analysis system using auscultated data could lead to faster and more efficient treatment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67938</th>\n",
       "      <td>data-driven automated cardiac health management with robust edge analytics and de-risking remote and automated healthcare management has shown the prospective to significantly impact the future of human prognosis rate internet of things iot enables the development and implementation ecosystem to cater the need of large number of relevant stakeholders in this paper, we consider the cardiac health management system to demonstrate that data-driven techniques produce substantial performance merits in terms of clinical efficacy by employing robust machine learning methods with relevant and selected signal processing features we consider phonocardiogram pcg or heart sound as the exemplary physiological signal pcg carries substantial cardiac health signature to establish our claim of data-centric superior clinical utility our method demonstrates close to 85% accuracy on publicly available mit-physionet pcg datasets and outperform relevant state-of-the-art algorithm due to its simpler computational architecture of shallow classifier with just three features, the proposed analytics method is performed at edge gateway however, it is to be noted that healthcare analytics deal with number of sensitive data and subsequent inferences, which need privacy protection additionally, the problem of healthcare data privacy prevention is addressed by de-risking of sensitive data management using differential privacy, such that controlled privacy protection on sensitive healthcare data can be enabled when a user sets for privacy protection, appropriate privacy preservation is guaranteed for defense against privacy-breaching knowledge mining attacks in this era of iot and machine intelligence, this work is of practical importance, which enables on-demand automated screening of cardiac health under minimizing the privacy breaching risk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18232</th>\n",
       "      <td>deep learning algorithm for automated cardiac murmur detection via a digital stethoscope platform background clinicians vary markedly in their ability to detect murmurs during cardiac auscultation and identify the underlying pathological features deep learning approaches have shown promise in medicine by transforming collected data into clinically significant information the objective of this research is to assess the performance of a deep learning algorithm to detect murmurs and clinically significant valvular heart disease using recordings from a commercial digital stethoscope platform methods and results using &gt;34 hours of previously acquired and annotated heart sound recordings, we trained a deep neural network to detect murmurs to test the algorithm, we enrolled 962 patients in a clinical study and collected recordings at the 4 primary auscultation locations ground truth was established using patient echocardiograms and annotations by 3 expert cardiologists algorithm performance for detecting murmurs has sensitivity and specificity of 763% and 914%, respectively by omitting softer murmurs, those with grade 1 intensity, sensitivity increased to 900% application of the algorithm at the appropriate anatomic auscultation location detected moderate-to-severe or greater aortic stenosis, with sensitivity of 932% and specificity of 860%, and moderate-to-severe or greater mitral regurgitation, with sensitivity of 662% and specificity of 946% conclusions the deep learning algorithms ability to detect murmurs and clinically significant aortic stenosis and mitral regurgitation is comparable to expert cardiologists based on the annotated subset of our database the findings suggest that such algorithms would have utility as front-line clinical support tools to aid clinicians in screening for cardiac murmurs caused by valvular heart disease registration url: https://clinicaltrialsgov; unique identifier: nct03458806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158146</th>\n",
       "      <td>detection of heart murmurs using wavelet analysis and artificial neural networks this paper presents the algorithm and technical aspects of an intelligent diagnostic system for the detection of heart murmurs the purpose of this research is to address the lack of effectively accurate cardiac auscultation present at the primary care physician office by development of an algorithm capable of operating within the hectic environment of the primary care office the proposed algorithm consists of three main stages first; denoising of input data digital recordings of heart sounds, via wavelet packet analysis second; input vector preparation through the use of principal component analysis and block processing third; classification of the heart sound using an artificial neural network initial testing revealed the intelligent diagnostic system can differentiate between normal healthy heart sounds and abnormal heart sounds eg, murmurs, with a specificity of 705% and a sensitivity of 647%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "83646                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               structural risk evaluation of a deep neural network and a markov model in extracting medical information from phonocardiography this paper presents a method for exploring structural risk of any artificial intelligence-based method in bioinformatics, the a-test method this method provides a way to not only quantitate the structural risk associated with a classification method, but provides a graphical representation to compare the learning capacity of different classification methods two different methods, deep time growing neural network dtgnn and hidden markov model hmm, are selected as two classification methods for comparison time series of heart sound signals are employed as the case study where the classifiers are trained to learn the disease-related changes results showed that the dtgnn offers a superior performance both in terms of the capacity and the structural risk the a-test method can be especially employed in comparing the learning methods with small data size   \n",
       "76809                                                                                                                                                                                                                                                                                                                                                                                                                                                                    artificial intelligence-assisted auscultation of heart murmurs: validation by virtual clinical trial artificial intelligence ai has potential to improve the accuracy of screening for valvular and congenital heart disease by auscultation however, despite recent advances in signal processing and classification algorithms focused on heart sounds, clinical acceptance of this technology has been limited, in part due to lack of objective performance data we hypothesized that a heart murmur detection algorithm could be quantitatively and objectively evaluated by virtual clinical trial all cases from the johns hopkins cardiac auscultatory recording database card with either a pathologic murmur, an innocent murmur or no murmur were selected the test algorithm, developed independently of card, analyzed each recording using an automated batch processing protocol 3180 heart sound recordings from 603 outpatient visits were selected from card algorithm estimation of heart rate was similar to gold standard sensitivity and specificity for detection of pathologic cases were 93% ci 90-95% and 81% ci 75-85%, respectively, with accuracy 88% ci 85-91% performance varied according to algorithm certainty measure, age of patient, heart rate, murmur intensity, location of recording on the chest and pathologic diagnosis this is the first reported comprehensive and objective evaluation of an ai-based murmur detection algorithm to our knowledge the test algorithm performed well in this virtual clinical trial this strategy can be used to efficiently compare performance of other algorithms against the same dataset and improve understanding of the potential clinical usefulness of ai-assisted auscultation   \n",
       "134904                                                                                                                                                                                                                                                                                                                                comparative classification of thrombotic formations on bileaflet mechanical heart valves by phonographic analysis haemodynamic performance of bileaflet mechanical heart valves can be severely affected by the formation of thrombotic deposits hence, early detection of thrombi is fundamental for a prompt diagnosis and adequate therapy this article aims at designing a novel diagnostic and prognostic tool able to detect valvular thrombosis at early stages of formation, ie, before the appearance of critical symptoms in patients who can be effectively treated by pharmacological therapy, preventing re-operation this approach relies on the acquisition of the acoustic signals produced by mechanical heart valves in the closing phase; the corresponding power spectra are then analysed by means of artificial neural networks trained to identify the presence of thrombi and classify their occurrence five commercial bileaflet mechanical heart valves were investigated in vitro in a sheffield pulse duplicator; for each valve six functional conditions were considered, each corresponding to a risk class for patients one normofunctioning and five thrombosed: they have been simulated by placing artificial deposits of increasing weight and different shape on the valve leaflet and on the annular housing; the case of one completely blocked leaflet was also investigated these six functional conditions represent risk classes: they were examined under various hydrodynamic regimes the acoustic signals produced by the valves were acquired by means of a phonocardiographic apparatus, then analysed and classified the ability to detect and classify thrombotic formations on mechanical valve leaflet would allow ranking patients by assigning them to one of the six risk classes, helping clinicians in establish adequate therapeutic approaches   \n",
       "133324                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  non-invasive algorithm for bowel motility estimation using a back-propagation neural network model of bowel sounds radiological scoring methods such as colon transit time ctt have been widely used for the assessment of bowel motility however, these radiograph-based methods need cumbersome radiological instruments and their frequent exposure to radiation therefore, a non-invasive estimation algorithm of bowel motility, based on a back-propagation neural network bpnn model of bowel sounds bs obtained by an auscultation, was devised   \n",
       "7728                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  detection of subclinical rheumatic heart disease in children using a deep learning algorithm on digital stethoscope: a study protocol rheumatic heart diseases rhds contribute significant morbidity and mortality globally to reduce the burden of rhd, timely initiation of secondary prophylaxis is important the objectives of this study are to determine the frequency of subclinical rhd and to train a deep learning dl algorithm using waveform data from the digital auscultatory stethoscope das in predicting subclinical rhd   \n",
       "59081                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             towards classifying non-segmented heart sound records using instantaneous frequency based features heart sound and its recorded signal which is known as phonocardiograph pcg are one of the most important biosignals that can be used to diagnose cardiac diseases alongside electrocardiogram ecg over the past few years, the use of pcg signals has become more widespread and researchers pay their attention to it and aim to provide an automated heart sound analysis and classification system that supports medical professionals in their decision in this paper, a new method for heart sound features extraction for the classification of non-segmented signals using instantaneous frequency was proposed the method has two major phases: the first phase is to estimate the instantaneous frequency of the recorded signal; the second phase is to extract a set of eleven features from the estimated instantaneous frequency the method was tested into two different datasets, one for binary classification normal and abnormal and the other for multi-classification five classes to ensure the robustness of the extracted features the overall accuracy, sensitivity, specificity, and precision for binary classification and multi-classification were all above 95% using both random forest and knn classifiers   \n",
       "160364                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        automatic wheeze detection based on auditory modelling automatic wheeze detection has several potential benefits compared with reliance on human auscultation: it is experience independent, an automated historical record can easily be kept, and it allows quantification of wheeze severity previous attempts to detect wheezes automatically have had partial success but have not been reliable enough to become widely accepted as a useful tool in this paper an improved algorithm for automatic wheeze detection based on auditory modelling is developed, called the frequency- and duration-dependent threshold algorithm the mean frequency and duration of each wheeze component are obtained automatically the detected wheezes are marked on a spectrogram in the new algorithm, the concept of a frequency- and duration-dependent threshold for wheeze detection is introduced another departure from previous work is that the threshold is based not on global power but on power corresponding to a particular frequency range the algorithm has been tested on 36 subjects, 11 of whom exhibited characteristics of wheeze the results show a marked improvement in the accuracy of wheeze detection when compared with previous algorithms   \n",
       "39299                                                                                                                                                                        a preliminary investigation of whether hrca signals can differentiate between swallows from healthy people and swallows from people with neurodegenerative diseases high-resolution cervical auscultation hrca is an emerging method for non-invasively assessing swallowing by using acoustic signals from a contact microphone, vibratory signals from an accelerometer, and advanced signal processing and machine learning techniques hrca has differentiated between safe and unsafe swallows, predicted components of the modified barium swallow impairment profile, and predicted kinematic events of swallowing such as hyoid bone displacement, laryngeal vestibular closure, and upper esophageal sphincter opening with a high degree of accuracy however, hrca has not been used to characterize swallow function in specific patient populations this study investigated the ability of hrca to differentiate between swallows from healthy people and people with neurodegenerative diseases we hypothesized that hrca would differentiate between swallows from healthy people and people with neurodegenerative diseases with a high degree of accuracy we analyzed 170 swallows from 20 patients with neurodegenerative diseases and 170 swallows from 51 healthy age-matched adults who underwent concurrent video fluoroscopy with non-invasive neck sensors we used a linear mixed model and several supervised machine learning classifiers that use hrca signal features and a leave-one-out procedure to differentiate between swallows twenty-two hrca signal features were statistically significant p < 005 for predicting whether swallows were from healthy people or from patients with neurodegenerative diseases using the hrca signal features alone, logistic regression and decision trees classified swallows between the two groups with 99% accuracy, 100% sensitivity, and 99% specificity this provides preliminary research evidence that hrca can differentiate swallow function between healthy and patient populations   \n",
       "78084                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        deep unsupervised representation learning for abnormal heart sound classification given the world-wide prevalence of heart disease, the robust and automatic detection of abnormal heart sounds could have profound effects on patient care and outcomes in this regard, a comparison of conventional and state-of-theart deep learning based computer audition paradigms for the audio classification task of normal, mild abnormalities, and moderate/severe abnormalities as present in phonocardiogram recordings, is presented herein in particular, we explore the suitability of deep feature representations as learnt by sequence to sequence autoencoders based on the audeep toolkit key results, gained on the new heart sounds shenzhen corpus, indicate that a fused combination of deep unsupervised features is well suited to the three-way classification problem, achieving our highest unweighted average recall of 479% on the test partition   \n",
       "70210   classifying heart sounds using images of motifs, mfcc and temporal features cardiovascular disease is the leading cause of death in the world, and its early detection is a key to improving long-term health outcomes the auscultation of the heart is still an important method in the medical process because it is very simple and cheap to detect possible heart anomalies at an early stage, an automatic method enabling cardiac health low-cost screening for the general population would be highly valuable by analyzing the phonocardiogram signals, it is possible to perform cardiac diagnosis and find possible anomalies at an early-term therefore, the development of intelligent and automated analysis tools of the phonocardiogram is very relevant in this work, we use simultaneously collected electrocardiograms and phonocardiograms from the physionet challenge database with the main objective of determining whether a phonocardiogram corresponds to a normal or abnormal physiological state our main contribution is the methodological combination of time domain features and frequency domain features of phonocardiogram signals to improve cardiac disease automatic classification this novel approach is developed using both features first, the phonocardiogram signals are segmented with an algorithm based on a logistic regression hidden semi-markov model, which uses electrocardiogram signals as a reference then, two groups of features from the time and frequency domain are extracted from the phonocardiogram segments one group is based on motifs and the other on mel-frequency cepstral coefficients after that, we combine these features into a two-dimensional time-frequency heat map representation lastly, a binary classifier is applied to both groups of features to learn a model that discriminates between normal and abnormal phonocardiogram signals in the experiments, three classification algorithms are used: support vector machines, convolutional neural network, and random forest the best results are achieved when both time and mel-frequency cepstral coefficients features are considered using a support vector machines with a radial kernel   \n",
       "136466                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  heart murmur classification with feature selection heart sounds entail crucial heart function information in conditions of heart abnormalities, such as valve dysfunctions and rapid blood flow, additional sounds are heard in regular heart sounds, which can be employed in pathology diagnosis these additional sounds, or so-called murmurs, show different characteristics with respect to cardiovascular heart diseases, namely heart valve disorders in this paper, we present a method of heart murmur classification composed by three basic steps: feature extraction, feature selection, and classification using a nonlinear classifier a new set of 17 features extracted in the time, frequency and in the state space domain is suggested the features applied for murmur classification are selected using the floating sequential forward method sffs using this approach, the original set of 17 features is reduced to 10 features the classification results achieved using the proposed method are compared on a common database with the classification results obtained using the feature sets proposed in two well-known state of the art methods for murmur classification the achieved results suggest that the proposed method achieves slightly better results using a smaller feature set   \n",
       "50601                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                automatic heart sound classification from segmented/unsegmented phonocardiogram signals using time and frequency features heart abnormality detection using heart sound signals phonocardiogram pcg has been an active research area for the last few decades in this paper, automatic heart sound classification using segmented and unsegmented pcg signals is presented   \n",
       "160892                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      a classifier based on the artificial neural network approach for cardiologic auscultation in pediatrics this research work was aimed at developing a reliable screening device for diagnosis of heart murmurs in pediatrics this is a significant problem in pediatric cardiology because of the high rate of incidence of heart murmurs in this population reportedly 77-95%, of which only a small fraction arises from congenital heart disease the screening devices currently available eg chest x-ray, electrocardiogram, etc suffer from poor sensitivity and specificity in detecting congenital heart disease thus, patients with heart murmurs today are frequently assessed by consultation as well with advanced imaging techniques the most prominent among these is echocardiography however, echocardiography is expensive and is usually only available in healthcare centers in major cities thus, for patients being evaluated with a heart murmur, developing a more accurate screening device is vital to efforts in reducing health care costs   \n",
       "33927                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       clinical research of a continuous auscultation recorder based on artificial intelligence <b>objective:</b> to investigate the feasibility and clinical significance of a continuous auscultation recorder of bowel sounds based on artificial intelligence in monitoring the bowel sounds <b>methods:</b> from november 1,2018 to august 12,2019, a continuous auscultation recorder of bowel sounds was applied to monitor the perioperative bowel sounds of 31 patients undergoing colorectal surgery, in order to discovery underlying rules which might be used to guide clinical practice <b>results:</b> after the operation, the bowel sounds continued to exist for 18±08 h, and then gradually weakened or disappeared, and recovered gradually after 112±35 h the first exhaust and the first defecation were detected at the time of 227±58 h and 287±69 h after surgery, respectively the bowel sounds rate increased after eating, and decreased significantly after exhaust/defecation <b>conclusions:</b> the continuous auscultation recorder of bowel sounds based on artificial intelligence was safe and effective, which can afford help to clinical evaluation   \n",
       "168760                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     artificial neural network-based method of screening heart murmurs in children early recognition of heart disease is an important goal in pediatrics efforts in developing an inexpensive screening device that can assist in the differentiation between innocent and pathological heart murmurs have met with limited success artificial neural networks anns are valuable tools used in complex pattern recognition and classification tasks the aim of the present study was to train an ann to distinguish between innocent and pathological murmurs effectively   \n",
       "56229                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          gated recurrent unit-based heart sound analysis for heart failure screening heart failure hf is a type of cardiovascular disease caused by abnormal cardiac structure and function early screening of hf has important implication for treatment in a timely manner heart sound hs conveys relevant information related to hf; this study is therefore based on the analysis of hs signals the objective is to develop an efficient tool to identify subjects of normal, hf with preserved ejection fraction and hf with reduced ejection fraction automatically   \n",
       "167028                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  classifying coronary dysfunction using neural networks through cardiovascular auscultation the paper applies artificial neural networks anns to the analysis of heart sound abnormalities through auscultation audio auscultation samples of 16 different coronary abnormalities were collected data pre-processing included down-sampling of the auscultated data and use of the fast fourier transform fft and the levinson-durbin autoregression algorithms for feature extraction and efficient data encoding these data were used in the training of a multi-layer perceptron mlp and radial basis function rbf neural network to develop a classification mechanism capable of distinguishing between different heart sound abnormalities the mlp and rbf networks attained classification accuracies of 84% and 88%, respectively the application of anns to the analysis of respiratory auscultation and consequently the development of a combined cardio-respiratory analysis system using auscultated data could lead to faster and more efficient treatment   \n",
       "67938                                                                                                                                                                                                                                                                                                                        data-driven automated cardiac health management with robust edge analytics and de-risking remote and automated healthcare management has shown the prospective to significantly impact the future of human prognosis rate internet of things iot enables the development and implementation ecosystem to cater the need of large number of relevant stakeholders in this paper, we consider the cardiac health management system to demonstrate that data-driven techniques produce substantial performance merits in terms of clinical efficacy by employing robust machine learning methods with relevant and selected signal processing features we consider phonocardiogram pcg or heart sound as the exemplary physiological signal pcg carries substantial cardiac health signature to establish our claim of data-centric superior clinical utility our method demonstrates close to 85% accuracy on publicly available mit-physionet pcg datasets and outperform relevant state-of-the-art algorithm due to its simpler computational architecture of shallow classifier with just three features, the proposed analytics method is performed at edge gateway however, it is to be noted that healthcare analytics deal with number of sensitive data and subsequent inferences, which need privacy protection additionally, the problem of healthcare data privacy prevention is addressed by de-risking of sensitive data management using differential privacy, such that controlled privacy protection on sensitive healthcare data can be enabled when a user sets for privacy protection, appropriate privacy preservation is guaranteed for defense against privacy-breaching knowledge mining attacks in this era of iot and machine intelligence, this work is of practical importance, which enables on-demand automated screening of cardiac health under minimizing the privacy breaching risk   \n",
       "18232                                                                                                                                                                                                                        deep learning algorithm for automated cardiac murmur detection via a digital stethoscope platform background clinicians vary markedly in their ability to detect murmurs during cardiac auscultation and identify the underlying pathological features deep learning approaches have shown promise in medicine by transforming collected data into clinically significant information the objective of this research is to assess the performance of a deep learning algorithm to detect murmurs and clinically significant valvular heart disease using recordings from a commercial digital stethoscope platform methods and results using >34 hours of previously acquired and annotated heart sound recordings, we trained a deep neural network to detect murmurs to test the algorithm, we enrolled 962 patients in a clinical study and collected recordings at the 4 primary auscultation locations ground truth was established using patient echocardiograms and annotations by 3 expert cardiologists algorithm performance for detecting murmurs has sensitivity and specificity of 763% and 914%, respectively by omitting softer murmurs, those with grade 1 intensity, sensitivity increased to 900% application of the algorithm at the appropriate anatomic auscultation location detected moderate-to-severe or greater aortic stenosis, with sensitivity of 932% and specificity of 860%, and moderate-to-severe or greater mitral regurgitation, with sensitivity of 662% and specificity of 946% conclusions the deep learning algorithms ability to detect murmurs and clinically significant aortic stenosis and mitral regurgitation is comparable to expert cardiologists based on the annotated subset of our database the findings suggest that such algorithms would have utility as front-line clinical support tools to aid clinicians in screening for cardiac murmurs caused by valvular heart disease registration url: https://clinicaltrialsgov; unique identifier: nct03458806   \n",
       "158146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            detection of heart murmurs using wavelet analysis and artificial neural networks this paper presents the algorithm and technical aspects of an intelligent diagnostic system for the detection of heart murmurs the purpose of this research is to address the lack of effectively accurate cardiac auscultation present at the primary care physician office by development of an algorithm capable of operating within the hectic environment of the primary care office the proposed algorithm consists of three main stages first; denoising of input data digital recordings of heart sounds, via wavelet packet analysis second; input vector preparation through the use of principal component analysis and block processing third; classification of the heart sound using an artificial neural network initial testing revealed the intelligent diagnostic system can differentiate between normal healthy heart sounds and abnormal heart sounds eg, murmurs, with a specificity of 705% and a sensitivity of 647%   \n",
       "\n",
       "       xr_text ct_text mri_text echo_text us_text ecg_text eeg_text emg_text  \\\n",
       "83646        0       0        0         0       0        0        0        0   \n",
       "76809        0       0        0         0       0        0        0        0   \n",
       "134904       0       0        0         0       0        0        0        0   \n",
       "133324       1       0        0         0       0        0        0        0   \n",
       "7728         0       0        0         0       0        0        0        0   \n",
       "59081        0       0        0         0       0        1        0        0   \n",
       "160364       0       0        0         0       0        0        0        0   \n",
       "39299        0       0        0         0       0        0        0        0   \n",
       "78084        0       0        0         0       0        0        0        0   \n",
       "70210        0       0        0         0       0        1        0        0   \n",
       "136466       0       0        0         0       0        0        0        0   \n",
       "50601        0       0        0         0       0        0        0        0   \n",
       "160892       1       0        0         1       0        1        0        0   \n",
       "33927        0       0        0         0       0        0        0        0   \n",
       "168760       0       0        0         0       0        0        0        0   \n",
       "56229        0       0        0         0       0        0        0        0   \n",
       "167028       0       0        0         0       0        0        0        0   \n",
       "67938        0       0        0         0       0        0        0        0   \n",
       "18232        0       0        0         1       0        0        0        0   \n",
       "158146       0       0        0         0       0        0        0        0   \n",
       "\n",
       "       histo_text oct_text mamm_text endo_text derm_text gene_text bio_text  \\\n",
       "83646           0        0         0         0         0         0        0   \n",
       "76809           0        0         0         0         0         0        0   \n",
       "134904          0        0         0         0         0         0        0   \n",
       "133324          0        0         0         0         0         0        0   \n",
       "7728            0        0         0         0         0         0        0   \n",
       "59081           0        0         0         0         0         0        0   \n",
       "160364          0        0         0         0         0         0        0   \n",
       "39299           0        0         0         0         0         0        0   \n",
       "78084           0        0         0         0         0         0        0   \n",
       "70210           0        0         0         0         0         0        0   \n",
       "136466          0        0         0         0         0         0        0   \n",
       "50601           0        0         0         0         0         0        0   \n",
       "160892          0        0         0         0         0         0        0   \n",
       "33927           0        0         0         0         0         0        0   \n",
       "168760          0        0         0         0         0         0        0   \n",
       "56229           0        0         0         0         0         0        0   \n",
       "167028          0        0         0         0         0         0        0   \n",
       "67938           0        0         0         0         0         0        0   \n",
       "18232           0        0         0         0         0         0        0   \n",
       "158146          0        0         0         0         0         0        0   \n",
       "\n",
       "       nlp_text ehr_text sensor_text prom_text phone_text sound_text  \n",
       "83646         0        0           0         0          0          1  \n",
       "76809         0        0           0         0          0          1  \n",
       "134904        0        0           0         0          0          1  \n",
       "133324        0        0           0         0          0          1  \n",
       "7728          0        0           0         0          0          1  \n",
       "59081         0        0           0         0          0          1  \n",
       "160364        0        0           0         0          0          1  \n",
       "39299         0        0           0         0          0          1  \n",
       "78084         0        0           0         0          0          1  \n",
       "70210         0        0           0         0          0          1  \n",
       "136466        0        0           0         0          0          1  \n",
       "50601         0        0           0         0          0          1  \n",
       "160892        0        0           0         0          0          1  \n",
       "33927         0        0           0         0          0          1  \n",
       "168760        0        0           0         0          0          1  \n",
       "56229         0        0           0         0          0          1  \n",
       "167028        0        0           0         0          0          1  \n",
       "67938         0        1           1         0          0          1  \n",
       "18232         0        0           0         0          0          1  \n",
       "158146        0        0           0         0          0          1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[feat['sound_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f83c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMBINE\n",
    "labelled['feat_xr'] = np.where(feat['xr_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_ct'] = np.where(feat['ct_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_mri'] = np.where(feat['mri_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_eeg'] = np.where(feat['eeg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_ecg'] = np.where(feat['ecg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_emg'] = np.where(feat['emg_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_us'] = np.where(feat['us_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_echo'] = np.where(feat['echo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_histo'] = np.where(feat['histo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_oct'] = np.where(feat['oct_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_mamm'] = np.where(feat['mamm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_endoscop'] = np.where(feat['endo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_derm'] = np.where(feat['derm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_gene'] = np.where(feat['gene_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_bio'] = np.where(feat['bio_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_nlp'] = np.where(feat['nlp_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_ehr'] = np.where(feat['ehr_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_sensor'] = np.where(feat['sensor_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_phone'] = np.where(feat['phone_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_prom'] = np.where(feat['prom_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['feat_sound'] = np.where(feat['sound_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "#feat.to_csv('output/feat_tagged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d94c23",
   "metadata": {},
   "source": [
    "## Tag Specialties / Use-Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90780931",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## CLASS TAGS - by mesh for disease type\n",
    "######################\n",
    "\n",
    "\n",
    "######################\n",
    "## CLASS TAGS - by specialty, not mutually exclusive\n",
    "######################\n",
    "## INTENSIVE CARE MEDICINE / icu\n",
    "\n",
    "## EMERGENCY MEDICINE / ed\n",
    "\n",
    "## INFECTIONS [C01] / id\n",
    "    #### SEPSIS / sepsis\n",
    "    #### COVID-19 / cov19\n",
    "    #### MALARIA / malaria\n",
    "    #### HIV / hiv\n",
    "    #### TB / tb\n",
    "    #### TROPICAL DISEASE / tropic\n",
    "    \n",
    "## DERMATOLOGY [C17] / derm\n",
    "    ####SKIN CANCERS / dermca\n",
    "\n",
    "## NEOPLASMS [C04] / onc\n",
    "    #### RADIOTHERAPY / rx\n",
    "    #### LUNG / lungca\n",
    "    #### NEURO / neuroca\n",
    "    #### GI / gica\n",
    "    #### HPB / hepca\n",
    "    #### GYNAE / gynonc\n",
    "    #### PROSTATE / prosca\n",
    "    #### RENAL / renalca\n",
    "    #### HAEM / haemonc\n",
    "    \n",
    "## BREAST / breast (<- almost entirely onc)\n",
    "    #### BREAST CA / breastca\n",
    "    \n",
    "## PSYCHIATRY / psych\n",
    "    #### SUICIDE / suicide\n",
    "    \n",
    "## MUSCULOSKELETAL [C05] / msk\n",
    "    #### FRACTURE / frac\n",
    "\n",
    "## CONNECTIVE TISSUE [C17] / rheum\n",
    "\n",
    "## GASTROINTESTINAL [C06] / gi\n",
    "\n",
    "## HEPATOLOGY & BILIARY [C06] / hep\n",
    "\n",
    "## RESPIRATORY [C08] / resp\n",
    "    #### PNEUMONIA / pneum\n",
    "    #### OBSTRUCTIVE SLEEP / osa\n",
    "    #### PULMONARY EMBOLISM / pe\n",
    "    \n",
    "## NERVOUS SYSTEM [C10] / neuro\n",
    "    #### STROKE / cva\n",
    "    #### SEIZURE / epilep\n",
    "    #### DEMENTIA / alzh\n",
    "\n",
    "## CARDIOVASCULAR [C14] / cvs\n",
    "    #### ISCHAEMIC HEART DISEASE / ihd\n",
    "    #### CARDIAC FAILURE / hf\n",
    "    #### ARRHYTHMIA / arrhyt\n",
    "    \n",
    "## ENDOCRINE [C19] (no dm) / endo\n",
    "\n",
    "## DIABETES / dm\n",
    "    #### INSULIN / insulin\n",
    "    #### RETINOPATHY / retina\n",
    "        \n",
    "## OPHTHALMOLOGY [C11] / eye\n",
    "\n",
    "## HAEMATOLOGIC [C15] / haem\n",
    "\n",
    "## GYNAE/OBSTETRIC [C13] / obs\n",
    "\n",
    "## NEPHROLOGY [C12] / renal\n",
    "    #### ACUTE & CHRONIC KIDNEY / ackd\n",
    "    \n",
    "## PAEDIATRICS / paeds\n",
    "\n",
    "## STOMATOGNATHIC [C07] / dental\n",
    "\n",
    "## AUDIOLOGY [C09] / ent\n",
    "\n",
    "## PUBLIC HEALTH / pubh\n",
    "\n",
    "########exclude?############# \n",
    "\n",
    "## ALCOHOL & SUBSTANCES [C25] / etoh\n",
    "## WOUNDS AND INJURIES [C26] -> TRAUMA\n",
    "## ENVIRONMENTAL [C21] / env\n",
    "\n",
    "\n",
    "######################\n",
    "## SPECIAL\n",
    "######################\n",
    "## BCI\n",
    "## CONTROL\n",
    "#### PROSTHESIS CONTROL\n",
    "#### WHEELCHAIR CONTROL\n",
    "\n",
    "\n",
    "\n",
    "## vitals monitoring / deterioration\n",
    "## trauma?\n",
    "## sleep\n",
    "## pulmonary embolism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "590779eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = groups[['text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d069aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33431, '1': 748})\n"
     ]
    }
   ],
   "source": [
    "## INTENSIVE CARE MEDICINE / icu\n",
    "\n",
    "## text\n",
    "text = ['intensive care', 'critical care', 'mechanical ventilation', 'invasive ventilation', 'ventilator', 'pressure ventilation', \n",
    "       'acute respiratory distress syndrome', 'organ failure', 'tracheal intubation', 'vasopressor', 'inotrope',\n",
    "       'hemofiltration', 'membrane oxygenation', 'ecmo', ' ett ', 'layngoscope', 'endotracheal tube']\n",
    "\n",
    "spec['icu_text'] = np.where(groups['text'].str.contains('intensive therapy unit'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['icu_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['icu_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "##output    \n",
    "print('text counts:')\n",
    "print(Counter(spec['icu_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68d4c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33768, '1': 411})\n"
     ]
    }
   ],
   "source": [
    "## EMERGENCY MEDICINE / ed\n",
    "\n",
    "## text\n",
    "text = ['emergency department', 'emergency room', 'emergency physician', 'emergency doctor', 'emergency medicine',\n",
    "       'emergency care', 'accident and emergency', 'a&e', 'accident & emergency', 'prehospital', 'pre-hospital',\n",
    "       'casualty room', 'emergency ward']\n",
    "\n",
    "spec['ed_text'] = np.where(groups['text'].str.contains('casualty department'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['ed_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['ed_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['ed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f704cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 31031, '1': 3148})\n"
     ]
    }
   ],
   "source": [
    "## INFECTIONS / id + bacteriology/virology/parasitology\n",
    "\n",
    "## text\n",
    "text = ['bacter', 'microbiol', 'sepsis', 'septic', 'toxic shock', 'microbe', 'tuberculosis',\n",
    "       'cholera', 'shigella', 'bubonic', 'plague', 'anthrax', 'gonorrhea', 'syphilis', 'diphtheria', 'legionell',\n",
    "       'leptospirosis', 'listeriosis', 'tetanus', 'pertussis', 'staph', 'strep', 'escherichia', 'leprosy', \n",
    "        'mycobacter', 'blood culture',\n",
    "       \n",
    "       'fungus', 'fungal', 'fungaemia', 'fungemia', 'candida', 'aspergill',\n",
    "       \n",
    "       'virolog', 'virus', 'viral', 'virulen', 'influenza', 'hepatitis', 'herpes', 'varicella',\n",
    "       'measles', 'covid', 'sars-cov', 'coronavirus', 'severe acute respiratory syndrome', 'yellow fever', 'dengue',\n",
    "       'rabies', 'zika', 'ebola', 'polio', 'hemorrhagic fever', 'haemorrhagic fever', 'rabies',\n",
    "       \n",
    "       'transmitted disease', 'sexually transmit', 'sexual transmis',\n",
    "       \n",
    "       'lyme', 'malaria', 'falciparum', 'anopheles', 'parasit', 'helminth', 'protozoa', \n",
    "        'leishmaniasis', 'trypanosom', 'chagas', 'schistosomiasis', 'filariasis', 'toxoplasm' 'tropical disease',\n",
    "       \n",
    "       ' hiv ', 'human immunodeficiency virus', 'acquired immune deficiency syndrome']\n",
    "\n",
    "spec['id_text'] = np.where(groups['text'].str.contains('infectio'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['id_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['id_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['id_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05c7a721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33928, '1': 251})\n"
     ]
    }
   ],
   "source": [
    "#### SEPSIS / sepsis\n",
    "\n",
    "## text\n",
    "text = ['sepsis', 'septic', 'bacteraem', 'bacterem', 'toxic shock syndrome', 'pyaemia']\n",
    "\n",
    "spec['sepsis_text'] = np.where(groups['text'].str.contains('pyemia'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['sepsis_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['sepsis_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['sepsis_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "534bcd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32994, '1': 1185})\n"
     ]
    }
   ],
   "source": [
    "#### COVID-19 / cov19\n",
    "\n",
    "## text\n",
    "text = ['sars-cov', 'coronavirus disease 2019', 'novel coronavirus', 'coronavirus disease 19', 'sars cov']\n",
    "\n",
    "spec['cov19_text'] = np.where(groups['text'].str.contains('covid'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['cov19_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['cov19_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['cov19_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78c203fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33996, '1': 183})\n"
     ]
    }
   ],
   "source": [
    "#### HIV / hiv\n",
    "\n",
    "## text\n",
    "text = ['human immunodeficiency virus', 'acquired immune deficiency syndrome', ' aids ']\n",
    "\n",
    "spec['hiv_text'] = np.where(groups['text'].str.contains(' hiv '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hiv_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hiv_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hiv_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f260f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34011, '1': 168})\n"
     ]
    }
   ],
   "source": [
    "#### TUBERCULOSIS / tb\n",
    "\n",
    "## text\n",
    "text = ['tuberculosis', 'mycobacterium tuberc']\n",
    "\n",
    "spec['tb_text'] = np.where(groups['text'].str.contains('tubercu'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['tb_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['tb_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['tb_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eee84916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34053, '1': 126})\n"
     ]
    }
   ],
   "source": [
    "#### TROPICAL DISEASE / tropic\n",
    "\n",
    "## text\n",
    "text = ['malaria', 'falciparum', 'anopheles', 'parasit', 'helminth', 'protozoa', \n",
    "        'leishmaniasis', 'trypanosom', 'chagas', 'schistosomiasis', 'filariasis', 'toxoplasm',\n",
    "       'yellow fever', 'dengue', 'rabies', 'cholera', 'zika', 'ebola', 'hemorrhagic fever', 'haemorrhagic fever',\n",
    "        'tropical disease', 'tropical medicine', 'filariasis']\n",
    "\n",
    "spec['tropic_text'] = np.where(groups['text'].str.contains('falciparum'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['tropic_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['tropic_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['tropic_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0be31c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34120, '1': 59})\n"
     ]
    }
   ],
   "source": [
    "#### MALARIA / malaria\n",
    "\n",
    "## text\n",
    "text = ['malaria', 'anopheles']\n",
    "\n",
    "spec['malaria_text'] = np.where(groups['text'].str.contains('falciparum'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['malaria_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['malaria_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['malaria_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7edd69f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33397, '1': 782})\n"
     ]
    }
   ],
   "source": [
    "## DERMATOLOGY / derm\n",
    "\n",
    "## text\n",
    "text = ['dermato', 'dermatitis', 'erythema', 'cutaneous', 'eczema', 'psoriasis', 'rosacea', 'vitiligo', 'urticaria',\n",
    "       'pruritus', 'impetigo', 'pemphigoid', 'pityriasis', 'melanoma', 'basal cell ca', 'merkel cell',\n",
    "       'skin cancer', 'skin lesion', 'skin rash', 'nevus', 'naevus', 'dermal cancer', 'dermal lesion']\n",
    "\n",
    "spec['derm_text'] = np.where(groups['text'].str.contains('emollient'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['derm_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['derm_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['derm_text'])\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['derm_text'])\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['derm_text'])\n",
    "spec['derm_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['derm_text'])\n",
    "                             \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['derm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea294850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33787, '1': 392})\n"
     ]
    }
   ],
   "source": [
    "#### SKIN CANCERS / dermca\n",
    "\n",
    "## text\n",
    "text = ['melanoma', 'melanocytic', 'casal cell ca', 'skin cancer', 'dysplastic nevus', 'dysplastic naevus',\n",
    "       'merkel cell', 'atypical nevus', 'atypical naevus']\n",
    "\n",
    "spec['dermca_text'] = np.where(groups['text'].str.contains('skin cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['dermca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['dermca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['dermca_text'])\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\"squamous cell\")) , \"1\", spec['dermca_text'])\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"skin\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['dermca_text'])\n",
    "spec['dermca_text'] = np.where((groups['text'].str.contains(\"dermal\")) &\n",
    "                             (groups['text'].str.contains(\" scc \")) , \"1\", spec['dermca_text'])\n",
    "                          \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['dermca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c6f5cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 26595, '1': 7584})\n"
     ]
    }
   ],
   "source": [
    "## ONCOLOGY / onc\n",
    "\n",
    "## text\n",
    "text = ['radiotherapy', 'radiation therapy', 'mammog', 'breast ca', 'breast tum', 'invasive lobular carcinoma', \n",
    "        ' dcis ', 'ductal carcinoma in situ', 'lung cancer', 'lung malignancy', 'lung carcinoma', 'lung nodule',\n",
    "        'pulmonary nodule', 'mesothelioma', 'nsclc',\n",
    "       'neuroonc', 'neuro onc', 'neuro-onc', 'brain cancer', 'brain tumor', 'brain tumour', 'brain malignancy',\n",
    "       'glioma', 'glioblastoma', 'astrocytoma', 'pituitary adenoma', 'acoustic neuroma', 'meningioma',\n",
    "       'cns lymphoma', 'oligodendroglioma', 'meningeal cancer', 'meningeal carcinomatosis',\n",
    "       'melanoma', 'melanocytic', 'casal cell ca', 'skin cancer', 'dysplastic nevus', 'dysplastic naevus',\n",
    "       'merkel cell', 'atypical nevus', 'atypical naevus',\n",
    "       'gi cancer', 'gastrointestinal cancer', 'colon cancer', 'colon carcinoma', 'colon polyp', 'colon adeno', 'colon tumo',\n",
    "       'colonic cancer', 'colonic carcinoma', 'colonic adeno', 'colonic polyp', 'colonic tumo', 'colonic neoplasm',\n",
    "        'rectal cancer', 'rectal carcinoma', 'rectal polyp', 'rectal tumo', 'rectal neoplasm', 'bowel cancer', 'bowel neoplasm',\n",
    "       'bowel tumo', 'stomach cancer', 'gastric cancer', 'gastric carcinoma', 'gastric neoplasm', 'gastric tumo',\n",
    "       'esophageal cancer', 'esophageal tumo', 'esophageal neoplasm',\n",
    "       'hepatocellular cancer', 'hepatocellular carcinoma', 'hepatic cancer', 'hepatic carcinoma', 'hepatic tumo',\n",
    "       'hepatic neoplasm', 'liver cancer', 'liver carcinoma', 'liver tumo', 'cholangioca', 'pancreatic cancer',\n",
    "       'pancreatic neoplasm', 'pancreatic tumo', 'biliary cancer', 'bile duct cancer',\n",
    "       'prostate cancer', 'prostate specific antigen', 'prostate carcinoma', 'prostate neoplasm', 'prostate tumo',\n",
    "       'prostate adeno', 'prostatic cancer', 'prostatic neoplasm', 'prostatic tumo', 'prostatic adeno', 'prostatectomy',\n",
    "       ' psa ', 'kidney cancer', 'kidney tumo', 'renal cell carcinoma', 'renal call cancer', 'renal tumo', 'renal cancer',\n",
    "       'wilms tumo', 'bladder cancer', 'bladder carcinoma', 'transitional cell ca', 'urothelial cancer', 'urothelial carcinoma',\n",
    "        'gynecologic cancer', 'gynecological cancer', 'gynaecologic cancer', 'gynaecological cancer', 'ovarian cancer',\n",
    "       'ovarian carcinoma', 'uterine cancer', 'uterine carcinoma', 'cervical cancer', 'cervical carcinoma', 'colposcop',\n",
    "       'haematological cancer', 'hematological cancer', 'haematological malig', 'hematological malig', 'myelodysplas',\n",
    "       'myeloprolif', 'lymphoprolif', 'leukaemoa', 'leukemia', 'myelofibro', 'thrombocythemia', 'polycythemia vera',\n",
    "       'polycythemia rubra vera', 'thrombocythaemia', 'polycythaemia vera', 'polycythaemia rubra vera', 'lymphoma',\n",
    "       'myeloma', ' gvhd', 'stem cell transpl', 'bone marrow aspirate']\n",
    "\n",
    "spec['onc_text'] = np.where(groups['text'].str.contains('metasta'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['onc_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['onc_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['onc_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb55cc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33920, '1': 259})\n"
     ]
    }
   ],
   "source": [
    "#### RADIOTHERAPY / rx\n",
    "\n",
    "## text\n",
    "spec['rx_text'] = np.where(groups['text'].str.contains(\"radiotherapy\"), \"1\", \"0\")\n",
    "spec['rx_text'] = np.where(groups['text'].str.contains(\"radiation therapy\"), \"1\", \"0\")\n",
    "\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(spec['rx_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4085c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32173, '1': 2006})\n"
     ]
    }
   ],
   "source": [
    "#### BREAST / breast\n",
    "\n",
    "## text\n",
    "text = ['mammog', 'breast ca', 'breast tum', 'invasive lobular carcinoma', ' dcis ', 'ductal carcinoma in situ']\n",
    "\n",
    "spec['breast_text'] = np.where(groups['text'].str.contains(' breast '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['breast_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['breast_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['breast_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5213e79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32422, '1': 1757})\n"
     ]
    }
   ],
   "source": [
    "#### BREAST CANCER / breastca\n",
    "\n",
    "## text\n",
    "text = ['mammog', 'breast ca', 'breast tum', 'invasive lobular carcinoma', ' dcis ', 'ductal carcinoma in situ']\n",
    "\n",
    "spec['breastca_text'] = np.where(groups['text'].str.contains('breast cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['breastca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['breastca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['breastca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d8ff1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33001, '1': 1178})\n"
     ]
    }
   ],
   "source": [
    "#### LUNG CA / lungca\n",
    "\n",
    "## text\n",
    "text = ['lung cancer', 'lung malignancy', 'lung carcinoma', 'lung nodule', 'pulmonary nodule', 'mesothelioma', 'nsclc']\n",
    "\n",
    "spec['lungca_text'] = np.where(groups['text'].str.contains('lung cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['lungca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['lungca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"adenoca\")) , \"1\", spec['lungca_text'])\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"small cell\")) , \"1\", spec['lungca_text'])\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"squamous\")) , \"1\", spec['lungca_text'])\n",
    "spec['lungca_text'] = np.where((groups['text'].str.contains(\"lung\")) &\n",
    "                             (groups['text'].str.contains(\"small-cell\")) , \"1\", spec['lungca_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['lungca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e524470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33275, '1': 904})\n"
     ]
    }
   ],
   "source": [
    "#### NEURO ONC / neuroca\n",
    "\n",
    "## text\n",
    "text = ['neuroonc', 'neuro onc', 'neuro-onc', 'brain cancer', 'brain tumor', 'brain tumour', 'brain malignancy',\n",
    "       'glioma', 'glioblastoma', 'astrocytoma', 'pituitary adenoma', 'acoustic neuroma', 'meningioma',\n",
    "       'cns lymphoma', 'oligodendroglioma', 'meningeal cancer', 'meningeal carcinomatosis']\n",
    "\n",
    "spec['brainca_text'] = np.where(groups['text'].str.contains('brain cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['brainca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['brainca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['brainca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69fa7878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33331, '1': 848})\n"
     ]
    }
   ],
   "source": [
    "#### GI ONC / gica\n",
    "\n",
    "## text\n",
    "text = ['gi cancer', 'gastrointestinal cancer', 'colon cancer', 'colon carcinoma', 'colon polyp', 'colon adeno', 'colon tumo',\n",
    "       'colonic cancer', 'colonic carcinoma', 'colonic adeno', 'colonic polyp', 'colonic tumo', 'colonic neoplasm',\n",
    "        'rectal cancer', 'rectal carcinoma', 'rectal polyp', 'rectal tumo', 'rectal neoplasm', 'bowel cancer', 'bowel neoplasm',\n",
    "       'bowel tumo', 'stomach cancer', 'gastric cancer', 'gastric carcinoma', 'gastric neoplasm', 'gastric tumo',\n",
    "       'esophageal cancer', 'esophageal tumo', 'esophageal neoplasm']\n",
    "\n",
    "spec['gica_text'] = np.where(groups['text'].str.contains('luminal cancer'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['gica_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['gica_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['gica_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1f9928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33682, '1': 497})\n"
     ]
    }
   ],
   "source": [
    "#### HPB ONC / hepca\n",
    "\n",
    "## text\n",
    "text = ['hepatocellular cancer', 'hepatocellular carcinoma', 'hepatic cancer', 'hepatic carcinoma', 'hepatic tumo',\n",
    "       'hepatic neoplasm', 'liver cancer', 'liver carcinoma', 'liver tumo', 'cholangioca', 'pancreatic cancer',\n",
    "       'pancreatic neoplasm', 'pancreatic tumo', 'biliary cancer', 'bile duct cancer']\n",
    "\n",
    "spec['hepca_text'] = np.where(groups['text'].str.contains('cancer of the pancreas'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hepca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hepca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hepca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "633c1220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33506, '1': 673})\n"
     ]
    }
   ],
   "source": [
    "#### PROSTATE ONC / prosca\n",
    "\n",
    "## text\n",
    "text = ['prostate cancer', 'prostate specific antigen', 'prostate carcinoma', 'prostate neoplasm', 'prostate tumo',\n",
    "       'prostate adeno', 'prostatic cancer', 'prostatic neoplasm', 'prostatic tumo', 'prostatic adeno', 'prostatectomy',\n",
    "       ' psa ']\n",
    "\n",
    "spec['prosca_text'] = np.where(groups['text'].str.contains('prostatectomy'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['prosca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['prosca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['prosca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ac6d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33920, '1': 259})\n"
     ]
    }
   ],
   "source": [
    "#### RENAL & BLADDER / renalca\n",
    "\n",
    "## text\n",
    "text = ['kidney cancer', 'kidney tumo', 'renal cell carcinoma', 'renal call cancer', 'renal tumo', 'renal cancer',\n",
    "       'wilms tumo', 'bladder cancer', 'bladder carcinoma', 'transitional cell ca', 'urothelial cancer', 'urothelial carcinoma']\n",
    "\n",
    "spec['renalca_text'] = np.where(groups['text'].str.contains('renal carcinoma'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['renalca_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['renalca_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['renalca_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6aaa489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33810, '1': 369})\n"
     ]
    }
   ],
   "source": [
    "#### GYNAE / gynonc\n",
    "\n",
    "## text\n",
    "text = ['gynecologic cancer', 'gynecological cancer', 'gynaecologic cancer', 'gynaecological cancer', 'ovarian cancer',\n",
    "       'ovarian carcinoma', 'uterine cancer', 'uterine carcinoma', 'cervical cancer', 'cervical carcinoma', 'colposcop',\n",
    "       'endometrial cancer']\n",
    "\n",
    "spec['gynonc_text'] = np.where(groups['text'].str.contains('pap smear'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['gynonc_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['gynonc_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['gynonc_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8418f189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33775, '1': 404})\n"
     ]
    }
   ],
   "source": [
    "#### HAEM / haemonc\n",
    "\n",
    "## text\n",
    "text = ['haematological cancer', 'hematological cancer', 'haematological malig', 'hematological malig', 'myelodysplas',\n",
    "       'myeloprolif', 'lymphoprolif', 'leukaemia', 'leukemia', 'myelofibro', 'thrombocythemia', 'polycythemia vera',\n",
    "       'polycythemia rubra vera', 'thrombocythaemia', 'polycythaemia vera', 'polycythaemia rubra vera', 'lymphoma',\n",
    "       'myeloma', ' gvhd', 'stem cell transpl', 'bone marrow aspirate']\n",
    "\n",
    "spec['haemonc_text'] = np.where(groups['text'].str.contains('bone marrow biopsy'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['haemonc_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['haemonc_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['haemonc_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b77cf66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32061, '1': 2118})\n"
     ]
    }
   ],
   "source": [
    "## PSYCHIATRY / psych\n",
    "\n",
    "## text\n",
    "text = ['psych', 'schizo', 'depressive disorder', 'anxiety disorder', 'stress disorder', 'suicide', 'suicidal', 'mood disorder',\n",
    "        'self harm', 'self-harm', 'self injury', 'self-injury',\n",
    "        'mental disorder', 'hyperactivity disorder', 'hyperactive disorder', 'psychological distress', 'bipolar', \n",
    "       'addiction disorder', 'autism', 'autistic']\n",
    "\n",
    "spec['psych_text'] = np.where(groups['text'].str.contains('mental health'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['psych_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['psych_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['psych_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67194724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33660, '1': 519})\n"
     ]
    }
   ],
   "source": [
    "## SUICIDE / suicide\n",
    "\n",
    "## text\n",
    "text = ['suicide', 'suicidal', 'self harm', 'self-harm', 'self injury', 'self-injury', 'depressive disorder']\n",
    "\n",
    "spec['suicide_text'] = np.where(groups['text'].str.contains('low mood'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['suicide_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['suicide_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['suicide_text'] = np.where((groups['text'].str.contains(\"psych\")) &\n",
    "                             (groups['text'].str.contains(\"depression\")) , \"1\", spec['suicide_text'])\n",
    "spec['suicide_text'] = np.where((groups['text'].str.contains(\"mental\")) &\n",
    "                             (groups['text'].str.contains(\"depression\")) , \"1\", spec['suicide_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['suicide_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b185895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33302, '1': 877})\n"
     ]
    }
   ],
   "source": [
    "## MUSCULOSKELETAL / msk\n",
    "\n",
    "## text\n",
    "text = ['musculoskeletal', 'bone disease', 'bone cyst', 'chondritis', 'fasciitis', 'ankylos', 'osteoarth', 'orthoped',\n",
    "       'orthopaed', 'bursitis', 'synovitis', 'congenital hip', 'joint instability', 'joint stability', 'myositis',\n",
    "       'polymyalgia', 'fibromyalgia', ' gout', 'tendinopath', 'arthro', 'ligament', 'fracture', 'hip surgery',\n",
    "       'hip replacement', 'acetabul', 'cruciate', 'joint space', 'dysplatic hip', 'hip dysplas', 'vertebral', 'discectomy',\n",
    "       'lumbar spine', 'thoracic spine', 'cervical spine', 'whole spine', 'osteoporosis', 'bone mineral density']\n",
    "\n",
    "spec['msk_text'] = np.where(groups['text'].str.contains('broken bone'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['msk_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['msk_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['msk_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5247fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33942, '1': 237})\n"
     ]
    }
   ],
   "source": [
    "#### FRACTURE / frac\n",
    "\n",
    "##text\n",
    "spec['frac_text'] = np.where(groups['text'].str.contains(\"fracture\"), \"1\", \"0\")\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['frac_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "50ac65be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34015, '1': 164})\n"
     ]
    }
   ],
   "source": [
    "## CONNECTIVE TISSUE [C17] / rheum\n",
    "\n",
    "## text\n",
    "text = ['rheumatoid', 'scleroderma', 'wegener', 'polyangiitis', 'churg-strauss', 'lupus', 'connective tissue disease',\n",
    "        'mixed connective tissue', 'polymyositis', 'dermatomyositis', 'sjogren', 'vasculitis', 'vasculitide', 'marfan',\n",
    "       'ehlers-danlos', 'osteogenesis imperfecta']\n",
    "\n",
    "spec['rheum_text'] = np.where(groups['text'].str.contains('rheumatolog'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['rheum_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['rheum_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['rheum_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58806800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32628, '1': 1551})\n"
     ]
    }
   ],
   "source": [
    "## LUMINAL GI / gi\n",
    "\n",
    "## text\n",
    "text = ['gastro', 'gastri', 'intestin', 'duoden', 'colonic', 'colonoscop', 'colitis', 'rectal', 'ileus', 'ileitis',\n",
    "       'crohn', 'esophag', 'proctitis', 'proctolog', 'bowel disease', 'bowel cancer', 'bowel neoplasm' ,'bowel tumo',\n",
    "       'celiac', 'coeliac', 'diverticulitis', 'diverticulosis', 'stomach', 'small bowel', 'large bowel']\n",
    "\n",
    "spec['gi_text'] = np.where(groups['text'].str.contains('gi tract'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['gi_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['gi_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['gi_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35b09f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32964, '1': 1215})\n"
     ]
    }
   ],
   "source": [
    "## HEPATOLOGY (and pancreatobiliary) / hep\n",
    "\n",
    "## text\n",
    "text = ['hepato', 'hepati', 'cholang', 'gallbladder', 'gall bladder', 'biliary' , 'pancreas', 'pancreat', 'wilson disease',\n",
    "       'wilsons disease', 'liver fibrosis' ,'liver cirrhosis', 'nafld', 'hemochromatosis', 'haemochromatosis']\n",
    "\n",
    "spec['hep_text'] = np.where(groups['text'].str.contains(' liver '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hep_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hep_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hep_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "295fda5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 30722, '1': 3457})\n"
     ]
    }
   ],
   "source": [
    "## RESPIRATORY / resp\n",
    "\n",
    "## text\n",
    "text = ['respiratory', 'pneumonia', 'lung cancer', 'lung disease', 'lung nodule', 'pulmonary', 'asthma', 'obstructive sleep ap',\n",
    "       'copd', 'pleura', 'mesothelioma', 'lung fibrosis', 'lung adeno', 'nsclc', 'interstitial lung', 'occupational lung', 'tuberculosis',\n",
    "       'bronch']\n",
    "\n",
    "spec['resp_text'] = np.where(groups['text'].str.contains(' lung '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['resp_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['resp_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['resp_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "931578e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33598, '1': 581})\n"
     ]
    }
   ],
   "source": [
    "#### PNEUMONIA / pneum\n",
    "\n",
    "## text\n",
    "text = ['respiratory infection', 'pulmonary infection', 'pneumonia', 'alveolar consolidation', 'lung consolidation', 'lung infection',\n",
    "       'pulmonary consolidation']\n",
    "\n",
    "spec['pneum_text'] = np.where(groups['text'].str.contains('lower respiratory tract infection'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['pneum_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['pneum_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['pneum_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1cda3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33909, '1': 270})\n"
     ]
    }
   ],
   "source": [
    "#### OBSTRUCTIVE SLEEP / osa\n",
    "\n",
    "## text\n",
    "text = ['obstructive sleep ap', 'sleep apnoea']\n",
    "\n",
    "spec['osa_text'] = np.where(groups['text'].str.contains('sleep apnea'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['osa_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['osa_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['osa_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "161d6d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34126, '1': 53})\n"
     ]
    }
   ],
   "source": [
    "#### PULMONARY EMBOLISM / pe\n",
    "\n",
    "## text\n",
    "text = ['saddle embol', 'pulmonary angiogr']\n",
    "\n",
    "spec['pe_text'] = np.where(groups['text'].str.contains('pulmonary embol'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['pe_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['pe_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['pe_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5465503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33865, '1': 314})\n"
     ]
    }
   ],
   "source": [
    "#### PUBLIC HEALTH / pubh\n",
    "\n",
    "## text\n",
    "spec['pubh_text'] = np.where(groups['text'].str.contains(\"public health\"), \"1\", \"0\")\n",
    "spec['pubh_text'] = np.where(groups['text'].str.contains(\"population health\"), \"1\", spec['pubh_text'])\n",
    "spec['pubh_text'] = np.where(groups['text'].str.contains(\"health protection\"), \"1\", spec['pubh_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['pubh_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "09055c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 26409, '1': 7770})\n"
     ]
    }
   ],
   "source": [
    "## NERVOUS SYSTEM / neuro\n",
    "\n",
    "## text\n",
    "text = ['neuro', 'brain', 'nervous system', 'multiple sclerosis', 'amyotrophic', 'motor neuron disease',\n",
    "       'dementia', 'cognitive impairment', 'alzheimer', 'epilepsy', 'parkinson', 'dyskinesia', 'cerebellar', 'cerebral',\n",
    "       'guillain', 'myelin', 'migraine', 'headache', 'meningeal', 'meningitis', 'encephalitis', 'ischemic stroke', 'ischaemic stroke',\n",
    "       'hemorrhagic stroke', 'haemorrhagic stroke', 'embolic stroke', 'thrombotic stroke', 'myasthenia', 'movement disorder',\n",
    "       'subdural', 'extradural', 'arachnoid', 'glioma', 'astrocytoma', 'glioblast', ' mci ', 'cerebrovascular']\n",
    "\n",
    "spec['neuro_text'] = np.where(groups['text'].str.contains('white matter'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['neuro_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['neuro_text']) #if yes then 1, if no, keep current\n",
    "         \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['neuro_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a435932c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33523, '1': 656})\n"
     ]
    }
   ],
   "source": [
    "#### STROKE/bleed / cva\n",
    "\n",
    "## text\n",
    "text = ['cerebrovascular', 'ischemic stroke', 'ischaemic stroke', 'hemorrhagic stroke', 'haemorrhagic stroke', \n",
    "        'embolic stroke', 'thrombotic stroke', 'subarachnoid hemorrhage', 'subarachnoid haemorrhage', 'cerebral artery stroke',\n",
    "       'cerebral artery infarct', 'malignant middle cerebral', 'malignant mca']\n",
    "\n",
    "spec['cva_text'] = np.where(groups['text'].str.contains(' ich '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['cva_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['cva_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"infarct\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"infarct\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"stroke\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"stroke\")) , \"1\", spec['cva_text'])    \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['cva_text'])   \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"bleed\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"bleed\")) , \"1\", spec['cva_text'])   \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"haemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"haemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cranial\")) &\n",
    "                             (groups['text'].str.contains(\"haemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"brain\")) &\n",
    "                             (groups['text'].str.contains(\"hemorrhage\")) , \"1\", spec['cva_text'])\n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cereb\")) &\n",
    "                             (groups['text'].str.contains(\"hemorrhage\")) , \"1\", spec['cva_text'])   \n",
    "spec['cva_text'] = np.where((groups['text'].str.contains(\"cranial\")) &\n",
    "                             (groups['text'].str.contains(\"hemorrhage\")) , \"1\", spec['cva_text'])\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['cva_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e018284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33357, '1': 822})\n"
     ]
    }
   ],
   "source": [
    "#### EPILEPSY / epilep\n",
    "\n",
    "## text\n",
    "spec['epilep_text'] = np.where(groups['text'].str.contains(\"epilep\"), \"1\", \"0\")\n",
    "spec['epilep_text'] = np.where(groups['text'].str.contains(\"seizure\"), \"1\", spec['epilep_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['epilep_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "27454ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32660, '1': 1519})\n"
     ]
    }
   ],
   "source": [
    "#### DEMENTIA / alzh\n",
    "\n",
    "## text\n",
    "text = ['dementia', 'cognitive impairment', 'alzheimer', 'cognitive dysfunction', 'cognitive decline', 'lewy body',\n",
    "       'huntington', 'progressive supranuclear', 'corticobasal degen']\n",
    "\n",
    "spec['alzh_text'] = np.where(groups['text'].str.contains(' mci '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['alzh_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['alzh_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['alzh_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "448b1bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 30439, '1': 3740})\n"
     ]
    }
   ],
   "source": [
    "## CARDIOVASCULAR / cvs\n",
    "\n",
    "## text\n",
    "text = ['cardiac', 'cardiovascular', 'cardial', 'cardiol', 'carditis', 'cardium', 'atherosclerosis', 'coronary', 'heart disease',\n",
    "       'cardiomegaly', 'cardiomyopathy', 'valve disease', 'mitral', 'tricuspid', 'pulmonary valve', 'aortic', 'atrial', 'heart failure',\n",
    "       'ventricular failure', 'right heart', 'left heart', 'cor pulm', 'hypertension', 'vascular disease', 'arrhythmia', \n",
    "       'vena cava', 'venous insuff', 'echocard', 'electrocard', 'sinus node', 'sinoatrial node', ' ecg', ' ekg', 'ventricular tachy', 'ventricular fibrillation',\n",
    "       'ischemic heart', 'ischaemic heart']\n",
    "\n",
    "spec['cvs_text'] = np.where(groups['text'].str.contains('cardiac'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['cvs_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['cvs_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['cvs_text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3635034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33229, '1': 950})\n"
     ]
    }
   ],
   "source": [
    "#### ISCHAEMIC HEART DISEASE / ihd\n",
    "\n",
    "## text\n",
    "text = ['coronary', 'cardiac risk', 'cardiovascular risk', 'cardiac stent',\n",
    "       'ischemic heart', 'ischaemic heart', 'cardial infarction']\n",
    "\n",
    "spec['ihd_text'] = np.where(groups['text'].str.contains('heart attack'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['ihd_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['ihd_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"ischemi\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"ischaemi\")) , \"1\", spec['ihd_text'])    \n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"infarction\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"infarction\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardia\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"vessel occlusion\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"angio\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"angio\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"atherosclero\")) , \"1\", spec['ihd_text'])\n",
    "spec['ihd_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"atherosclero\")) , \"1\", spec['ihd_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['ihd_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28f5d458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60082</th>\n",
       "      <td>evaluation of risk prediction models of atrial fibrillation from the multi-ethnic study of atherosclerosis mesa atrial fibrillation af is prevalent and strongly associated with higher cardiovascular disease cvd risk machine learning is increasingly used to identify novel predictors of cvd risk, but prediction improvements beyond established risk scores are uncertain we evaluated improvements in predicting 5-year af risk when adding novel candidate variables identified by machine learning to the charge-af enriched score, which includes age, race/ethnicity, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and nt-probnp we included 3,534 participants mean age, 613 years; 520% female with complete data from the prospective multi-ethnic study of atherosclerosis incident af was defined based on study electrocardiograms and hospital discharge diagnosis icd-9 codes, supplemented by medicare claims prediction performance was evaluated using cox regression and a parsimonious model was selected using lasso within 5 years of baseline, 124 participants had incident af compared with the charge-af enriched model c-statistic, 0804, variables identified by machine learning, including biomarkers, cardiac magnetic resonance imaging variables, electrocardiogram variables, and subclinical cvd variables, did not significantly improve prediction a 23-item score derived by machine learning achieved a c-statistic of 0806, whereas a parsimonious model including the clinical risk factors age, weight, current smoking, nt-probnp, coronary artery calcium score, and cardiac troponin-t achieved a c-statistic of 0802 this analysis confirms that the charge-af enriched model and a parsimonious 6-item model performed similarly to a more extensive model derived by machine learning in conclusion, these simple models remain the gold standard for risk prediction of af, although addition of the coronary artery calcium score should be considered</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158310</th>\n",
       "      <td>selection of patients for myocardial perfusion scintigraphy based on fuzzy sets theory applied to clinical-epidemiological data and treadmill test results coronary artery disease cad is a worldwide leading cause of death the standard method for evaluating critical partial occlusions is coronary arteriography, a catheterization technique which is invasive, time consuming, and costly there are noninvasive approaches for the early detection of cad the basis for the noninvasive diagnosis of cad has been laid in a sequential analysis of the risk factors, and the results of the treadmill test and myocardial perfusion scintigraphy mps many investigators have demonstrated that the diagnostic applications of mps are appropriate for patients who have an intermediate likelihood of disease although this information is useful, it is only partially utilized in clinical practice due to the difficulty to properly classify the patients since the seminal work of lotfi zadeh, fuzzy logic has been applied in numerous areas in the present study, we proposed and tested a model to select patients for mps based on fuzzy sets theory a group of 1053 patients was used to develop the model and another group of 1045 patients was used to test it receiver operating characteristic curves were used to compare the performance of the fuzzy model against expert physician opinions, and showed that the performance of the fuzzy model was equal or superior to that of the physicians therefore, we conclude that the fuzzy model could be a useful tool to assist the general practitioner in the selection of patients for mps</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103279</th>\n",
       "      <td>electrocardiographic diagnosis of st segment elevation myocardial infarction: an evaluation of three automated interpretation algorithms to assess the validity of three different computerized electrocardiogram ecg interpretation algorithms in correctly identifying stemi patients in the prehospital environment who require emergent cardiac intervention</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62798</th>\n",
       "      <td>machine learning versus traditional risk stratification methods in acute coronary syndrome: a pooled randomized clinical trial analysis traditional statistical models allow population based inferences and comparisons machine learning ml explores datasets to develop algorithms that do not assume linear relationships between variables and outcomes and that may account for higher order interactions to make individualized outcome predictions to evaluate the performance of machine learning models compared to traditional risk stratification methods for the prediction of major adverse cardiovascular events mace and bleeding in patients with acute coronary syndrome acs that are treated with antithrombotic therapy data on 24,178 acs patients were pooled from four randomized controlled trials the super learner ensemble algorithm selected weights for 23 machine learning models and was compared to traditional models the efficacy endpoint was a composite of cardiovascular death, myocardial infarction, or stroke the safety endpoint was a composite of timi major and minor bleeding or bleeding requiring medical attention for the mace outcome, the super learner model produced a higher c-statistic 0734 than logistic regression 0714, the timi risk score 0489, and a new cardiovascular risk score developed in the dataset 0644 for the bleeding outcome, the super learner demonstrated a similar c-statistic as the logistic regression model 0670 vs 0671 the machine learning risk estimates were highly calibrated with observed efficacy and bleeding outcomes hosmer-lemeshow p value = 0692 and 0970, respectively the super learner algorithm was highly calibrated on both efficacy and safety outcomes and produced the highest c-statistic for prediction of mace compared to traditional risk stratification methods this analysis demonstrates a contemporary application of machine learning to guide patient-level antithrombotic therapy treatment decisionsclinical trial registration atlas acs-2 timi 46: https://clinicaltrialsgov/ct2/show/nct00402597 unique identifier: nct00402597 atlas acs-2 timi 51: https://clinicaltrialsgov/ct2/show/nct00809965 unique identifier: nct00809965 gemini acs-1: https://clinicaltrialsgov/ct2/show/nct02293395 unique identifier: nct02293395 pioneer-af pci: https://clinicaltrialsgov/ct2/show/nct01830543 unique identifier: nct01830543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70462</th>\n",
       "      <td>predicting coronary artery disease: a comparison between two data mining algorithms cardiovascular diseases cads are the first leading cause of death across the world world health organization has estimated that morality rate caused by heart diseases will mount to 23 million cases by 2030 hence, the use of data mining algorithms could be useful in predicting coronary artery diseases therefore, the present study aimed to compare the positive predictive value ppv of cad using artificial neural network ann and svm algorithms and their distinction in terms of predicting cad in the selected hospitals</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96346</th>\n",
       "      <td>automatic determination of cardiovascular risk by ct attenuation correction maps in rb-82 pet/ct we investigated fully automatic coronary artery calcium cac scoring and cardiovascular disease cvd risk categorization from ct attenuation correction ctac acquired at rest and stress during cardiac pet/ct and compared it with manual annotations in ctac and with dedicated calcium scoring ct csct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36752</th>\n",
       "      <td>convolution pyramid network: a classification network on coronary artery angiogram images with the development of convolutional neural network, the classification on ordinary natural images has made remarkable progress by using single feature maps however, it is difficult to always produce good results on coronary artery angiograms because there is a lot of photographing noise and small class gaps between the classification targets on angiograms in this paper, we propose a new network to enhance the richness and relevance of features in the training process by using multiple convolutions with different kernel sizes, which can improve the final classification result our network has a strong generalization ability, that is, it can perform a variety of classification tasks on angiograms better compared with some state-of-the-art image classification networks, the classification recall increases by 305% and precision increases by 191% in the best results of our network</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88735</th>\n",
       "      <td>application of stacked convolutional and long short-term memory network for accurate identification of cad ecg signals coronary artery disease cad is the most common cause of heart disease globally this is because there is no symptom exhibited in its initial phase until the disease progresses to an advanced stage the electrocardiogram ecg is a widely accessible diagnostic tool to diagnose cad that captures abnormal activity of the heart however, it lacks diagnostic sensitivity one reason is that, it is very challenging to visually interpret the ecg signal due to its very low amplitude hence, identification of abnormal ecg morphology by clinicians may be prone to error thus, it is essential to develop a software which can provide an automated and objective interpretation of the ecg signal this paper proposes the implementation of long short-term memory lstm network with convolutional neural network cnn to automatically diagnose cad ecg signals accurately our proposed deep learning model is able to detect cad ecg signals with a diagnostic accuracy of 9985% with blindfold strategy the developed prototype model is ready to be tested with an appropriate huge database before the clinical usage</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97442</th>\n",
       "      <td>computer aided decision making for heart disease detection using hybrid neural network-genetic algorithm cardiovascular disease is one of the most rampant causes of death around the world and was deemed as a major illness in middle and old ages coronary artery disease, in particular, is a widespread cardiovascular malady entailing high mortality rates angiography is, more often than not, regarded as the best method for the diagnosis of coronary artery disease; on the other hand, it is associated with high costs and major side effects much research has, therefore, been conducted using machine learning and data mining so as to seek alternative modalities accordingly, we herein propose a highly accurate hybrid method for the diagnosis of coronary artery disease as a matter of fact, the proposed method is able to increase the performance of neural network by approximately 10% through enhancing its initial weights using genetic algorithm which suggests better weights for neural network making use of such methodology, we achieved accuracy, sensitivity and specificity rates of 9385%, 97% and 92% respectively, on z-alizadeh sani dataset</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156758</th>\n",
       "      <td>localizing calcifications in cardiac ct data sets using a new vessel segmentation approach the new generation of multislice computed tomography ct scanners allows for the acquisition of high-resolution images of the heart based on that image data, the heart can be analyzed in a noninvasive way-improving the diagnosis of cardiovascular malfunctions on one hand, and the planning of an eventually necessary intervention on the other one important parameter for the evaluation of the severity of a coronary artery disease is the number and localization of calcifications hard plaques this work presents a method for localizing these calcifications by employing a newly developed vessel segmentation approach this extraction technique has been developed for, and tested with, contrast-enhanced ct data sets of the heart the algorithm provides enough information to compute the vessel diameter along the extracted segment an approach for automatically detecting calcified regions that combines diameter information and gray value analysis is presented in addition, specially adapted methods for the visualization of these analysis results are described</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45529</th>\n",
       "      <td>initial evaluation of a convolutional neural network used for noninvasive assessment of coronary artery disease severity from coronary computed tomography angiography data coronary computed tomography angiography cta has one of the highest diagnostic sensitivities for detection of the significance of coronary artery disease cad; however, sensitivity is moderate and may result in increased catheterization rates we performed an efficacy study to determine whether a trained machine learning algorithm that uses coronary cta data may improve cad diagnosis accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103897</th>\n",
       "      <td>regional infarction identification from cardiac ct images: a computer-aided biomechanical approach regional infarction identification is important for heart disease diagnosis and management, and myocardial deformation has been shown to be effective for this purpose although tagged and strain-encoded mr images can provide such measurements, they are uncommon in clinical routine on the contrary, cardiac ct images are more available with lower costs, but they only provide motion of cardiac boundaries and additional constraints are required to obtain the myocardial strains the goal of this study is to verify the potential of contrast-enhanced ct images on computer-aided regional infarction identification</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11776</th>\n",
       "      <td>machine learning predictive models for coronary artery disease coronary artery disease cad is the commonest type of heart disease and over 80% of the deaths resulted from the diseases occurred in developing countries including nigeria, with majority being in those victims are below 70 years of age though, cad is not a well known disease in nigeria but however in year 2014, 282% of the total of deaths occurred in the country were due to the disease in this study, a machine leaning predictive models for cad has been developed with diagnostic cad dataset obtained in the two general hospitals in kano state-nigeria the dataset applied on machine learning algorithms which include support vector machine, k nearest neighbor, random tree, naïve bayes, gradient boosting and logistic regression algorithms to build the predictive models and the models were evaluated based accuracy, specificity, sensitivity and receiver operating curve roc performance evaluation techniques in terms of accuracy random forest-based machine learning model emerged to be the best model with 9204%, for specificity naive bayes based machine learning model emerged to be the best model with 9240%, while for sensitivity support vector machine based machine learning model emerged to be the best model with 8734% and for roc, random forest-based machine learning model emerged to be the best model with 9220% the decision tree generated with random forest machine learning algorithm which happened to be best model in terms accuracy and roc can be converted into production rules and be used develop expert system for diagnosis of cad patients in nigeria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42694</th>\n",
       "      <td>radiomics of coronary artery calcium in the framingham heart study to extract radiomic features from coronary artery calcium cac on ct images and to determine whether this approach could improve the ability to identify individuals at risk for a composite endpoint of clinical events</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>deep learning for vessel-specific coronary artery calcium scoring: validation on a multi-centre dataset to present and validate a fully automated, deep learning dl-based branch-wise coronary artery calcium cac scoring algorithm on a multi-centre dataset</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70340</th>\n",
       "      <td>an artificial intelligence approach to early predict non-st-elevation myocardial infarction patients with chest pain hospital admission rate for the patients with chest pain has already been increased worldwide but no existing risk score has been designed to stratify non-st-elevation myocardial infarction nstemi from non-cardiogenic chest pain clinical diagnosis of chest pain in the emergency department is always highly subjective and variable we, therefore, aimed to develop an artificial intelligence approach to predict stable nstemi that would give valuable insight to reduce misdiagnosis in the real clinical setting</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41563</th>\n",
       "      <td>classification models for spect myocardial perfusion imaging the main goal of this work is to develop computer-aided classification models for single-photon emission computed tomography spect myocardial perfusion imaging mpi to identify perfusion abnormalities myocardial ischemia and/or infarction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16904</th>\n",
       "      <td>identification of risk of cardiovascular disease by automatic quantification of coronary artery calcifications on radiotherapy planning ct scans in patients with breast cancer cardiovascular disease cvd is common in patients treated for breast cancer, especially in patients treated with systemic treatment and radiotherapy and in those with preexisting cvd risk factors coronary artery calcium cac, a strong independent cvd risk factor, can be automatically quantified on radiotherapy planning computed tomography ct scans and may help identify patients at increased cvd risk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41802</th>\n",
       "      <td>improved cardiovascular risk prediction in patients with end-stage renal disease on hemodialysis using machine learning modeling and circulating microribonucleic acids &lt;b&gt;rationale:&lt;/b&gt; to test whether novel biomarkers, such as microribonucleic acids mirnas, and nonstandard predictive models, such as decision tree learning, provide useful information for medical decision-making in patients on hemodialysis hd &lt;b&gt;methods:&lt;/b&gt; samples from patients with end-stage renal disease receiving hd included in the aurora trial were investigated n=810 the study included two independent phases: phase i matched cases and controls, n=410 and phase ii unmatched cases and controls, n=400 the composite endpoint was cardiovascular death, nonfatal myocardial infarction or nonfatal stroke mirna quantification was performed using mirna sequencing and rt-qpcr the cart algorithm was used to construct regression tree models a bagging-based procedure was used for validation &lt;b&gt;results:&lt;/b&gt; in phase i, mirna sequencing in a subset of samples n=20 revealed mir-632 as a candidate fold change=29 mir-632 was associated with the endpoint, even after adjusting for confounding factors hr from 143 to 153 these findings were not reproduced in phase ii regression tree models identified eight patient subgroups with specific risk patterns mir-186-5p and mir-632 entered the tree by redefining two risk groups: patients older than 64 years and with hscrp&lt;0827 mg/l and diabetic patients younger than 64 years mirnas improved the discrimination accuracy at the beginning of the follow-up 24 months compared to the models without mirnas integrated auc iauc=071 &lt;b&gt;conclusions:&lt;/b&gt; the circulating mirna profile complements conventional risk factors to identify specific cardiovascular risk patterns among patients receiving maintenance hd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102909</th>\n",
       "      <td>machine learning for prediction of all-cause mortality in patients with suspected coronary artery disease: a 5-year multicentre prospective registry analysis traditional prognostic risk assessment in patients undergoing non-invasive imaging is based upon a limited selection of clinical and imaging findings machine learning ml can consider a greater number and complexity of variables therefore, we investigated the feasibility and accuracy of ml to predict 5-year all-cause mortality acm in patients undergoing coronary computed tomographic angiography ccta, and compared the performance to existing clinical or ccta metrics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "60082                                                                                                                                                                                                                                                                                                                                                                    evaluation of risk prediction models of atrial fibrillation from the multi-ethnic study of atherosclerosis mesa atrial fibrillation af is prevalent and strongly associated with higher cardiovascular disease cvd risk machine learning is increasingly used to identify novel predictors of cvd risk, but prediction improvements beyond established risk scores are uncertain we evaluated improvements in predicting 5-year af risk when adding novel candidate variables identified by machine learning to the charge-af enriched score, which includes age, race/ethnicity, height, weight, systolic and diastolic blood pressure, current smoking, use of antihypertensive medication, diabetes, and nt-probnp we included 3,534 participants mean age, 613 years; 520% female with complete data from the prospective multi-ethnic study of atherosclerosis incident af was defined based on study electrocardiograms and hospital discharge diagnosis icd-9 codes, supplemented by medicare claims prediction performance was evaluated using cox regression and a parsimonious model was selected using lasso within 5 years of baseline, 124 participants had incident af compared with the charge-af enriched model c-statistic, 0804, variables identified by machine learning, including biomarkers, cardiac magnetic resonance imaging variables, electrocardiogram variables, and subclinical cvd variables, did not significantly improve prediction a 23-item score derived by machine learning achieved a c-statistic of 0806, whereas a parsimonious model including the clinical risk factors age, weight, current smoking, nt-probnp, coronary artery calcium score, and cardiac troponin-t achieved a c-statistic of 0802 this analysis confirms that the charge-af enriched model and a parsimonious 6-item model performed similarly to a more extensive model derived by machine learning in conclusion, these simple models remain the gold standard for risk prediction of af, although addition of the coronary artery calcium score should be considered   \n",
       "158310                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    selection of patients for myocardial perfusion scintigraphy based on fuzzy sets theory applied to clinical-epidemiological data and treadmill test results coronary artery disease cad is a worldwide leading cause of death the standard method for evaluating critical partial occlusions is coronary arteriography, a catheterization technique which is invasive, time consuming, and costly there are noninvasive approaches for the early detection of cad the basis for the noninvasive diagnosis of cad has been laid in a sequential analysis of the risk factors, and the results of the treadmill test and myocardial perfusion scintigraphy mps many investigators have demonstrated that the diagnostic applications of mps are appropriate for patients who have an intermediate likelihood of disease although this information is useful, it is only partially utilized in clinical practice due to the difficulty to properly classify the patients since the seminal work of lotfi zadeh, fuzzy logic has been applied in numerous areas in the present study, we proposed and tested a model to select patients for mps based on fuzzy sets theory a group of 1053 patients was used to develop the model and another group of 1045 patients was used to test it receiver operating characteristic curves were used to compare the performance of the fuzzy model against expert physician opinions, and showed that the performance of the fuzzy model was equal or superior to that of the physicians therefore, we conclude that the fuzzy model could be a useful tool to assist the general practitioner in the selection of patients for mps   \n",
       "103279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         electrocardiographic diagnosis of st segment elevation myocardial infarction: an evaluation of three automated interpretation algorithms to assess the validity of three different computerized electrocardiogram ecg interpretation algorithms in correctly identifying stemi patients in the prehospital environment who require emergent cardiac intervention   \n",
       "62798   machine learning versus traditional risk stratification methods in acute coronary syndrome: a pooled randomized clinical trial analysis traditional statistical models allow population based inferences and comparisons machine learning ml explores datasets to develop algorithms that do not assume linear relationships between variables and outcomes and that may account for higher order interactions to make individualized outcome predictions to evaluate the performance of machine learning models compared to traditional risk stratification methods for the prediction of major adverse cardiovascular events mace and bleeding in patients with acute coronary syndrome acs that are treated with antithrombotic therapy data on 24,178 acs patients were pooled from four randomized controlled trials the super learner ensemble algorithm selected weights for 23 machine learning models and was compared to traditional models the efficacy endpoint was a composite of cardiovascular death, myocardial infarction, or stroke the safety endpoint was a composite of timi major and minor bleeding or bleeding requiring medical attention for the mace outcome, the super learner model produced a higher c-statistic 0734 than logistic regression 0714, the timi risk score 0489, and a new cardiovascular risk score developed in the dataset 0644 for the bleeding outcome, the super learner demonstrated a similar c-statistic as the logistic regression model 0670 vs 0671 the machine learning risk estimates were highly calibrated with observed efficacy and bleeding outcomes hosmer-lemeshow p value = 0692 and 0970, respectively the super learner algorithm was highly calibrated on both efficacy and safety outcomes and produced the highest c-statistic for prediction of mace compared to traditional risk stratification methods this analysis demonstrates a contemporary application of machine learning to guide patient-level antithrombotic therapy treatment decisionsclinical trial registration atlas acs-2 timi 46: https://clinicaltrialsgov/ct2/show/nct00402597 unique identifier: nct00402597 atlas acs-2 timi 51: https://clinicaltrialsgov/ct2/show/nct00809965 unique identifier: nct00809965 gemini acs-1: https://clinicaltrialsgov/ct2/show/nct02293395 unique identifier: nct02293395 pioneer-af pci: https://clinicaltrialsgov/ct2/show/nct01830543 unique identifier: nct01830543   \n",
       "70462                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                predicting coronary artery disease: a comparison between two data mining algorithms cardiovascular diseases cads are the first leading cause of death across the world world health organization has estimated that morality rate caused by heart diseases will mount to 23 million cases by 2030 hence, the use of data mining algorithms could be useful in predicting coronary artery diseases therefore, the present study aimed to compare the positive predictive value ppv of cad using artificial neural network ann and svm algorithms and their distinction in terms of predicting cad in the selected hospitals   \n",
       "96346                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  automatic determination of cardiovascular risk by ct attenuation correction maps in rb-82 pet/ct we investigated fully automatic coronary artery calcium cac scoring and cardiovascular disease cvd risk categorization from ct attenuation correction ctac acquired at rest and stress during cardiac pet/ct and compared it with manual annotations in ctac and with dedicated calcium scoring ct csct   \n",
       "36752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       convolution pyramid network: a classification network on coronary artery angiogram images with the development of convolutional neural network, the classification on ordinary natural images has made remarkable progress by using single feature maps however, it is difficult to always produce good results on coronary artery angiograms because there is a lot of photographing noise and small class gaps between the classification targets on angiograms in this paper, we propose a new network to enhance the richness and relevance of features in the training process by using multiple convolutions with different kernel sizes, which can improve the final classification result our network has a strong generalization ability, that is, it can perform a variety of classification tasks on angiograms better compared with some state-of-the-art image classification networks, the classification recall increases by 305% and precision increases by 191% in the best results of our network   \n",
       "88735                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    application of stacked convolutional and long short-term memory network for accurate identification of cad ecg signals coronary artery disease cad is the most common cause of heart disease globally this is because there is no symptom exhibited in its initial phase until the disease progresses to an advanced stage the electrocardiogram ecg is a widely accessible diagnostic tool to diagnose cad that captures abnormal activity of the heart however, it lacks diagnostic sensitivity one reason is that, it is very challenging to visually interpret the ecg signal due to its very low amplitude hence, identification of abnormal ecg morphology by clinicians may be prone to error thus, it is essential to develop a software which can provide an automated and objective interpretation of the ecg signal this paper proposes the implementation of long short-term memory lstm network with convolutional neural network cnn to automatically diagnose cad ecg signals accurately our proposed deep learning model is able to detect cad ecg signals with a diagnostic accuracy of 9985% with blindfold strategy the developed prototype model is ready to be tested with an appropriate huge database before the clinical usage   \n",
       "97442                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                computer aided decision making for heart disease detection using hybrid neural network-genetic algorithm cardiovascular disease is one of the most rampant causes of death around the world and was deemed as a major illness in middle and old ages coronary artery disease, in particular, is a widespread cardiovascular malady entailing high mortality rates angiography is, more often than not, regarded as the best method for the diagnosis of coronary artery disease; on the other hand, it is associated with high costs and major side effects much research has, therefore, been conducted using machine learning and data mining so as to seek alternative modalities accordingly, we herein propose a highly accurate hybrid method for the diagnosis of coronary artery disease as a matter of fact, the proposed method is able to increase the performance of neural network by approximately 10% through enhancing its initial weights using genetic algorithm which suggests better weights for neural network making use of such methodology, we achieved accuracy, sensitivity and specificity rates of 9385%, 97% and 92% respectively, on z-alizadeh sani dataset   \n",
       "156758                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            localizing calcifications in cardiac ct data sets using a new vessel segmentation approach the new generation of multislice computed tomography ct scanners allows for the acquisition of high-resolution images of the heart based on that image data, the heart can be analyzed in a noninvasive way-improving the diagnosis of cardiovascular malfunctions on one hand, and the planning of an eventually necessary intervention on the other one important parameter for the evaluation of the severity of a coronary artery disease is the number and localization of calcifications hard plaques this work presents a method for localizing these calcifications by employing a newly developed vessel segmentation approach this extraction technique has been developed for, and tested with, contrast-enhanced ct data sets of the heart the algorithm provides enough information to compute the vessel diameter along the extracted segment an approach for automatically detecting calcified regions that combines diameter information and gray value analysis is presented in addition, specially adapted methods for the visualization of these analysis results are described   \n",
       "45529                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     initial evaluation of a convolutional neural network used for noninvasive assessment of coronary artery disease severity from coronary computed tomography angiography data coronary computed tomography angiography cta has one of the highest diagnostic sensitivities for detection of the significance of coronary artery disease cad; however, sensitivity is moderate and may result in increased catheterization rates we performed an efficacy study to determine whether a trained machine learning algorithm that uses coronary cta data may improve cad diagnosis accuracy   \n",
       "103897                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    regional infarction identification from cardiac ct images: a computer-aided biomechanical approach regional infarction identification is important for heart disease diagnosis and management, and myocardial deformation has been shown to be effective for this purpose although tagged and strain-encoded mr images can provide such measurements, they are uncommon in clinical routine on the contrary, cardiac ct images are more available with lower costs, but they only provide motion of cardiac boundaries and additional constraints are required to obtain the myocardial strains the goal of this study is to verify the potential of contrast-enhanced ct images on computer-aided regional infarction identification   \n",
       "11776                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         machine learning predictive models for coronary artery disease coronary artery disease cad is the commonest type of heart disease and over 80% of the deaths resulted from the diseases occurred in developing countries including nigeria, with majority being in those victims are below 70 years of age though, cad is not a well known disease in nigeria but however in year 2014, 282% of the total of deaths occurred in the country were due to the disease in this study, a machine leaning predictive models for cad has been developed with diagnostic cad dataset obtained in the two general hospitals in kano state-nigeria the dataset applied on machine learning algorithms which include support vector machine, k nearest neighbor, random tree, naïve bayes, gradient boosting and logistic regression algorithms to build the predictive models and the models were evaluated based accuracy, specificity, sensitivity and receiver operating curve roc performance evaluation techniques in terms of accuracy random forest-based machine learning model emerged to be the best model with 9204%, for specificity naive bayes based machine learning model emerged to be the best model with 9240%, while for sensitivity support vector machine based machine learning model emerged to be the best model with 8734% and for roc, random forest-based machine learning model emerged to be the best model with 9220% the decision tree generated with random forest machine learning algorithm which happened to be best model in terms accuracy and roc can be converted into production rules and be used develop expert system for diagnosis of cad patients in nigeria   \n",
       "42694                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                radiomics of coronary artery calcium in the framingham heart study to extract radiomic features from coronary artery calcium cac on ct images and to determine whether this approach could improve the ability to identify individuals at risk for a composite endpoint of clinical events   \n",
       "8514                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              deep learning for vessel-specific coronary artery calcium scoring: validation on a multi-centre dataset to present and validate a fully automated, deep learning dl-based branch-wise coronary artery calcium cac scoring algorithm on a multi-centre dataset   \n",
       "70340                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         an artificial intelligence approach to early predict non-st-elevation myocardial infarction patients with chest pain hospital admission rate for the patients with chest pain has already been increased worldwide but no existing risk score has been designed to stratify non-st-elevation myocardial infarction nstemi from non-cardiogenic chest pain clinical diagnosis of chest pain in the emergency department is always highly subjective and variable we, therefore, aimed to develop an artificial intelligence approach to predict stable nstemi that would give valuable insight to reduce misdiagnosis in the real clinical setting   \n",
       "41563                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                classification models for spect myocardial perfusion imaging the main goal of this work is to develop computer-aided classification models for single-photon emission computed tomography spect myocardial perfusion imaging mpi to identify perfusion abnormalities myocardial ischemia and/or infarction   \n",
       "16904                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          identification of risk of cardiovascular disease by automatic quantification of coronary artery calcifications on radiotherapy planning ct scans in patients with breast cancer cardiovascular disease cvd is common in patients treated for breast cancer, especially in patients treated with systemic treatment and radiotherapy and in those with preexisting cvd risk factors coronary artery calcium cac, a strong independent cvd risk factor, can be automatically quantified on radiotherapy planning computed tomography ct scans and may help identify patients at increased cvd risk   \n",
       "41802                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 improved cardiovascular risk prediction in patients with end-stage renal disease on hemodialysis using machine learning modeling and circulating microribonucleic acids <b>rationale:</b> to test whether novel biomarkers, such as microribonucleic acids mirnas, and nonstandard predictive models, such as decision tree learning, provide useful information for medical decision-making in patients on hemodialysis hd <b>methods:</b> samples from patients with end-stage renal disease receiving hd included in the aurora trial were investigated n=810 the study included two independent phases: phase i matched cases and controls, n=410 and phase ii unmatched cases and controls, n=400 the composite endpoint was cardiovascular death, nonfatal myocardial infarction or nonfatal stroke mirna quantification was performed using mirna sequencing and rt-qpcr the cart algorithm was used to construct regression tree models a bagging-based procedure was used for validation <b>results:</b> in phase i, mirna sequencing in a subset of samples n=20 revealed mir-632 as a candidate fold change=29 mir-632 was associated with the endpoint, even after adjusting for confounding factors hr from 143 to 153 these findings were not reproduced in phase ii regression tree models identified eight patient subgroups with specific risk patterns mir-186-5p and mir-632 entered the tree by redefining two risk groups: patients older than 64 years and with hscrp<0827 mg/l and diabetic patients younger than 64 years mirnas improved the discrimination accuracy at the beginning of the follow-up 24 months compared to the models without mirnas integrated auc iauc=071 <b>conclusions:</b> the circulating mirna profile complements conventional risk factors to identify specific cardiovascular risk patterns among patients receiving maintenance hd   \n",
       "102909                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       machine learning for prediction of all-cause mortality in patients with suspected coronary artery disease: a 5-year multicentre prospective registry analysis traditional prognostic risk assessment in patients undergoing non-invasive imaging is based upon a limited selection of clinical and imaging findings machine learning ml can consider a greater number and complexity of variables therefore, we investigated the feasibility and accuracy of ml to predict 5-year all-cause mortality acm in patients undergoing coronary computed tomographic angiography ccta, and compared the performance to existing clinical or ccta metrics   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "60082         0       0       1           0          0        0       0   \n",
       "158310        0       0       0           0          0        0       0   \n",
       "103279        0       1       0           0          0        0       0   \n",
       "62798         0       0       0           0          0        0       0   \n",
       "70462         0       0       0           0          0        0       0   \n",
       "96346         0       0       0           0          0        0       0   \n",
       "36752         0       0       0           0          0        0       0   \n",
       "88735         0       0       0           0          0        0       0   \n",
       "97442         0       0       0           0          0        0       0   \n",
       "156758        0       0       0           0          0        0       0   \n",
       "45529         0       0       0           0          0        0       0   \n",
       "103897        0       0       0           0          0        0       0   \n",
       "11776         0       0       0           0          0        0       0   \n",
       "42694         0       0       0           0          0        0       0   \n",
       "8514          0       0       0           0          0        0       0   \n",
       "70340         0       1       0           0          0        0       0   \n",
       "41563         0       0       0           0          0        0       0   \n",
       "16904         0       0       0           0          0        0       0   \n",
       "41802         0       0       1           0          0        0       0   \n",
       "102909        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "60082            0            0         0           0        0       0   \n",
       "158310           0            0         0           0        0       0   \n",
       "103279           0            0         0           0        0       0   \n",
       "62798            0            0         0           0        0       0   \n",
       "70462            0            0         0           0        0       0   \n",
       "96346            0            0         0           0        0       0   \n",
       "36752            0            0         0           0        0       0   \n",
       "88735            0            0         0           0        0       0   \n",
       "97442            0            0         0           0        0       0   \n",
       "156758           0            0         0           0        0       0   \n",
       "45529            0            0         0           0        0       0   \n",
       "103897           0            0         0           0        0       0   \n",
       "11776            0            0         0           0        0       0   \n",
       "42694            0            0         0           0        0       0   \n",
       "8514             0            0         0           0        0       0   \n",
       "70340            0            0         0           0        0       0   \n",
       "41563            0            0         0           0        0       0   \n",
       "16904            0            0         0           0        1       0   \n",
       "41802            0            0         0           0        0       0   \n",
       "102909           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "60082            0             0           0            0         0   \n",
       "158310           0             0           0            0         0   \n",
       "103279           0             0           0            0         0   \n",
       "62798            0             0           0            0         0   \n",
       "70462            0             0           0            0         0   \n",
       "96346            0             0           0            0         0   \n",
       "36752            0             0           0            0         0   \n",
       "88735            0             0           0            0         0   \n",
       "97442            0             0           0            0         0   \n",
       "156758           0             0           0            0         0   \n",
       "45529            0             0           0            0         0   \n",
       "103897           0             0           0            0         0   \n",
       "11776            0             0           0            0         0   \n",
       "42694            0             0           0            0         0   \n",
       "8514             0             0           0            0         0   \n",
       "70340            0             0           0            0         0   \n",
       "41563            0             0           0            0         0   \n",
       "16904            1             1           0            0         0   \n",
       "41802            0             0           0            0         0   \n",
       "102909           0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "60082           0           0            0           0            0   \n",
       "158310          0           0            0           0            0   \n",
       "103279          0           0            0           0            0   \n",
       "62798           0           0            0           0            0   \n",
       "70462           0           0            0           0            0   \n",
       "96346           0           0            0           0            0   \n",
       "36752           0           0            0           0            0   \n",
       "88735           0           0            0           0            0   \n",
       "97442           0           0            0           0            0   \n",
       "156758          0           0            0           0            0   \n",
       "45529           0           0            0           0            0   \n",
       "103897          0           0            0           0            0   \n",
       "11776           0           0            0           0            0   \n",
       "42694           0           0            0           0            0   \n",
       "8514            0           0            0           0            0   \n",
       "70340           0           0            0           0            0   \n",
       "41563           0           0            0           0            0   \n",
       "16904           0           0            0           0            0   \n",
       "41802           0           0            0           0            0   \n",
       "102909          0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "60082           0            0        0         0          0       0        0   \n",
       "158310          0            0        0         0          0       0        0   \n",
       "103279          0            0        0         0          0       0        0   \n",
       "62798           0            0        0         0          0       0        0   \n",
       "70462           0            0        0         0          0       0        0   \n",
       "96346           0            0        0         0          0       0        0   \n",
       "36752           0            0        0         0          0       0        0   \n",
       "88735           0            0        0         0          0       0        0   \n",
       "97442           0            0        0         0          0       0        0   \n",
       "156758          0            0        0         0          0       0        0   \n",
       "45529           0            0        0         0          0       0        0   \n",
       "103897          0            0        0         0          0       0        0   \n",
       "11776           0            0        0         0          0       0        0   \n",
       "42694           0            0        0         0          0       0        0   \n",
       "8514            0            0        0         0          0       0        0   \n",
       "70340           0            0        0         0          0       0        0   \n",
       "41563           0            0        0         0          0       0        0   \n",
       "16904           0            0        0         0          0       0        0   \n",
       "41802           0            0        0         0          0       0        0   \n",
       "102909          0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "60082          0          0        0       0         0          0        0   \n",
       "158310         0          0        0       0         0          0        0   \n",
       "103279         0          0        0       0         0          0        0   \n",
       "62798          0          0        0       0         0          0        0   \n",
       "70462          0          0        0       0         0          0        0   \n",
       "96346          0          0        0       0         0          0        0   \n",
       "36752          0          0        0       0         0          0        0   \n",
       "88735          0          0        0       0         0          0        0   \n",
       "97442          0          0        0       0         0          0        0   \n",
       "156758         0          0        0       0         0          0        0   \n",
       "45529          0          0        0       0         0          0        0   \n",
       "103897         0          0        0       0         0          0        0   \n",
       "11776          0          0        0       0         0          0        0   \n",
       "42694          0          0        0       0         0          0        0   \n",
       "8514           0          0        0       0         0          0        0   \n",
       "70340          0          0        0       0         0          0        0   \n",
       "41563          0          0        0       0         0          0        0   \n",
       "16904          0          0        0       0         0          0        0   \n",
       "41802          0          0        0       0         0          0        0   \n",
       "102909         0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text  \n",
       "60082            0         0        1        1  \n",
       "158310           0         0        1        1  \n",
       "103279           0         0        1        1  \n",
       "62798            0         0        1        1  \n",
       "70462            0         0        1        1  \n",
       "96346            0         0        1        1  \n",
       "36752            0         0        1        1  \n",
       "88735            0         0        1        1  \n",
       "97442            0         0        1        1  \n",
       "156758           0         0        1        1  \n",
       "45529            0         0        1        1  \n",
       "103897           0         0        1        1  \n",
       "11776            0         0        1        1  \n",
       "42694            0         0        1        1  \n",
       "8514             0         0        1        1  \n",
       "70340            0         0        1        1  \n",
       "41563            0         0        1        1  \n",
       "16904            0         0        1        1  \n",
       "41802            0         0        1        1  \n",
       "102909           0         0        1        1  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['ihd_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1186dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33731, '1': 448})\n"
     ]
    }
   ],
   "source": [
    "#### HEART FAILURE or VENTRICULAR FUNCTION / hf\n",
    "\n",
    "## text\n",
    "text = ['heart failure', 'cardiac failure', 'ejection fraction', 'ventricular dysfunction', 'cardiac dysfunction']\n",
    "\n",
    "spec['hf_text'] = np.where(groups['text'].str.contains(' lvf '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['hf_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['hf_text']) #if yes then 1, if no, keep current\n",
    "    \n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"left ventric\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"right ventric\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"left ventric\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"right ventric\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"diastolic\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"diastolic\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"systolic\")) &\n",
    "                             (groups['text'].str.contains(\"function\")) , \"1\", spec['hf_text'])\n",
    "spec['hf_text'] = np.where((groups['text'].str.contains(\"systolic\")) &\n",
    "                             (groups['text'].str.contains(\"failure\")) , \"1\", spec['hf_text'])\n",
    "##output\n",
    "print('text counts:')\n",
    "print(Counter(spec['hf_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "289aa278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5065</th>\n",
       "      <td>a machine-learning-based method to predict adverse events in patients with dilated cardiomyopathy and severely reduced ejection fractions patients with dilated cardiomyopathy dcm and severely reduced left ventricular ejection fractions lvefs are at very high risks of experiencing adverse cardiac events a machine learning ml method could enable more effective risk stratification for these high-risk patients by incorporating various types of data the aim of this study was to build an ml model to predict adverse events including all-cause deaths and heart transplantation in dcm patients with severely impaired lv systolic function</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15425</th>\n",
       "      <td>deep-learning models for the echocardiographic assessment of diastolic dysfunction the authors explored a deep neural network deepnn model that integrates multidimensional echocardiographic data to identify distinct patient subgroups with heart failure with preserved ejection fraction hfpef</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80680</th>\n",
       "      <td>direct delineation of myocardial infarction without contrast agents using a joint motion feature learning architecture changes in mechanical properties of myocardium caused by a infarction can lead to kinematic abnormalities this phenomenon has inspired us to develop this work for delineation of myocardial infarction area directly from non-contrast agents cardiac mr imaging sequences the main contribution of this work is to develop a new joint motion feature learning architecture to efficiently establish direct correspondences between motion features and tissue properties this architecture consists of three seamless connected function layers: the heart localization layers can automatically crop the region of interest roi sequences involving the left ventricle from the cardiac mr imaging sequences; the motion feature extraction layers, using long short-term memory-recurrent neural networks, a builds patch-based motion features through local intensity changes between fixed-size patch sequences cropped from image sequences, and b uses optical flow techniques to build image-based features through global intensity changes between adjacent images to describe the motion of each pixel; the fully connected discriminative layers can combine two types of motion features together in each pixel and then build the correspondences between motion features and tissue identities that is, infarct or not in each pixel we validated the performance of our framework in 165 cine cardiac mr imaging datasets by comparing to the ground truths manually segmented from delayed gadolinium-enhanced mr cardiac images by two radiologists with more than 10 years of experience our experimental results show that our proposed method has a high and stable accuracy pixel-level: 9503% and consistency kappa statistic: 091; dice: 8987%; rmse: 072  mm; hausdorff distance: 591  mm compared to manual delineation results overall, the advantage of our framework is that it can determine the tissue identity in each pixel from its motion pattern captured by normal cine cardiac mr images, which makes it an attractive tool for the clinical diagnosis of infarction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11969</th>\n",
       "      <td>deep learning-based automated left ventricular ejection fraction assessment using 2-d echocardiography deep learning dl has been applied for automatic left ventricle lv ejection fraction ef measurement, but the diagnostic performance was rarely evaluated for various phenotypes of heart disease this study aims to evaluate a new dl algorithm for automated lvef measurement using two-dimensional echocardiography 2de images collected from three centers the impact of three ultrasound machines and three phenotypes of heart diseases on the automatic lvef measurement was evaluated using 36890 frames of 2de from 340 patients, we developed a dl algorithm based on u-net dps-net and the biplane simpsons method was applied for lvef calculation results showed a high performance in lv segmentation and lvef measurement across phenotypes and echo systems by using dps-net good performance was obtained for lv segmentation when dps-net was tested on the camus data set dice coefficient of 0932 and 0928 for ed and es better performance of lv segmentation in study-wise evaluation was observed by comparing the dps-net v2 to the echonet-dynamic algorithm &lt;i&gt;p&lt;/i&gt; = 0008 dps-net was associated with high correlations and good agreements for the lvef measurement high diagnostic performance was obtained that the area under receiver operator characteristic curve was 0974, 0948, 0968, and 0972 for normal hearts and disease phenotypes including atrial fibrillation, hypertrophic cardiomyopathy, dilated cardiomyopathy, respectively high performance was obtained by using dps-net in lv detection and lvef measurement for heart failure with several phenotypes high performance was observed in a large-scale dataset, suggesting that the dps-net was highly adaptive across different echocardiographic systems&lt;b&gt;new &amp; noteworthy&lt;/b&gt; a new strategy of feature extraction and fusion could enhance the accuracy of automatic lvef assessment based on multiview 2-d echocardiographic sequences high diagnostic performance for the determination of heart failure was obtained by using dps-net in cases with different phenotypes of heart diseases high performance for left ventricle segmentation was obtained by using dps-net, suggesting the potential for a wider range of application in the interpretation of 2de images</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78478</th>\n",
       "      <td>deep learning analysis of left ventricular myocardium in ct angiographic intermediate-degree coronary stenosis improves the diagnostic accuracy for identification of functionally significant stenosis to evaluate the added value of deep learning dl analysis of the left ventricular myocardium lvm in resting coronary ct angiography ccta over determination of coronary degree of stenosis ds, for identification of patients with functionally significant coronary artery stenosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126156</th>\n",
       "      <td>machine learning techniques for arterial pressure waveform analysis the arterial pressure waveform apw can provide essential information about arterial wall integrity and arterial stiffness most of apw analysis frameworks individually process each hemodynamic parameter and do not evaluate inter-dependencies in the overall pulse morphology the key contribution of this work is the use of machine learning algorithms to deal with vectorized features extracted from apw with this purpose, we follow a five-step evaluation methodology: 1 a custom-designed, non-invasive, electromechanical device was used in the data collection from 50 subjects; 2 the acquired position and amplitude of onset, systolic peak sp, point of inflection pi and dicrotic wave dw were used for the computation of some morphological attributes; 3 pre-processing work on the datasets was performed in order to reduce the number of input features and increase the model accuracy by selecting the most relevant ones; 4 classification of the dataset was carried out using four different machine learning algorithms: random forest, bayesnet probabilistic, j48 decision tree and ripper rule-based induction; and 5 we evaluate the trained models, using the majority-voting system, comparatively to the respective calculated augmentation index aix classification algorithms have been proved to be efficient, in particular random forest has shown good accuracy 9695% and high area under the curve auc of a receiver operating characteristic roc curve 0961 finally, during validation tests, a correlation between high risk labels, retrieved from the multi-parametric approach, and positive aix values was verified this approach gives allowance for designing new hemodynamic morphology vectors and techniques for multiple apw analysis, thus improving the arterial pulse understanding, especially when compared to traditional single-parameter analysis, where the failure in one parameter measurement component, such as pi, can jeopardize the whole evaluation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37851</th>\n",
       "      <td>a comparison of artificial intelligence-based algorithms for the identification of patients with depressed right ventricular function from 2-dimentional echocardiography parameters and clinical features recognizing low right ventricular rv function from 2-dimentiontial echocardiography 2d-echo is challenging when parameters are contradictory we aim to develop a model to predict low rv function integrating the various 2d-echo parameters in reference to cardiac magnetic resonance cmr-the gold standard</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138429</th>\n",
       "      <td>a neuro-fuzzy decision support system for the diagnosis of heart failure a neuro-fuzzy decision support system is proposed for the diagnosis of heart failure the system comprises; knowledge base database, neural networks and fuzzy logic of both the quantitative and qualitative knowledge of the diagnosis of heart failure, neuro-fuzzy inference engine and decision support engine the neural networks employ a multi-layers perception back propagation learning process while the fuzzy logic uses the root sum square inference procedure the neuro-fuzzy inference engine uses a weighted average of the premise and consequent parameters with the fuzzy rules serving as the nodes and the fuzzy sets representing the weights of the nodes the decision support engine carries out the cognitive and emotional filtering of the objective and subjective feelings of the medical practitioner an experimental study of the decision support system was carried out using cases of some patients from three hospitals in nigeria with the assistance of their medical personnel who collected patientsdata over a period of six months the results of the study show that the neuro-fuzzy system provides a highly reliable diagnosis, while the emotional and cognitive filters further refine the diagnosis results by taking care of the contextual elements of medical diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86419</th>\n",
       "      <td>machine learning methods improve prognostication, identify clinically distinct phenotypes, and detect heterogeneity in response to therapy in a large cohort of heart failure patients whereas heart failure hf is a complex clinical syndrome, conventional approaches to its management have treated it as a singular disease, leading to inadequate patient care and inefficient clinical trials we hypothesized that applying advanced analytics to a large cohort of hf patients would improve prognostication of outcomes, identify distinct patient phenotypes, and detect heterogeneity in treatment response</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9626</th>\n",
       "      <td>prediction of 1-year mortality after heart transplantation using machine learning approaches: a single-center study from china heart transplantation htx remains the gold-standard treatment for end-stage heart failure the aim of this study was to establish a risk-prediction model for assessing prognosis of htx using machine-learning approach</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29322</th>\n",
       "      <td>predicting 90 day acute heart failure readmission and death using machine learning-supported decision analysis readmission or death soon after heart failure hf admission is a significant problem traditional analyses for predicting such events often fail to consider the gamut of characteristics that may contribute- tending to focus on 30-day outcomes even though the window of increased vulnerability may last up to 90 days risk assessments incorporating machine learning ml methods may be better suited than traditional statistical analyses alone to sort through multitude of data in the electronic health record ehr and identify patients at higher risk hypothesis: ml-based decision analysis may better identify patients at increased risk for 90-day acute hf readmission or death after incident hf admission methods and results: among 3189 patients who underwent index hf hospitalization, 152% experienced primary or acute hf readmission and 115% died within 90 days for risk assessment models, 98 variables were considered across nine data categories ml techniques were used to help select variables for a final logistic regression lr model the final models auc was 0760 95% ci 0752 to 0767, with sensitivity of 83% this proved superior to an lr model alone auc 0744 95% ci 0732 to 0755 eighteen variables were identified as risk factors including dilated inferior vena cava, elevated blood pressure, elevated bun, reduced albumin, abnormal sodium or bicarbonate, and nt pro-bnp elevation a risk prediction ml-based model developed from comprehensive characteristics within the ehr can efficiently identify patients at elevated risk of 90-day acute hf readmission or death for whom closer follow-up or further interventions may be considered</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12302</th>\n",
       "      <td>a novel algorithm for the computation of the diastolic pressure ratio in the invasive assessment of the functional significance of coronary artery disease invasive functional assessment is a mainstay in the management of patients with coronary artery disease cad, but there is uncertainty on the comparative accuracy of diagnostic indices of functional significance we aimed to validate the diagnostic performance of a novel non-hyperemic diastolic pressure ratio dpr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64045</th>\n",
       "      <td>comparison of machine learning techniques for prediction of hospitalization in heart failure patients the present study aims to compare the performance of eight machine learning techniques mlts in the prediction of hospitalization among patients with heart failure, using data from the gestione integrata dello scompenso cardiaco gisc study the gisc project is an ongoing study that takes place in the region of puglia, southern italy patients with a diagnosis of heart failure are enrolled in a long-term assistance program that includes the adoption of an online platform for data sharing between general practitioners and cardiologists working in hospitals and community health districts logistic regression, generalized linear model net glmn, classification and regression tree, random forest, adaboost, logitboost, support vector machine, and neural networks were applied to evaluate the feasibility of such techniques in predicting hospitalization of 380 patients enrolled in the gisc study, using data about demographic characteristics, medical history, and clinical characteristics of each patient the mlts were compared both without and with missing data imputation overall, models trained without missing data imputation showed higher predictive performances the glmn showed better performance in predicting hospitalization than the other mlts, with an average accuracy, positive predictive value and negative predictive value of 812%, 875%, and 75%, respectively present findings suggest that mlts may represent a promising opportunity to predict hospital admission of heart failure patients by exploiting health care information generated by the contact of such patients with the health care system</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40659</th>\n",
       "      <td>machine learning assessment of left ventricular diastolic function based on electrocardiographic features left ventricular lv diastolic dysfunction is recognized as playing a major role in the pathophysiology of heart failure; however, clinical tools for identifying diastolic dysfunction before echocardiography remain imprecise</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29458</th>\n",
       "      <td>use of machine learning to develop a risk-stratification tool for emergency department patients with acute heart failure we use variables from a recently derived acute heart failure risk-stratification rule stratify as a basis to develop and optimize risk prediction using additional patient clinical data from electronic health records and machine-learning models</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29353</th>\n",
       "      <td>formulating multi diseases dataset for identifying, triaging and prioritizing patients to multi medical emergency levels: simulated dataset accompanied with codes this paper provides simulated datasets for triaging and prioritizing patients that are essentially required to support multi emergency levels to this end, four types of input signals are presented, namely, electrocardiogram ecg, blood pressure, and oxygen saturation spo2, where the latter is text to obtain the aforementioned signals, the physionet online library 1, is used, which is considered as one of the most reliable and relevant libraries in the healthcare services and bioinformatics sciences in particular, this library contains collections of several databases and signals, where some of these signals are related to ecg, blood pressure, and spo2 sensor the simulated datasets, which are accompanied by codes, are presented in this paper the contributions of our work, which are related to the presented dataset, can be summarized as follow 1 the presented dataset is considered as an essential feature that is extracted from the signal records specifically, the dataset includes medical vital features such as: qrs width; st elevation; peaks number; cycle interval from ecg signal; spo2 level from spo2 signal; high blood systolic pressure value; and low-pressure diastolic value from blood pressure signal these essential features have been extracted based on our machine learning algorithms in addition, new medical features are added based on medical doctorsrecommendations, which are given as text-inputs, eg, chest pain, shortness of breath, palpitation, and whether the patient at rest or not all these features are considered to be significant symptoms for many diseases such as: heart attack or stroke; sleep apnea; heart failure; arrhythmia; and blood pressure chronic diseases 2 the formulated dataset is considered in the doctor diagnostic procedures for identifying the patientsemergency level 3 in the physionet online library 1, the ecg, blood pressure, and spo2 have been represented as signals in contrast, we use some signal processing techniques to re-present the dataset by numeric values, which enable us to extract the essential features of the dataset in excel sheet representations 4 the dataset is re-organized and re-formatted to be presented in a useful structure feasible format specifically, the dataset is re-presented in terms of tables to illustrate the patients profile and the type of diseases 5 the presented dataset is utilized in the evaluation of medical monitoring and healthcare provisioning systems 2 6 some simulated codes for feature extractions are also provided in this paper</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33095</th>\n",
       "      <td>predicting the development of adverse cardiac events in patients with hypertrophic cardiomyopathy using machine learning only a subset of patients with hypertrophic cardiomyopathy hcm develop adverse cardiac events - eg, end-stage heart failure, cardiovascular death current risk stratification methods are imperfect, limiting identification of high-risk patients with hcm our aim was to improve the prediction of adverse cardiac events in patients with hcm using machine learning methods</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79080</th>\n",
       "      <td>novel urinary biomarkers for improved prediction of progressive egfr loss in early chronic kidney disease stages and in high risk individuals without chronic kidney disease chronic kidney disease is associated with increased risk of ckd progression and death therapeutic approaches to limit progression are limited developing tools for the early identification of those individuals most likely to progress will allow enriching clinical trials in high risk early ckd patients the ckd273 classifier is a panel of 273 urinary peptides that enables early detection of ckd and prognosis of progression we have generated urine capillary electrophoresis-mass spectrometry-based peptidomics ckd273 subclassifiers specific for ckd stages to allow the early identification of patients at high risk of ckd progression in the validation cohort, the ckd273 subclassifiers outperformed albuminuria and ckd273 classifier for predicting rapid loss of egfr in individuals with baseline egfr &gt; 60 ml/min/173 m&lt;sup&gt;2&lt;/sup&gt; in individuals with egfr &gt; 60 ml/min/173 m&lt;sup&gt;2&lt;/sup&gt; and albuminuria &lt;30 mg/day, the ckd273 subclassifiers predicted rapid egfr loss with auc ranging from 0797 0743-0844 to 0736 0689-0780 the association between ckd273 subclassifiers and rapid progression remained significant after adjustment for age, sex, albuminuria, dm, baseline egfr, and systolic blood pressure urinary peptidomics ckd273 subclassifiers outperformed albuminuria and ckd273 classifier for predicting the risk of rapid ckd progression in individuals with egfr &gt; 60 ml/min/173 m&lt;sup&gt;2&lt;/sup&gt; these ckd273 subclassifiers represented the earliest evidence of rapidly progressive ckd in non-albuminuric individuals with preserved renal function</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>machine learning augmented echocardiography for diastolic function assessment cardiac diastolic dysfunction is prevalent and is a diagnostic criterion for heart failure with preserved ejection fraction-a burgeoning global health issue as gold-standard invasive haemodynamic assessment of diastolic function is not routinely performed, clinical guidelines advise using echocardiography measures to determine the grade of diastolic function however, the current process has suboptimal accuracy, regular indeterminate classifications and is susceptible to confounding from comorbidities advances in artificial intelligence in recent years have created revolutionary ways to evaluate and integrate large quantities of cardiology data imaging is an area of particular strength for the sub-field of machine-learning, with evidence that trained algorithms can accurately discern cardiac structures, reliably estimate chamber volumes, and output systolic function metrics from echocardiographic images in this review, we present the emerging field of machine-learning based echocardiographic diastolic function assessment we summarise how machine-learning has made use of diastolic parameters to accurately differentiate pathology, to identify novel phenotypes within diastolic disease, and to grade diastolic function perspectives are given about how these innovations could be used to augment clinical practice, whilst areas for future investigation are identified</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137971</th>\n",
       "      <td>automatic detection of end-diastole and end-systole from echocardiography images using manifold learning the automatic detection of end-diastole and end-systole frames of echocardiography images is the first step for calculation of the ejection fraction, stroke volume and some other features related to heart motion abnormalities in this paper, the manifold learning algorithm is applied on 2d echocardiography images to find out the relationship between the frames of one cycle of heart motion by this approach the nonlinear embedded information in sequential images is represented in a two-dimensional manifold by the lle algorithm and each image is depicted by a point on reconstructed manifold there are three dense regions on the manifold which correspond to the three phases of cardiac cycle isovolumetric contraction, isovolumetric relaxation, reduced filling, wherein there is no prominent change in ventricular volume by the fact that the end-systolic and end-diastolic frames are in isovolumic phases of the cardiac cycle, the dense regions can be used to find these frames by calculating the distance between consecutive points in the manifold, the isovolumic frames are mapped on the three minimums of the distance diagrams which were used to select the corresponding images the minimum correlation between these images leads to detection of end-systole and end-diastole frames the results on six healthy volunteers have been validated by an experienced echo cardiologist and depict the usefulness of the presented method</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "5065                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 a machine-learning-based method to predict adverse events in patients with dilated cardiomyopathy and severely reduced ejection fractions patients with dilated cardiomyopathy dcm and severely reduced left ventricular ejection fractions lvefs are at very high risks of experiencing adverse cardiac events a machine learning ml method could enable more effective risk stratification for these high-risk patients by incorporating various types of data the aim of this study was to build an ml model to predict adverse events including all-cause deaths and heart transplantation in dcm patients with severely impaired lv systolic function   \n",
       "15425                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       deep-learning models for the echocardiographic assessment of diastolic dysfunction the authors explored a deep neural network deepnn model that integrates multidimensional echocardiographic data to identify distinct patient subgroups with heart failure with preserved ejection fraction hfpef   \n",
       "80680                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      direct delineation of myocardial infarction without contrast agents using a joint motion feature learning architecture changes in mechanical properties of myocardium caused by a infarction can lead to kinematic abnormalities this phenomenon has inspired us to develop this work for delineation of myocardial infarction area directly from non-contrast agents cardiac mr imaging sequences the main contribution of this work is to develop a new joint motion feature learning architecture to efficiently establish direct correspondences between motion features and tissue properties this architecture consists of three seamless connected function layers: the heart localization layers can automatically crop the region of interest roi sequences involving the left ventricle from the cardiac mr imaging sequences; the motion feature extraction layers, using long short-term memory-recurrent neural networks, a builds patch-based motion features through local intensity changes between fixed-size patch sequences cropped from image sequences, and b uses optical flow techniques to build image-based features through global intensity changes between adjacent images to describe the motion of each pixel; the fully connected discriminative layers can combine two types of motion features together in each pixel and then build the correspondences between motion features and tissue identities that is, infarct or not in each pixel we validated the performance of our framework in 165 cine cardiac mr imaging datasets by comparing to the ground truths manually segmented from delayed gadolinium-enhanced mr cardiac images by two radiologists with more than 10 years of experience our experimental results show that our proposed method has a high and stable accuracy pixel-level: 9503% and consistency kappa statistic: 091; dice: 8987%; rmse: 072  mm; hausdorff distance: 591  mm compared to manual delineation results overall, the advantage of our framework is that it can determine the tissue identity in each pixel from its motion pattern captured by normal cine cardiac mr images, which makes it an attractive tool for the clinical diagnosis of infarction   \n",
       "11969                                                                                                                                                                                                                                                                                                                                                                                                                 deep learning-based automated left ventricular ejection fraction assessment using 2-d echocardiography deep learning dl has been applied for automatic left ventricle lv ejection fraction ef measurement, but the diagnostic performance was rarely evaluated for various phenotypes of heart disease this study aims to evaluate a new dl algorithm for automated lvef measurement using two-dimensional echocardiography 2de images collected from three centers the impact of three ultrasound machines and three phenotypes of heart diseases on the automatic lvef measurement was evaluated using 36890 frames of 2de from 340 patients, we developed a dl algorithm based on u-net dps-net and the biplane simpsons method was applied for lvef calculation results showed a high performance in lv segmentation and lvef measurement across phenotypes and echo systems by using dps-net good performance was obtained for lv segmentation when dps-net was tested on the camus data set dice coefficient of 0932 and 0928 for ed and es better performance of lv segmentation in study-wise evaluation was observed by comparing the dps-net v2 to the echonet-dynamic algorithm <i>p</i> = 0008 dps-net was associated with high correlations and good agreements for the lvef measurement high diagnostic performance was obtained that the area under receiver operator characteristic curve was 0974, 0948, 0968, and 0972 for normal hearts and disease phenotypes including atrial fibrillation, hypertrophic cardiomyopathy, dilated cardiomyopathy, respectively high performance was obtained by using dps-net in lv detection and lvef measurement for heart failure with several phenotypes high performance was observed in a large-scale dataset, suggesting that the dps-net was highly adaptive across different echocardiographic systems<b>new & noteworthy</b> a new strategy of feature extraction and fusion could enhance the accuracy of automatic lvef assessment based on multiview 2-d echocardiographic sequences high diagnostic performance for the determination of heart failure was obtained by using dps-net in cases with different phenotypes of heart diseases high performance for left ventricle segmentation was obtained by using dps-net, suggesting the potential for a wider range of application in the interpretation of 2de images   \n",
       "78478                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               deep learning analysis of left ventricular myocardium in ct angiographic intermediate-degree coronary stenosis improves the diagnostic accuracy for identification of functionally significant stenosis to evaluate the added value of deep learning dl analysis of the left ventricular myocardium lvm in resting coronary ct angiography ccta over determination of coronary degree of stenosis ds, for identification of patients with functionally significant coronary artery stenosis   \n",
       "126156                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      machine learning techniques for arterial pressure waveform analysis the arterial pressure waveform apw can provide essential information about arterial wall integrity and arterial stiffness most of apw analysis frameworks individually process each hemodynamic parameter and do not evaluate inter-dependencies in the overall pulse morphology the key contribution of this work is the use of machine learning algorithms to deal with vectorized features extracted from apw with this purpose, we follow a five-step evaluation methodology: 1 a custom-designed, non-invasive, electromechanical device was used in the data collection from 50 subjects; 2 the acquired position and amplitude of onset, systolic peak sp, point of inflection pi and dicrotic wave dw were used for the computation of some morphological attributes; 3 pre-processing work on the datasets was performed in order to reduce the number of input features and increase the model accuracy by selecting the most relevant ones; 4 classification of the dataset was carried out using four different machine learning algorithms: random forest, bayesnet probabilistic, j48 decision tree and ripper rule-based induction; and 5 we evaluate the trained models, using the majority-voting system, comparatively to the respective calculated augmentation index aix classification algorithms have been proved to be efficient, in particular random forest has shown good accuracy 9695% and high area under the curve auc of a receiver operating characteristic roc curve 0961 finally, during validation tests, a correlation between high risk labels, retrieved from the multi-parametric approach, and positive aix values was verified this approach gives allowance for designing new hemodynamic morphology vectors and techniques for multiple apw analysis, thus improving the arterial pulse understanding, especially when compared to traditional single-parameter analysis, where the failure in one parameter measurement component, such as pi, can jeopardize the whole evaluation    \n",
       "37851                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  a comparison of artificial intelligence-based algorithms for the identification of patients with depressed right ventricular function from 2-dimentional echocardiography parameters and clinical features recognizing low right ventricular rv function from 2-dimentiontial echocardiography 2d-echo is challenging when parameters are contradictory we aim to develop a model to predict low rv function integrating the various 2d-echo parameters in reference to cardiac magnetic resonance cmr-the gold standard   \n",
       "138429                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      a neuro-fuzzy decision support system for the diagnosis of heart failure a neuro-fuzzy decision support system is proposed for the diagnosis of heart failure the system comprises; knowledge base database, neural networks and fuzzy logic of both the quantitative and qualitative knowledge of the diagnosis of heart failure, neuro-fuzzy inference engine and decision support engine the neural networks employ a multi-layers perception back propagation learning process while the fuzzy logic uses the root sum square inference procedure the neuro-fuzzy inference engine uses a weighted average of the premise and consequent parameters with the fuzzy rules serving as the nodes and the fuzzy sets representing the weights of the nodes the decision support engine carries out the cognitive and emotional filtering of the objective and subjective feelings of the medical practitioner an experimental study of the decision support system was carried out using cases of some patients from three hospitals in nigeria with the assistance of their medical personnel who collected patientsdata over a period of six months the results of the study show that the neuro-fuzzy system provides a highly reliable diagnosis, while the emotional and cognitive filters further refine the diagnosis results by taking care of the contextual elements of medical diagnosis   \n",
       "86419                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     machine learning methods improve prognostication, identify clinically distinct phenotypes, and detect heterogeneity in response to therapy in a large cohort of heart failure patients whereas heart failure hf is a complex clinical syndrome, conventional approaches to its management have treated it as a singular disease, leading to inadequate patient care and inefficient clinical trials we hypothesized that applying advanced analytics to a large cohort of hf patients would improve prognostication of outcomes, identify distinct patient phenotypes, and detect heterogeneity in treatment response   \n",
       "9626                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     prediction of 1-year mortality after heart transplantation using machine learning approaches: a single-center study from china heart transplantation htx remains the gold-standard treatment for end-stage heart failure the aim of this study was to establish a risk-prediction model for assessing prognosis of htx using machine-learning approach   \n",
       "29322                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         predicting 90 day acute heart failure readmission and death using machine learning-supported decision analysis readmission or death soon after heart failure hf admission is a significant problem traditional analyses for predicting such events often fail to consider the gamut of characteristics that may contribute- tending to focus on 30-day outcomes even though the window of increased vulnerability may last up to 90 days risk assessments incorporating machine learning ml methods may be better suited than traditional statistical analyses alone to sort through multitude of data in the electronic health record ehr and identify patients at higher risk hypothesis: ml-based decision analysis may better identify patients at increased risk for 90-day acute hf readmission or death after incident hf admission methods and results: among 3189 patients who underwent index hf hospitalization, 152% experienced primary or acute hf readmission and 115% died within 90 days for risk assessment models, 98 variables were considered across nine data categories ml techniques were used to help select variables for a final logistic regression lr model the final models auc was 0760 95% ci 0752 to 0767, with sensitivity of 83% this proved superior to an lr model alone auc 0744 95% ci 0732 to 0755 eighteen variables were identified as risk factors including dilated inferior vena cava, elevated blood pressure, elevated bun, reduced albumin, abnormal sodium or bicarbonate, and nt pro-bnp elevation a risk prediction ml-based model developed from comprehensive characteristics within the ehr can efficiently identify patients at elevated risk of 90-day acute hf readmission or death for whom closer follow-up or further interventions may be considered   \n",
       "12302                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       a novel algorithm for the computation of the diastolic pressure ratio in the invasive assessment of the functional significance of coronary artery disease invasive functional assessment is a mainstay in the management of patients with coronary artery disease cad, but there is uncertainty on the comparative accuracy of diagnostic indices of functional significance we aimed to validate the diagnostic performance of a novel non-hyperemic diastolic pressure ratio dpr   \n",
       "64045                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            comparison of machine learning techniques for prediction of hospitalization in heart failure patients the present study aims to compare the performance of eight machine learning techniques mlts in the prediction of hospitalization among patients with heart failure, using data from the gestione integrata dello scompenso cardiaco gisc study the gisc project is an ongoing study that takes place in the region of puglia, southern italy patients with a diagnosis of heart failure are enrolled in a long-term assistance program that includes the adoption of an online platform for data sharing between general practitioners and cardiologists working in hospitals and community health districts logistic regression, generalized linear model net glmn, classification and regression tree, random forest, adaboost, logitboost, support vector machine, and neural networks were applied to evaluate the feasibility of such techniques in predicting hospitalization of 380 patients enrolled in the gisc study, using data about demographic characteristics, medical history, and clinical characteristics of each patient the mlts were compared both without and with missing data imputation overall, models trained without missing data imputation showed higher predictive performances the glmn showed better performance in predicting hospitalization than the other mlts, with an average accuracy, positive predictive value and negative predictive value of 812%, 875%, and 75%, respectively present findings suggest that mlts may represent a promising opportunity to predict hospital admission of heart failure patients by exploiting health care information generated by the contact of such patients with the health care system   \n",
       "40659                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 machine learning assessment of left ventricular diastolic function based on electrocardiographic features left ventricular lv diastolic dysfunction is recognized as playing a major role in the pathophysiology of heart failure; however, clinical tools for identifying diastolic dysfunction before echocardiography remain imprecise   \n",
       "29458                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              use of machine learning to develop a risk-stratification tool for emergency department patients with acute heart failure we use variables from a recently derived acute heart failure risk-stratification rule stratify as a basis to develop and optimize risk prediction using additional patient clinical data from electronic health records and machine-learning models   \n",
       "29353   formulating multi diseases dataset for identifying, triaging and prioritizing patients to multi medical emergency levels: simulated dataset accompanied with codes this paper provides simulated datasets for triaging and prioritizing patients that are essentially required to support multi emergency levels to this end, four types of input signals are presented, namely, electrocardiogram ecg, blood pressure, and oxygen saturation spo2, where the latter is text to obtain the aforementioned signals, the physionet online library 1, is used, which is considered as one of the most reliable and relevant libraries in the healthcare services and bioinformatics sciences in particular, this library contains collections of several databases and signals, where some of these signals are related to ecg, blood pressure, and spo2 sensor the simulated datasets, which are accompanied by codes, are presented in this paper the contributions of our work, which are related to the presented dataset, can be summarized as follow 1 the presented dataset is considered as an essential feature that is extracted from the signal records specifically, the dataset includes medical vital features such as: qrs width; st elevation; peaks number; cycle interval from ecg signal; spo2 level from spo2 signal; high blood systolic pressure value; and low-pressure diastolic value from blood pressure signal these essential features have been extracted based on our machine learning algorithms in addition, new medical features are added based on medical doctorsrecommendations, which are given as text-inputs, eg, chest pain, shortness of breath, palpitation, and whether the patient at rest or not all these features are considered to be significant symptoms for many diseases such as: heart attack or stroke; sleep apnea; heart failure; arrhythmia; and blood pressure chronic diseases 2 the formulated dataset is considered in the doctor diagnostic procedures for identifying the patientsemergency level 3 in the physionet online library 1, the ecg, blood pressure, and spo2 have been represented as signals in contrast, we use some signal processing techniques to re-present the dataset by numeric values, which enable us to extract the essential features of the dataset in excel sheet representations 4 the dataset is re-organized and re-formatted to be presented in a useful structure feasible format specifically, the dataset is re-presented in terms of tables to illustrate the patients profile and the type of diseases 5 the presented dataset is utilized in the evaluation of medical monitoring and healthcare provisioning systems 2 6 some simulated codes for feature extractions are also provided in this paper   \n",
       "33095                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  predicting the development of adverse cardiac events in patients with hypertrophic cardiomyopathy using machine learning only a subset of patients with hypertrophic cardiomyopathy hcm develop adverse cardiac events - eg, end-stage heart failure, cardiovascular death current risk stratification methods are imperfect, limiting identification of high-risk patients with hcm our aim was to improve the prediction of adverse cardiac events in patients with hcm using machine learning methods   \n",
       "79080                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      novel urinary biomarkers for improved prediction of progressive egfr loss in early chronic kidney disease stages and in high risk individuals without chronic kidney disease chronic kidney disease is associated with increased risk of ckd progression and death therapeutic approaches to limit progression are limited developing tools for the early identification of those individuals most likely to progress will allow enriching clinical trials in high risk early ckd patients the ckd273 classifier is a panel of 273 urinary peptides that enables early detection of ckd and prognosis of progression we have generated urine capillary electrophoresis-mass spectrometry-based peptidomics ckd273 subclassifiers specific for ckd stages to allow the early identification of patients at high risk of ckd progression in the validation cohort, the ckd273 subclassifiers outperformed albuminuria and ckd273 classifier for predicting rapid loss of egfr in individuals with baseline egfr > 60 ml/min/173 m<sup>2</sup> in individuals with egfr > 60 ml/min/173 m<sup>2</sup> and albuminuria <30 mg/day, the ckd273 subclassifiers predicted rapid egfr loss with auc ranging from 0797 0743-0844 to 0736 0689-0780 the association between ckd273 subclassifiers and rapid progression remained significant after adjustment for age, sex, albuminuria, dm, baseline egfr, and systolic blood pressure urinary peptidomics ckd273 subclassifiers outperformed albuminuria and ckd273 classifier for predicting the risk of rapid ckd progression in individuals with egfr > 60 ml/min/173 m<sup>2</sup> these ckd273 subclassifiers represented the earliest evidence of rapidly progressive ckd in non-albuminuric individuals with preserved renal function   \n",
       "6153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         machine learning augmented echocardiography for diastolic function assessment cardiac diastolic dysfunction is prevalent and is a diagnostic criterion for heart failure with preserved ejection fraction-a burgeoning global health issue as gold-standard invasive haemodynamic assessment of diastolic function is not routinely performed, clinical guidelines advise using echocardiography measures to determine the grade of diastolic function however, the current process has suboptimal accuracy, regular indeterminate classifications and is susceptible to confounding from comorbidities advances in artificial intelligence in recent years have created revolutionary ways to evaluate and integrate large quantities of cardiology data imaging is an area of particular strength for the sub-field of machine-learning, with evidence that trained algorithms can accurately discern cardiac structures, reliably estimate chamber volumes, and output systolic function metrics from echocardiographic images in this review, we present the emerging field of machine-learning based echocardiographic diastolic function assessment we summarise how machine-learning has made use of diastolic parameters to accurately differentiate pathology, to identify novel phenotypes within diastolic disease, and to grade diastolic function perspectives are given about how these innovations could be used to augment clinical practice, whilst areas for future investigation are identified   \n",
       "137971                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           automatic detection of end-diastole and end-systole from echocardiography images using manifold learning the automatic detection of end-diastole and end-systole frames of echocardiography images is the first step for calculation of the ejection fraction, stroke volume and some other features related to heart motion abnormalities in this paper, the manifold learning algorithm is applied on 2d echocardiography images to find out the relationship between the frames of one cycle of heart motion by this approach the nonlinear embedded information in sequential images is represented in a two-dimensional manifold by the lle algorithm and each image is depicted by a point on reconstructed manifold there are three dense regions on the manifold which correspond to the three phases of cardiac cycle isovolumetric contraction, isovolumetric relaxation, reduced filling, wherein there is no prominent change in ventricular volume by the fact that the end-systolic and end-diastolic frames are in isovolumic phases of the cardiac cycle, the dense regions can be used to find these frames by calculating the distance between consecutive points in the manifold, the isovolumic frames are mapped on the three minimums of the distance diagrams which were used to select the corresponding images the minimum correlation between these images leads to detection of end-systole and end-diastole frames the results on six healthy volunteers have been validated by an experienced echo cardiologist and depict the usefulness of the presented method   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "5065          0       0       0           0          0        0       0   \n",
       "15425         0       0       0           0          0        0       0   \n",
       "80680         0       0       0           0          0        0       0   \n",
       "11969         0       0       0           0          0        0       0   \n",
       "78478         0       0       0           0          0        0       0   \n",
       "126156        0       0       0           0          0        0       0   \n",
       "37851         0       0       0           0          0        0       0   \n",
       "138429        0       0       0           0          0        0       0   \n",
       "86419         0       0       0           0          0        0       0   \n",
       "9626          0       0       0           0          0        0       0   \n",
       "29322         0       0       0           0          0        0       0   \n",
       "12302         0       0       0           0          0        0       0   \n",
       "64045         0       0       0           0          0        0       0   \n",
       "40659         0       0       0           0          0        0       0   \n",
       "29458         0       1       0           0          0        0       0   \n",
       "29353         0       0       0           0          0        0       0   \n",
       "33095         0       0       0           0          0        0       0   \n",
       "79080         0       0       0           0          0        0       0   \n",
       "6153          0       0       0           0          0        0       0   \n",
       "137971        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "5065             0            0         0           0        0       0   \n",
       "15425            0            0         0           0        0       0   \n",
       "80680            0            0         0           0        0       0   \n",
       "11969            0            0         0           0        0       0   \n",
       "78478            0            0         0           0        0       0   \n",
       "126156           0            0         0           0        0       0   \n",
       "37851            0            0         0           0        0       0   \n",
       "138429           0            0         0           0        0       0   \n",
       "86419            0            0         0           0        0       0   \n",
       "9626             0            0         0           0        0       0   \n",
       "29322            0            0         0           0        0       0   \n",
       "12302            0            0         0           0        0       0   \n",
       "64045            0            0         0           0        0       0   \n",
       "40659            0            0         0           0        0       0   \n",
       "29458            0            0         0           0        0       0   \n",
       "29353            0            0         0           0        0       0   \n",
       "33095            0            0         0           0        0       0   \n",
       "79080            0            0         0           0        0       0   \n",
       "6153             0            0         0           0        0       0   \n",
       "137971           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "5065             0             0           0            0         0   \n",
       "15425            0             0           0            0         0   \n",
       "80680            0             0           0            0         0   \n",
       "11969            0             0           0            0         0   \n",
       "78478            0             0           0            0         0   \n",
       "126156           0             0           0            0         0   \n",
       "37851            0             0           0            0         0   \n",
       "138429           0             0           0            0         0   \n",
       "86419            0             0           0            0         0   \n",
       "9626             0             0           0            0         0   \n",
       "29322            0             0           0            0         0   \n",
       "12302            0             0           0            0         0   \n",
       "64045            0             0           0            0         0   \n",
       "40659            0             0           0            0         0   \n",
       "29458            0             0           0            0         0   \n",
       "29353            0             0           0            0         0   \n",
       "33095            0             0           0            0         0   \n",
       "79080            0             0           0            0         0   \n",
       "6153             0             0           0            0         0   \n",
       "137971           0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "5065            0           0            0           0            0   \n",
       "15425           0           0            0           0            0   \n",
       "80680           0           0            0           0            0   \n",
       "11969           0           0            0           0            0   \n",
       "78478           0           0            0           0            0   \n",
       "126156          0           0            0           0            0   \n",
       "37851           0           0            0           0            0   \n",
       "138429          0           0            0           0            0   \n",
       "86419           0           0            0           0            0   \n",
       "9626            0           0            0           0            0   \n",
       "29322           0           0            0           0            0   \n",
       "12302           0           0            0           0            0   \n",
       "64045           0           0            0           0            0   \n",
       "40659           0           0            0           0            0   \n",
       "29458           0           0            0           0            0   \n",
       "29353           0           0            0           0            0   \n",
       "33095           0           0            0           0            0   \n",
       "79080           0           0            0           0            0   \n",
       "6153            0           0            0           0            0   \n",
       "137971          0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "5065            0            0        0         0          0       0        0   \n",
       "15425           0            0        0         0          0       0        0   \n",
       "80680           0            0        0         0          0       0        0   \n",
       "11969           0            0        0         0          0       0        0   \n",
       "78478           0            0        0         0          0       0        0   \n",
       "126156          0            0        0         0          0       0        0   \n",
       "37851           0            0        0         0          0       0        0   \n",
       "138429          0            0        0         0          0       0        0   \n",
       "86419           0            0        0         0          0       0        0   \n",
       "9626            0            0        0         0          0       0        0   \n",
       "29322           0            0        0         0          0       0        0   \n",
       "12302           0            0        0         0          0       0        0   \n",
       "64045           0            0        0         0          0       0        0   \n",
       "40659           0            0        0         0          0       0        0   \n",
       "29458           0            0        0         0          0       0        0   \n",
       "29353           0            0        0         0          0       0        0   \n",
       "33095           0            0        0         0          0       0        0   \n",
       "79080           0            0        0         0          0       0        0   \n",
       "6153            0            0        0         0          0       0        0   \n",
       "137971          0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "5065           0          0        0       0         0          0        0   \n",
       "15425          0          0        0       0         0          0        0   \n",
       "80680          0          0        0       0         0          0        0   \n",
       "11969          0          0        0       0         0          0        0   \n",
       "78478          0          0        0       0         0          0        0   \n",
       "126156         0          0        0       0         0          0        0   \n",
       "37851          0          0        0       0         0          0        0   \n",
       "138429         0          0        0       0         0          1        0   \n",
       "86419          0          0        0       0         0          0        0   \n",
       "9626           0          0        0       0         0          0        0   \n",
       "29322          0          0        0       0         0          0        0   \n",
       "12302          0          0        0       0         0          0        0   \n",
       "64045          0          0        0       0         0          0        0   \n",
       "40659          0          0        0       0         0          0        0   \n",
       "29458          0          0        0       0         0          0        0   \n",
       "29353          0          0        1       0         0          0        0   \n",
       "33095          0          0        0       0         0          0        0   \n",
       "79080          0          0        0       0         0          0        0   \n",
       "6153           0          0        0       0         0          0        0   \n",
       "137971         0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text  \n",
       "5065             0         0        1        0       1  \n",
       "15425            0         0        1        0       1  \n",
       "80680            0         0        1        1       1  \n",
       "11969            0         0        1        0       1  \n",
       "78478            0         0        1        1       1  \n",
       "126156           0         0        0        0       1  \n",
       "37851            0         0        1        0       1  \n",
       "138429           0         0        1        0       1  \n",
       "86419            0         0        1        0       1  \n",
       "9626             0         0        1        0       1  \n",
       "29322            0         0        1        0       1  \n",
       "12302            0         0        1        1       1  \n",
       "64045            0         0        1        0       1  \n",
       "40659            0         0        1        0       1  \n",
       "29458            0         0        1        0       1  \n",
       "29353            0         0        1        1       1  \n",
       "33095            0         0        1        0       1  \n",
       "79080            0         0        0        0       1  \n",
       "6153             0         0        1        0       1  \n",
       "137971           0         0        1        0       1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['hf_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e7466e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33397, '1': 782})\n"
     ]
    }
   ],
   "source": [
    "#### ARRHYTHMIA / arrhyt\n",
    "\n",
    "## text\n",
    "text = ['sinus node', 'sinoatrial', 'atrial tachy', 'atrial flutter', 'accessory pathway', 'long qt', 'holter',\n",
    "        'pacemaker', 'ventricular tachy', 'atrial fibrill', 'ventricular fibrill', 'supraventricular tachy',\n",
    "        'cardiover', 'defibrillat', 'heart block', 'degree block', 'av block', 'ventricular block', ' p-wave', ' p wave', 'pr interval',\n",
    "       'p-r interval', 'pr-interval', 'corrected qt', ' qtc ', ' qrs complex ', 'brugada', 'short qt', 'qt syndrome', 'long qt']\n",
    "\n",
    "spec['arrhyt_text'] = np.where(groups['text'].str.contains('arrhythmi'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['arrhyt_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['arrhyt_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"ablation\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"ablation\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"bradycardia\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"bradycardia\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"electrophys\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"electrophys\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"heart\")) &\n",
    "                             (groups['text'].str.contains(\"rhythm\")) , \"1\", spec['arrhyt_text'])\n",
    "spec['arrhyt_text'] = np.where((groups['text'].str.contains(\"cardiac\")) &\n",
    "                             (groups['text'].str.contains(\"rhythm\")) , \"1\", spec['arrhyt_text'])\n",
    "\n",
    "\n",
    "## outputs\n",
    "print('text counts:')\n",
    "print(Counter(spec['arrhyt_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "800af8f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38123</th>\n",
       "      <td>classification of normal sinus rhythm, abnormal arrhythmia and congestive heart failure ecg signals using lstm and hybrid cnn-svm deep neural networks effective monitoring of heart patients according to heart signals can save a huge amount of life in the last decade, the classification and prediction of heart diseases according to ecg signals has gained great importance for patients and doctors in this paper, the deep learning architecture with high accuracy and popularity has been proposed in recent years for the classification of normal sinus rhythm, nsr abnormal arrhythmia arr and congestive heart failure chf ecg signals the proposed architecture is based on hybrid alexnet-svm support vector machine 96 arrhythmia, 30 chf, 36 nsr signals are available in a total of 192 ecg signals in order to demonstrate the classification performance of deep learning architectures, arr, chr and nsr signals are firstly classified by svm, knn algorithm, achieving 6875% and 6563% accuracy the signals are then classified in their raw form with lstm long short time memory with 9067% accuracy by obtaining the spectrograms of the signals, hybrid alexnet-svm algorithm is applied to the images and 9677% accuracy is obtained the results show that with the proposed deep learning architecture, it classifies ecg signals with higher accuracy than conventional machine learning classifiers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>application of a time-series deep learning model to predict cardiac dysrhythmias in electronic health records cardiac dysrhythmias cd affect millions of americans in the united states us, and are associated with considerable morbidity and mortality new strategies to combat this growing problem are urgently needed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17870</th>\n",
       "      <td>detecting digoxin toxicity by artificial intelligence-assisted electrocardiography although digoxin is important in heart rate control, the utilization of digoxin is declining due to its narrow therapeutic window misdiagnosis or delayed diagnosis of digoxin toxicity is common due to the lack of awareness and the time-consuming laboratory work that is involved electrocardiography ecg may be able to detect potential digoxin toxicity based on characteristic presentations our study attempted to develop a deep learning model to detect digoxin toxicity based on ecg manifestations this study included 61 ecgs from patients with digoxin toxicity and 177,066 ecgs from patients in the emergency room from november 2011 to february 2019 the deep learning algorithm was trained using approximately 80% of ecgs the other 20% of ecgs were used to validate the performance of the artificial intelligence ai system and to conduct a human-machine competition area under the receiver operating characteristic curve auc, sensitivity, and specificity were used to evaluate the performance of ecg interpretation between humans and our deep learning system the aucs of our deep learning system for identifying digoxin toxicity were 0912 and 0929 in the validation cohort and the human-machine competition, respectively, which reached 846% of sensitivity and 946% of specificity interestingly, the deep learning system using only lead i auc = 0960 was not worse than using complete 12 leads 0912 stratified analysis showed that our deep learning system was more applicable to patients with heart failure hf and without atrial fibrillation af than those without hf and with af our ecg-based deep learning system provides a high-accuracy, economical, rapid, and accessible way to detect digoxin toxicity, which can be applied as a promising decision supportive system for diagnosing digoxin toxicity in clinical practice</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29065</th>\n",
       "      <td>an iot and fog computing-based monitoring system for cardiovascular patients with automatic ecg classification using deep neural networks telemedicine and all types of monitoring systems have proven to be a useful and low-cost tool with a high level of applicability in cardiology the objective of this work is to present an iot-based monitoring system for cardiovascular patients the system sends the ecg signal to a fog layer service by using the lora communication protocol also, it includes an ai algorithm based on deep learning for the detection of atrial fibrillation and other heart rhythms the automatic detection of arrhythmias can be complementary to the diagnosis made by the physician, achieving a better clinical vision that improves therapeutic decision making the performance of the proposed system is evaluated on a dataset of 8528 short single-lead ecg records using two merge mobilenet networks that classify data with an accuracy of 90% for atrial fibrillation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144156</th>\n",
       "      <td>finding features for real-time premature ventricular contraction detection using a fuzzy neural network system fuzzy neural networks fnns have been successfully applied to generate predictive rules for medical or diagnostic data this brief presents an approach to detect premature ventricular contractions pvcs using the neural network with weighted fuzzy membership functions newfms the newfm classifies normal and pvc beats by the trained bounded sum of weighted fuzzy membership functions bswfms using wavelet transformed coefficients from the mit-bih pvc database the eight generalized coefficients, locally related to the time signal, are extracted by the nonoverlap area distribution measurement method the eight generalized coefficients are used for the three pvc data sets with reliable accuracy rates of 9980%, 9921%, and 9878%, respectively, which means that the selected features are less dependent on the data sets it is shown that the locations of the eight features are not only around the qrs complex that represents ventricular depolarization in the electrocardiogram ecg containing a q wave, an r wave, and an s wave, but also the qr segment from the q wave to the r wave has more discriminate information than the rs segment from the r wave to the s wave the bswfms of the eight features trained by newfm are shown visually, which makes the features explicitly interpretable since each bswfm combines multiple weighted fuzzy membership functions into one using the bounded sum, the eight small-sized bswfms can realize real-time pvc detection in a mobile environment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127910</th>\n",
       "      <td>dynamic time warping and machine learning for signal quality assessment of pulsatile signals in this work, we describe a beat-by-beat method for assessing the clinical utility of pulsatile waveforms, primarily recorded from cardiovascular blood volume or pressure changes, concentrating on the photoplethysmogram ppg physiological blood flow is nonstationary, with pulses changing in height, width and morphology due to changes in heart rate, cardiac output, sensor type and hardware or software pre-processing requirements moreover, considerable inter-individual and sensor-location variability exists simple template matching methods are therefore inappropriate, and a patient-specific adaptive initialization is therefore required we introduce dynamic time warping to stretch each beat to match a running template and combine it with several other features related to signal quality, including correlation and the percentage of the beat that appeared to be clipped the features were then presented to a multi-layer perceptron neural network to learn the relationships between the parameters in the presence of good- and bad-quality pulses an expert-labeled database of 1055 segments of ppg, each 6 s long, recorded from 104 separate critical care admissions during both normal and verified arrhythmic events, was used to train and test our algorithms an accuracy of 975% on the training set and 952% on test set was found the algorithm could be deployed as a stand-alone signal quality assessment algorithm for vetting the clinical utility of ppg traces or any similar quasi-periodic signal</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104702</th>\n",
       "      <td>medical decision support system for diagnosis of heart arrhythmia using dwt and random forests classifier in this study, random forests rf classifier is proposed for ecg heartbeat signal classification in diagnosis of heart arrhythmia discrete wavelet transform dwt is used to decompose ecg signals into different successive frequency bands a set of different statistical features were extracted from the obtained frequency bands to denote the distribution of wavelet coefficients this study shows that rf classifier achieves superior performances compared to other decision tree methods using 10-fold cross-validation for the ecg datasets and the obtained results suggest that further significant improvements in terms of classification accuracy can be accomplished by the proposed classification system accurate ecg signal classification is the major requirement for detection of all arrhythmia types performances of the proposed system have been evaluated on two different databases, namely mit-bih database and st -petersburg institute of cardiological technics 12-lead arrhythmia database for mit-bih database, rf classifier yielded an overall accuracy 9933 % against 9844 and 9867 % for the c45 and cart classifiers, respectively for st -petersburg institute of cardiological technics 12-lead arrhythmia database, rf classifier yielded an overall accuracy 9995 % against 9980 % for both c45 and cart classifiers, respectively the combined model with multiscale principal component analysis mspca de-noising, discrete wavelet transform dwt and rf classifier also achieves better performance with the area under the receiver operating characteristic roc curve auc and f-measure equal to 0999 and 0993 for mit-bih database and 1 and 0999 for and st -petersburg institute of cardiological technics 12-lead arrhythmia database, respectively obtained results demonstrate that the proposed system has capacity for reliable classification of ecg signals, and to assist the clinicians for making an accurate diagnosis of cardiovascular disorders cvds</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12029</th>\n",
       "      <td>premature beats detection based on a novel convolutional neural network &lt;i&gt;objective&lt;/i&gt;automatic detection of premature beats on long electrocardiogram ecg recordings is of great significance for clinical diagnosis in this paper, we propose a novel deep learning model, the ecgdet, to detect premature beats, including premature ventricular contractions pvcs and supraventricular premature beats spbs on single-lead long-term ecgs&lt;i&gt;approach&lt;/i&gt;the ecgdet is proposed based on a convolutional neural network and squeeze-and-excitation network it outputs the probabilities that the ecg samples belong to a premature contraction non-max suppression was used to select the most appropriate locations for the premature beats the ecgdet was trained and tested on the mit-bih arrhythmia database mitdb using a five-fold cross-validation approach a novel loss calculation method was introduced in the model training process then it was tuned and further tested on the china physiological signal challenge 2020 database cpscdb&lt;i&gt;main results&lt;/i&gt;the results showed that the average f1 value of pvc detection was 926%, while that of spb detection was 722% on mitdb the ecgdet bagged the 2nd place for pvc detection and ranked 7th place of spb detection in the china physiological signal challenge 2020&lt;i&gt;significance&lt;/i&gt;the proposed ecgdet can automatically detect premature heartbeats without manually extracting the features this technique can be used for long-term ecg signal analysis and has potential for clinical applications</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>deep learning methods for screening patientss-icd implantation eligibility subcutaneous implantable cardioverter-defibrillators s-icds are used for prevention of sudden cardiac death triggered by ventricular arrhythmias t wave over sensing twos is an inherent risk with s-icds which can lead to inappropriate shocks a major predictor of twos is a high t:r ratio the ratio between the amplitudes of the t and r waves currently, patientselectrocardiograms ecgs are screened over 10 s to measure the t:r ratio to determine the patientseligibility for s-icd implantation due to temporal variations in the t:r ratio, 10 s is not a long enough window to reliably determine the normal values of a patients t:r ratio in this paper, we develop a convolutional neural network cnn based model utilising phase space reconstruction matrices to predict t:r ratios from 10-second ecg segments without explicitly locating the r or t waves, thus avoiding the issue of twos this tool can be used to automatically screen patients over a much longer period and provide an in-depth description of the behavior of the t:r ratio over that period the tool can also enable much more reliable and descriptive screenings to better assess patientseligibility for s-icd implantation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172090</th>\n",
       "      <td>ann-based qrs-complex analysis of ecg reliable detection the qrs complex in either a normal or an abnormal ecg and its analysis is the first and foremost task in almost every ecg signal analysis system aimed at the diagnostic interpretation of ecg conventionally, detection of the qrs complex is accomplished using a rule-based/algorithmic approach this work, uses the learn and generalize approach of an artificial neural network ann for the detection of qrs complexes in either a normal or an abnormal ecg this is followed by the analysis of the qrs complex to designate and measure the morphological components within the qrs complex in all 12 standard leads an ann has been developed to detect the qrs complex in ecg and trained, with the help of back propagation algorithm, on more than a hundred ecgs selected from the cse data set-3 the trained ann was tested on all the recordings of the cse data set-3 and the sensitivity has been found to be 9911% subsequent to the identification of the qrs complex, an analysis of this complex and measurement of peak amplitudes of the component waves is done the results are validated using the cse multilead measurement results both the qrs detection and the qrs analysis software developed in c-language have been successfully implemented on a pc-at the results are found to be in agreement with visual measurements carried out by medical experts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122856</th>\n",
       "      <td>an expert system based on fisher score and ls-svm for cardiac arrhythmia diagnosis an expert system having two stages is proposed for cardiac arrhythmia diagnosis in the first stage, fisher score is used for feature selection to reduce the feature space dimension of a data set the second stage is classification stage in which least squares support vector machines classifier is performed by using the feature subset selected in the first stage to diagnose cardiac arrhythmia performance of the proposed expert system is evaluated by using an arrhythmia data set which is taken from uci machine learning repository</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21547</th>\n",
       "      <td>heartnetec: a deep representation learning approach for ecg beat classification one of the most crucial and informative tools available at the disposal of a cardiologist for examining the condition of a patients cardiovascular system is the electrocardiogram ecg/ekg a major reason behind the need for accurate reconstruction of ecg comes from the fact that the shape of ecg tracing is very crucial for determining the health condition of an individual whether the patient is prone to or diagnosed with cardiovascular diseases cvds, this information can be gathered through examination of ecg signal among various other methods, one of the most helpful methods in identifying cardiac abnormalities is a beat-wise categorization of a patients ecg record in this work, a highly efficient &lt;i&gt;deep representation learning&lt;/i&gt; approach for ecg beat classification is proposed, which can significantly reduce the burden and time spent by a cardiologist for ecg analysis this work consists of two sub-systems: denoising block and beat classification block the initial block is a denoising block that acquires the ecg signal from the patient and denoises that the next stage is the beat classification part this processes the input ecg signal for finding out the different classes of beats in the ecg through an efficient algorithm in both stages, deep learning-based methods have been employed for the purpose our proposed approach has been tested on physionets mit-bih arrhythmia database, for beat-wise classification into ten important types of heartbeats as per the results obtained, the proposed approach is capable of making meaningful predictions and gives superior results on relevant metrics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107369</th>\n",
       "      <td>personalized and automated remote monitoring of atrial fibrillation remote monitoring of cardiac implantable electronic devices is a growing standard; yet, remote follow-up and management of alerts represents a time-consuming task for physicians or trained staff this study evaluates an automatic mechanism based on artificial intelligence tools to filter atrial fibrillation af alerts based on their medical significance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27932</th>\n",
       "      <td>explainable artificial intelligence for heart rate variability in ecg signal electrocardiogram ecg signal is one of the most reliable methods to analyse the cardiovascular system in the literature, there are different deep learning architectures proposed to detect various types of tachycardia diseases, such as atrial fibrillation, ventricular fibrillation, and sinus tachycardia even though all types of tachycardia diseases have fast beat rhythm as the common characteristic feature, existing deep learning architectures are trained with the corresponding disease-specific features most of the proposed works lack the interpretation and understanding of the results obtained hence, the objective of this letter is to explore the features learned by the deep learning models for the detection of the different types of tachycardia diseases, the authors used a transfer learning approach in this method, the model is trained with one of the tachycardia diseases called atrial fibrillation and tested with other tachycardia diseases, such as ventricular fibrillation and sinus tachycardia the analysis was done using different deep learning models, such as rnn, lstm, gru, cnn, and rscnn rnn achieved an accuracy of 9647% for atrial fibrillation data set, 9088% accuracy for cu-ventricular tachycardia data set, and also achieved an accuracy of 9471, and 9418% for mit-bih malignant ventricular ectopy database for ecg lead i and lead ii, respectively the rnn model could only achieve an accuracy of 2373% for the sinus tachycardia data set a similar trend is shown by other models from the analysis, it was evident that even though tachycardia diseases have fast beat rhythm as their common feature, the model was not able to detect different types of tachycardia diseases the deep learning model could only detect atrial fibrillation and ventricular fibrillation and failed in the case of sinus tachycardia from the analysis, they were able to interpret that, along with the fast beat rhythm, the model has learned the absence of p-wave which is a common feature for ventricular fibrillation and atrial fibrillation but sinus tachycardia disease has an upright positive p-wave the time-based analysis is conducted to find the time complexity of the models the analysis conveyed that rnn and rscnn models could achieve better performance with lesser time complexity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109113</th>\n",
       "      <td>ecg prediction based on classification via neural networks and linguistic fuzzy logic forecaster the paper deals with ecg prediction based on neural networks classification of different types of time courses of ecg signals the main objective is to recognise normal cycles and arrhythmias and perform further diagnosis we proposed two detection systems that have been created with usage of neural networks the experimental part makes it possible to load ecg signals, preprocess them, and classify them into given classes outputs from the classifiers carry a predictive character all experimental results from both of the proposed classifiers are mutually compared in the conclusion we also experimented with the new method of time series transparent prediction based on fuzzy transform with linguistic if-then rules preliminary results show interesting results based on the unique capability of this approach bringing natural language interpretation of particular prediction, that is, the properties of time series</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49407</th>\n",
       "      <td>atrial fibrillation detection from raw photoplethysmography waveforms: a deep learning application atrial fibrillation af, a common cause of stroke, often is asymptomatic smartphones and smartwatches can detect af using heart rate patterns inferred using photoplethysmography ppg; however, enhanced accuracy is required to reduce false positives in screening populations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84428</th>\n",
       "      <td>a novel application of deep learning for single-lead ecg classification detecting and classifying cardiac arrhythmias is critical to the diagnosis of patients with cardiac abnormalities in this paper, a novel approach based on deep learning methodology is proposed for the classification of single-lead electrocardiogram ecg signals we demonstrate the application of the restricted boltzmann machine rbm and deep belief networks dbn for ecg classification following detection of ventricular and supraventricular heartbeats using single-lead ecg the effectiveness of this proposed algorithm is illustrated using real ecg signals from the widely-used mit-bih database simulation results demonstrate that with a suitable choice of parameters, rbm and dbn can achieve high average recognition accuracies of ventricular ectopic beats 9363% and of supraventricular ectopic beats 9557% at a low sampling rate of 114 hz experimental results indicate that classifiers built into this deep learning-based framework achieved state-of-the art performance models at lower sampling rates and simple features when compared to traditional methods further, employing features extracted at a sampling rate of 114 hz when combined with deep learning provided enough discriminatory power for the classification task this performance is comparable to that of traditional methods and uses a much lower sampling rate and simpler features thus, our proposed deep neural network algorithm demonstrates that deep learning-based methods offer accurate ecg classification and could potentially be extended to other physiological signal classifications, such as those in arterial blood pressure abp, nerve conduction emg, and heart rate variability hrv studies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87732</th>\n",
       "      <td>a deep learning approach for fetal qrs complex detection non-invasive foetal electrocardiography ni-fecg has the potential to provide more additional clinical information for detecting and diagnosing fetal diseases we propose and demonstrate a deep learning approach for fetal qrs complex detection from raw ni-fecg signals by using a convolutional neural network cnn model the main objective is to investigate whether reliable fetal qrs complex detection performance can still be obtained from features of single-channel ni-fecg signals, without canceling maternal ecg mecg signals</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77584</th>\n",
       "      <td>support vector machine-based assessment of the t-wave morphology improves long qt syndrome diagnosis diagnosing long qt syndrome lqts is challenging due to a considerable overlap of the qtc-interval between lqts patients and healthy controls the aim of this study was to investigate the added value of t-wave morphology markers obtained from 12-lead electrocardiograms ecgs in diagnosing lqts in a large cohort of gene-positive lqts patients and gene-negative family members using a support vector machine</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131138</th>\n",
       "      <td>a vectorcardiogram-based classification system for the detection of myocardial infarction myocardial infarction mi, generally known as a heart attack, is one of the top leading causes of mortality in the world in clinical diagnosis, cardiologists generally utilize 12-lead ecg system to classify patients into mi symptoms: 1 st segment elevation, 2 st segment depression or t wave inversion however unstable ischemic syndromes have rapidly changing supply versus demand characteristics that is one of the several limitations of 12-lead ecg system for mi detection in addition, the ecg sensor placements of 12-lead system is not easily donned and doffed for tele-healthcare monitoring at home vectorcardiogram vcg system in clinic is another type of diagnosis plot which represents the magnitude and direction of the electrical potential in the form of a vector loop during cardiac electric activity the vcg system can easily acquire three ecg waves from x, y, z directions to composite vector signal in space and the vcg signals can be transferred to 12-lead ecg signal through dower transformation and vice versa hence, this study attempts to develop a vcg-based classification system for the detection of myocardial infarction in the experiment results, the proposed system can select the proper ecg features based on cardiologists knowledge and proposed principal moments of qrs complex the classification performance of mi detection can be reached to 9989% of sensitivity, 9251% of specificity, 9535% of positive predictive value, and 9696% overall accuracy with maximum-likelihood classifier mlc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "38123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           classification of normal sinus rhythm, abnormal arrhythmia and congestive heart failure ecg signals using lstm and hybrid cnn-svm deep neural networks effective monitoring of heart patients according to heart signals can save a huge amount of life in the last decade, the classification and prediction of heart diseases according to ecg signals has gained great importance for patients and doctors in this paper, the deep learning architecture with high accuracy and popularity has been proposed in recent years for the classification of normal sinus rhythm, nsr abnormal arrhythmia arr and congestive heart failure chf ecg signals the proposed architecture is based on hybrid alexnet-svm support vector machine 96 arrhythmia, 30 chf, 36 nsr signals are available in a total of 192 ecg signals in order to demonstrate the classification performance of deep learning architectures, arr, chr and nsr signals are firstly classified by svm, knn algorithm, achieving 6875% and 6563% accuracy the signals are then classified in their raw form with lstm long short time memory with 9067% accuracy by obtaining the spectrograms of the signals, hybrid alexnet-svm algorithm is applied to the images and 9677% accuracy is obtained the results show that with the proposed deep learning architecture, it classifies ecg signals with higher accuracy than conventional machine learning classifiers   \n",
       "3865                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        application of a time-series deep learning model to predict cardiac dysrhythmias in electronic health records cardiac dysrhythmias cd affect millions of americans in the united states us, and are associated with considerable morbidity and mortality new strategies to combat this growing problem are urgently needed   \n",
       "17870                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  detecting digoxin toxicity by artificial intelligence-assisted electrocardiography although digoxin is important in heart rate control, the utilization of digoxin is declining due to its narrow therapeutic window misdiagnosis or delayed diagnosis of digoxin toxicity is common due to the lack of awareness and the time-consuming laboratory work that is involved electrocardiography ecg may be able to detect potential digoxin toxicity based on characteristic presentations our study attempted to develop a deep learning model to detect digoxin toxicity based on ecg manifestations this study included 61 ecgs from patients with digoxin toxicity and 177,066 ecgs from patients in the emergency room from november 2011 to february 2019 the deep learning algorithm was trained using approximately 80% of ecgs the other 20% of ecgs were used to validate the performance of the artificial intelligence ai system and to conduct a human-machine competition area under the receiver operating characteristic curve auc, sensitivity, and specificity were used to evaluate the performance of ecg interpretation between humans and our deep learning system the aucs of our deep learning system for identifying digoxin toxicity were 0912 and 0929 in the validation cohort and the human-machine competition, respectively, which reached 846% of sensitivity and 946% of specificity interestingly, the deep learning system using only lead i auc = 0960 was not worse than using complete 12 leads 0912 stratified analysis showed that our deep learning system was more applicable to patients with heart failure hf and without atrial fibrillation af than those without hf and with af our ecg-based deep learning system provides a high-accuracy, economical, rapid, and accessible way to detect digoxin toxicity, which can be applied as a promising decision supportive system for diagnosing digoxin toxicity in clinical practice   \n",
       "29065                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             an iot and fog computing-based monitoring system for cardiovascular patients with automatic ecg classification using deep neural networks telemedicine and all types of monitoring systems have proven to be a useful and low-cost tool with a high level of applicability in cardiology the objective of this work is to present an iot-based monitoring system for cardiovascular patients the system sends the ecg signal to a fog layer service by using the lora communication protocol also, it includes an ai algorithm based on deep learning for the detection of atrial fibrillation and other heart rhythms the automatic detection of arrhythmias can be complementary to the diagnosis made by the physician, achieving a better clinical vision that improves therapeutic decision making the performance of the proposed system is evaluated on a dataset of 8528 short single-lead ecg records using two merge mobilenet networks that classify data with an accuracy of 90% for atrial fibrillation   \n",
       "144156                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                finding features for real-time premature ventricular contraction detection using a fuzzy neural network system fuzzy neural networks fnns have been successfully applied to generate predictive rules for medical or diagnostic data this brief presents an approach to detect premature ventricular contractions pvcs using the neural network with weighted fuzzy membership functions newfms the newfm classifies normal and pvc beats by the trained bounded sum of weighted fuzzy membership functions bswfms using wavelet transformed coefficients from the mit-bih pvc database the eight generalized coefficients, locally related to the time signal, are extracted by the nonoverlap area distribution measurement method the eight generalized coefficients are used for the three pvc data sets with reliable accuracy rates of 9980%, 9921%, and 9878%, respectively, which means that the selected features are less dependent on the data sets it is shown that the locations of the eight features are not only around the qrs complex that represents ventricular depolarization in the electrocardiogram ecg containing a q wave, an r wave, and an s wave, but also the qr segment from the q wave to the r wave has more discriminate information than the rs segment from the r wave to the s wave the bswfms of the eight features trained by newfm are shown visually, which makes the features explicitly interpretable since each bswfm combines multiple weighted fuzzy membership functions into one using the bounded sum, the eight small-sized bswfms can realize real-time pvc detection in a mobile environment   \n",
       "127910                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       dynamic time warping and machine learning for signal quality assessment of pulsatile signals in this work, we describe a beat-by-beat method for assessing the clinical utility of pulsatile waveforms, primarily recorded from cardiovascular blood volume or pressure changes, concentrating on the photoplethysmogram ppg physiological blood flow is nonstationary, with pulses changing in height, width and morphology due to changes in heart rate, cardiac output, sensor type and hardware or software pre-processing requirements moreover, considerable inter-individual and sensor-location variability exists simple template matching methods are therefore inappropriate, and a patient-specific adaptive initialization is therefore required we introduce dynamic time warping to stretch each beat to match a running template and combine it with several other features related to signal quality, including correlation and the percentage of the beat that appeared to be clipped the features were then presented to a multi-layer perceptron neural network to learn the relationships between the parameters in the presence of good- and bad-quality pulses an expert-labeled database of 1055 segments of ppg, each 6 s long, recorded from 104 separate critical care admissions during both normal and verified arrhythmic events, was used to train and test our algorithms an accuracy of 975% on the training set and 952% on test set was found the algorithm could be deployed as a stand-alone signal quality assessment algorithm for vetting the clinical utility of ppg traces or any similar quasi-periodic signal   \n",
       "104702                                                                                                                                                                                                                                                                                                                                 medical decision support system for diagnosis of heart arrhythmia using dwt and random forests classifier in this study, random forests rf classifier is proposed for ecg heartbeat signal classification in diagnosis of heart arrhythmia discrete wavelet transform dwt is used to decompose ecg signals into different successive frequency bands a set of different statistical features were extracted from the obtained frequency bands to denote the distribution of wavelet coefficients this study shows that rf classifier achieves superior performances compared to other decision tree methods using 10-fold cross-validation for the ecg datasets and the obtained results suggest that further significant improvements in terms of classification accuracy can be accomplished by the proposed classification system accurate ecg signal classification is the major requirement for detection of all arrhythmia types performances of the proposed system have been evaluated on two different databases, namely mit-bih database and st -petersburg institute of cardiological technics 12-lead arrhythmia database for mit-bih database, rf classifier yielded an overall accuracy 9933 % against 9844 and 9867 % for the c45 and cart classifiers, respectively for st -petersburg institute of cardiological technics 12-lead arrhythmia database, rf classifier yielded an overall accuracy 9995 % against 9980 % for both c45 and cart classifiers, respectively the combined model with multiscale principal component analysis mspca de-noising, discrete wavelet transform dwt and rf classifier also achieves better performance with the area under the receiver operating characteristic roc curve auc and f-measure equal to 0999 and 0993 for mit-bih database and 1 and 0999 for and st -petersburg institute of cardiological technics 12-lead arrhythmia database, respectively obtained results demonstrate that the proposed system has capacity for reliable classification of ecg signals, and to assist the clinicians for making an accurate diagnosis of cardiovascular disorders cvds   \n",
       "12029                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               premature beats detection based on a novel convolutional neural network <i>objective</i>automatic detection of premature beats on long electrocardiogram ecg recordings is of great significance for clinical diagnosis in this paper, we propose a novel deep learning model, the ecgdet, to detect premature beats, including premature ventricular contractions pvcs and supraventricular premature beats spbs on single-lead long-term ecgs<i>approach</i>the ecgdet is proposed based on a convolutional neural network and squeeze-and-excitation network it outputs the probabilities that the ecg samples belong to a premature contraction non-max suppression was used to select the most appropriate locations for the premature beats the ecgdet was trained and tested on the mit-bih arrhythmia database mitdb using a five-fold cross-validation approach a novel loss calculation method was introduced in the model training process then it was tuned and further tested on the china physiological signal challenge 2020 database cpscdb<i>main results</i>the results showed that the average f1 value of pvc detection was 926%, while that of spb detection was 722% on mitdb the ecgdet bagged the 2nd place for pvc detection and ranked 7th place of spb detection in the china physiological signal challenge 2020<i>significance</i>the proposed ecgdet can automatically detect premature heartbeats without manually extracting the features this technique can be used for long-term ecg signal analysis and has potential for clinical applications   \n",
       "3533                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             deep learning methods for screening patientss-icd implantation eligibility subcutaneous implantable cardioverter-defibrillators s-icds are used for prevention of sudden cardiac death triggered by ventricular arrhythmias t wave over sensing twos is an inherent risk with s-icds which can lead to inappropriate shocks a major predictor of twos is a high t:r ratio the ratio between the amplitudes of the t and r waves currently, patientselectrocardiograms ecgs are screened over 10 s to measure the t:r ratio to determine the patientseligibility for s-icd implantation due to temporal variations in the t:r ratio, 10 s is not a long enough window to reliably determine the normal values of a patients t:r ratio in this paper, we develop a convolutional neural network cnn based model utilising phase space reconstruction matrices to predict t:r ratios from 10-second ecg segments without explicitly locating the r or t waves, thus avoiding the issue of twos this tool can be used to automatically screen patients over a much longer period and provide an in-depth description of the behavior of the t:r ratio over that period the tool can also enable much more reliable and descriptive screenings to better assess patientseligibility for s-icd implantation   \n",
       "172090                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ann-based qrs-complex analysis of ecg reliable detection the qrs complex in either a normal or an abnormal ecg and its analysis is the first and foremost task in almost every ecg signal analysis system aimed at the diagnostic interpretation of ecg conventionally, detection of the qrs complex is accomplished using a rule-based/algorithmic approach this work, uses the learn and generalize approach of an artificial neural network ann for the detection of qrs complexes in either a normal or an abnormal ecg this is followed by the analysis of the qrs complex to designate and measure the morphological components within the qrs complex in all 12 standard leads an ann has been developed to detect the qrs complex in ecg and trained, with the help of back propagation algorithm, on more than a hundred ecgs selected from the cse data set-3 the trained ann was tested on all the recordings of the cse data set-3 and the sensitivity has been found to be 9911% subsequent to the identification of the qrs complex, an analysis of this complex and measurement of peak amplitudes of the component waves is done the results are validated using the cse multilead measurement results both the qrs detection and the qrs analysis software developed in c-language have been successfully implemented on a pc-at the results are found to be in agreement with visual measurements carried out by medical experts   \n",
       "122856                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        an expert system based on fisher score and ls-svm for cardiac arrhythmia diagnosis an expert system having two stages is proposed for cardiac arrhythmia diagnosis in the first stage, fisher score is used for feature selection to reduce the feature space dimension of a data set the second stage is classification stage in which least squares support vector machines classifier is performed by using the feature subset selected in the first stage to diagnose cardiac arrhythmia performance of the proposed expert system is evaluated by using an arrhythmia data set which is taken from uci machine learning repository    \n",
       "21547                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    heartnetec: a deep representation learning approach for ecg beat classification one of the most crucial and informative tools available at the disposal of a cardiologist for examining the condition of a patients cardiovascular system is the electrocardiogram ecg/ekg a major reason behind the need for accurate reconstruction of ecg comes from the fact that the shape of ecg tracing is very crucial for determining the health condition of an individual whether the patient is prone to or diagnosed with cardiovascular diseases cvds, this information can be gathered through examination of ecg signal among various other methods, one of the most helpful methods in identifying cardiac abnormalities is a beat-wise categorization of a patients ecg record in this work, a highly efficient <i>deep representation learning</i> approach for ecg beat classification is proposed, which can significantly reduce the burden and time spent by a cardiologist for ecg analysis this work consists of two sub-systems: denoising block and beat classification block the initial block is a denoising block that acquires the ecg signal from the patient and denoises that the next stage is the beat classification part this processes the input ecg signal for finding out the different classes of beats in the ecg through an efficient algorithm in both stages, deep learning-based methods have been employed for the purpose our proposed approach has been tested on physionets mit-bih arrhythmia database, for beat-wise classification into ten important types of heartbeats as per the results obtained, the proposed approach is capable of making meaningful predictions and gives superior results on relevant metrics   \n",
       "107369                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           personalized and automated remote monitoring of atrial fibrillation remote monitoring of cardiac implantable electronic devices is a growing standard; yet, remote follow-up and management of alerts represents a time-consuming task for physicians or trained staff this study evaluates an automatic mechanism based on artificial intelligence tools to filter atrial fibrillation af alerts based on their medical significance   \n",
       "27932   explainable artificial intelligence for heart rate variability in ecg signal electrocardiogram ecg signal is one of the most reliable methods to analyse the cardiovascular system in the literature, there are different deep learning architectures proposed to detect various types of tachycardia diseases, such as atrial fibrillation, ventricular fibrillation, and sinus tachycardia even though all types of tachycardia diseases have fast beat rhythm as the common characteristic feature, existing deep learning architectures are trained with the corresponding disease-specific features most of the proposed works lack the interpretation and understanding of the results obtained hence, the objective of this letter is to explore the features learned by the deep learning models for the detection of the different types of tachycardia diseases, the authors used a transfer learning approach in this method, the model is trained with one of the tachycardia diseases called atrial fibrillation and tested with other tachycardia diseases, such as ventricular fibrillation and sinus tachycardia the analysis was done using different deep learning models, such as rnn, lstm, gru, cnn, and rscnn rnn achieved an accuracy of 9647% for atrial fibrillation data set, 9088% accuracy for cu-ventricular tachycardia data set, and also achieved an accuracy of 9471, and 9418% for mit-bih malignant ventricular ectopy database for ecg lead i and lead ii, respectively the rnn model could only achieve an accuracy of 2373% for the sinus tachycardia data set a similar trend is shown by other models from the analysis, it was evident that even though tachycardia diseases have fast beat rhythm as their common feature, the model was not able to detect different types of tachycardia diseases the deep learning model could only detect atrial fibrillation and ventricular fibrillation and failed in the case of sinus tachycardia from the analysis, they were able to interpret that, along with the fast beat rhythm, the model has learned the absence of p-wave which is a common feature for ventricular fibrillation and atrial fibrillation but sinus tachycardia disease has an upright positive p-wave the time-based analysis is conducted to find the time complexity of the models the analysis conveyed that rnn and rscnn models could achieve better performance with lesser time complexity   \n",
       "109113                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ecg prediction based on classification via neural networks and linguistic fuzzy logic forecaster the paper deals with ecg prediction based on neural networks classification of different types of time courses of ecg signals the main objective is to recognise normal cycles and arrhythmias and perform further diagnosis we proposed two detection systems that have been created with usage of neural networks the experimental part makes it possible to load ecg signals, preprocess them, and classify them into given classes outputs from the classifiers carry a predictive character all experimental results from both of the proposed classifiers are mutually compared in the conclusion we also experimented with the new method of time series transparent prediction based on fuzzy transform with linguistic if-then rules preliminary results show interesting results based on the unique capability of this approach bringing natural language interpretation of particular prediction, that is, the properties of time series    \n",
       "49407                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               atrial fibrillation detection from raw photoplethysmography waveforms: a deep learning application atrial fibrillation af, a common cause of stroke, often is asymptomatic smartphones and smartwatches can detect af using heart rate patterns inferred using photoplethysmography ppg; however, enhanced accuracy is required to reduce false positives in screening populations   \n",
       "84428                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              a novel application of deep learning for single-lead ecg classification detecting and classifying cardiac arrhythmias is critical to the diagnosis of patients with cardiac abnormalities in this paper, a novel approach based on deep learning methodology is proposed for the classification of single-lead electrocardiogram ecg signals we demonstrate the application of the restricted boltzmann machine rbm and deep belief networks dbn for ecg classification following detection of ventricular and supraventricular heartbeats using single-lead ecg the effectiveness of this proposed algorithm is illustrated using real ecg signals from the widely-used mit-bih database simulation results demonstrate that with a suitable choice of parameters, rbm and dbn can achieve high average recognition accuracies of ventricular ectopic beats 9363% and of supraventricular ectopic beats 9557% at a low sampling rate of 114 hz experimental results indicate that classifiers built into this deep learning-based framework achieved state-of-the art performance models at lower sampling rates and simple features when compared to traditional methods further, employing features extracted at a sampling rate of 114 hz when combined with deep learning provided enough discriminatory power for the classification task this performance is comparable to that of traditional methods and uses a much lower sampling rate and simpler features thus, our proposed deep neural network algorithm demonstrates that deep learning-based methods offer accurate ecg classification and could potentially be extended to other physiological signal classifications, such as those in arterial blood pressure abp, nerve conduction emg, and heart rate variability hrv studies   \n",
       "87732                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           a deep learning approach for fetal qrs complex detection non-invasive foetal electrocardiography ni-fecg has the potential to provide more additional clinical information for detecting and diagnosing fetal diseases we propose and demonstrate a deep learning approach for fetal qrs complex detection from raw ni-fecg signals by using a convolutional neural network cnn model the main objective is to investigate whether reliable fetal qrs complex detection performance can still be obtained from features of single-channel ni-fecg signals, without canceling maternal ecg mecg signals   \n",
       "77584                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        support vector machine-based assessment of the t-wave morphology improves long qt syndrome diagnosis diagnosing long qt syndrome lqts is challenging due to a considerable overlap of the qtc-interval between lqts patients and healthy controls the aim of this study was to investigate the added value of t-wave morphology markers obtained from 12-lead electrocardiograms ecgs in diagnosing lqts in a large cohort of gene-positive lqts patients and gene-negative family members using a support vector machine   \n",
       "131138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                a vectorcardiogram-based classification system for the detection of myocardial infarction myocardial infarction mi, generally known as a heart attack, is one of the top leading causes of mortality in the world in clinical diagnosis, cardiologists generally utilize 12-lead ecg system to classify patients into mi symptoms: 1 st segment elevation, 2 st segment depression or t wave inversion however unstable ischemic syndromes have rapidly changing supply versus demand characteristics that is one of the several limitations of 12-lead ecg system for mi detection in addition, the ecg sensor placements of 12-lead system is not easily donned and doffed for tele-healthcare monitoring at home vectorcardiogram vcg system in clinic is another type of diagnosis plot which represents the magnitude and direction of the electrical potential in the form of a vector loop during cardiac electric activity the vcg system can easily acquire three ecg waves from x, y, z directions to composite vector signal in space and the vcg signals can be transferred to 12-lead ecg signal through dower transformation and vice versa hence, this study attempts to develop a vcg-based classification system for the detection of myocardial infarction in the experiment results, the proposed system can select the proper ecg features based on cardiologists knowledge and proposed principal moments of qrs complex the classification performance of mi detection can be reached to 9989% of sensitivity, 9251% of specificity, 9535% of positive predictive value, and 9696% overall accuracy with maximum-likelihood classifier mlc   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "38123         0       0       0           0          0        0       0   \n",
       "3865          0       0       0           0          0        0       0   \n",
       "17870         0       1       0           0          0        0       0   \n",
       "29065         0       0       0           0          0        0       0   \n",
       "144156        0       0       0           0          0        0       0   \n",
       "127910        1       0       0           0          0        0       0   \n",
       "104702        0       0       0           0          0        0       0   \n",
       "12029         0       0       0           0          0        0       0   \n",
       "3533          0       0       0           0          0        0       0   \n",
       "172090        0       0       0           0          0        0       0   \n",
       "122856        0       0       0           0          0        0       0   \n",
       "21547         0       0       0           0          0        0       0   \n",
       "107369        0       0       0           0          0        0       0   \n",
       "27932         0       0       0           0          0        0       0   \n",
       "109113        0       0       0           0          0        0       0   \n",
       "49407         0       0       0           0          0        0       0   \n",
       "84428         0       0       0           0          0        0       0   \n",
       "87732         0       0       0           0          0        0       0   \n",
       "77584         0       0       0           0          0        0       0   \n",
       "131138        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "38123            0            0         0           0        0       0   \n",
       "3865             0            0         0           0        0       0   \n",
       "17870            0            0         0           0        0       0   \n",
       "29065            0            0         0           0        0       0   \n",
       "144156           0            0         0           0        0       0   \n",
       "127910           0            0         0           0        0       0   \n",
       "104702           0            0         0           0        0       0   \n",
       "12029            0            0         0           0        0       0   \n",
       "3533             0            0         1           0        0       0   \n",
       "172090           0            0         0           0        0       0   \n",
       "122856           0            0         0           0        0       0   \n",
       "21547            0            0         0           0        0       0   \n",
       "107369           0            0         0           0        0       0   \n",
       "27932            0            0         0           0        0       0   \n",
       "109113           0            0         0           0        0       0   \n",
       "49407            0            0         0           0        0       0   \n",
       "84428            0            0         0           0        0       0   \n",
       "87732            0            0         0           0        0       0   \n",
       "77584            0            0         0           0        0       0   \n",
       "131138           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "38123            0             0           0            0         0   \n",
       "3865             0             0           0            0         0   \n",
       "17870            0             0           0            0         0   \n",
       "29065            0             0           0            0         0   \n",
       "144156           0             0           0            0         0   \n",
       "127910           0             0           0            0         0   \n",
       "104702           0             0           0            0         0   \n",
       "12029            0             0           0            0         0   \n",
       "3533             0             0           0            0         0   \n",
       "172090           0             0           0            0         0   \n",
       "122856           0             0           0            0         0   \n",
       "21547            0             0           0            0         0   \n",
       "107369           0             0           0            0         0   \n",
       "27932            0             0           0            0         0   \n",
       "109113           0             0           0            0         0   \n",
       "49407            0             0           0            0         0   \n",
       "84428            0             0           0            0         0   \n",
       "87732            0             0           0            0         0   \n",
       "77584            0             0           0            0         0   \n",
       "131138           0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "38123           0           0            0           0            0   \n",
       "3865            0           0            0           0            0   \n",
       "17870           0           0            0           0            0   \n",
       "29065           0           0            0           0            0   \n",
       "144156          0           0            0           0            0   \n",
       "127910          0           0            0           0            0   \n",
       "104702          0           0            0           0            0   \n",
       "12029           0           0            0           0            0   \n",
       "3533            0           0            0           0            0   \n",
       "172090          0           0            0           0            0   \n",
       "122856          0           0            0           0            0   \n",
       "21547           0           0            0           0            0   \n",
       "107369          0           0            0           0            0   \n",
       "27932           0           0            0           0            0   \n",
       "109113          0           0            0           0            0   \n",
       "49407           0           0            0           0            0   \n",
       "84428           0           0            0           0            0   \n",
       "87732           0           0            0           0            0   \n",
       "77584           0           0            0           0            0   \n",
       "131138          0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "38123           0            0        0         0          0       0        0   \n",
       "3865            0            0        0         0          0       0        0   \n",
       "17870           0            0        0         0          0       0        0   \n",
       "29065           0            0        0         0          0       0        0   \n",
       "144156          0            0        0         0          0       0        0   \n",
       "127910          0            0        0         0          0       0        0   \n",
       "104702          0            0        0         0          0       0        0   \n",
       "12029           0            0        0         0          0       0        0   \n",
       "3533            0            0        0         0          0       0        0   \n",
       "172090          0            0        0         0          0       0        0   \n",
       "122856          0            0        0         0          0       0        0   \n",
       "21547           0            0        0         0          0       0        0   \n",
       "107369          0            0        0         0          0       0        0   \n",
       "27932           0            0        0         0          0       0        0   \n",
       "109113          0            0        0         0          0       0        0   \n",
       "49407           0            0        0         0          0       0        0   \n",
       "84428           0            0        0         0          0       0        0   \n",
       "87732           0            0        0         0          0       0        0   \n",
       "77584           0            0        0         0          0       0        0   \n",
       "131138          0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "38123          0          0        0       0         0          0        0   \n",
       "3865           0          0        0       0         0          0        0   \n",
       "17870          0          0        0       0         0          0        0   \n",
       "29065          0          0        0       0         0          0        0   \n",
       "144156         0          0        0       0         0          0        0   \n",
       "127910         0          0        0       0         0          0        0   \n",
       "104702         0          0        0       0         0          0        0   \n",
       "12029          0          0        0       0         0          0        0   \n",
       "3533           0          0        0       0         0          0        0   \n",
       "172090         0          0        0       0         0          0        0   \n",
       "122856         0          0        0       0         0          0        0   \n",
       "21547          0          0        0       0         0          0        0   \n",
       "107369         0          0        0       0         0          0        0   \n",
       "27932          0          0        0       0         0          0        0   \n",
       "109113         0          0        0       0         0          0        0   \n",
       "49407          0          0        0       0         0          0        0   \n",
       "84428          0          0        0       0         0          0        0   \n",
       "87732          0          0        0       0         0          0        0   \n",
       "77584          0          0        0       0         0          0        0   \n",
       "131138         0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text  \n",
       "38123            0         0        1        0       1           1  \n",
       "3865             0         0        1        0       0           1  \n",
       "17870            0         0        1        0       1           1  \n",
       "29065            0         0        1        0       0           1  \n",
       "144156           0         0        1        0       0           1  \n",
       "127910           0         0        1        0       0           1  \n",
       "104702           0         0        1        0       0           1  \n",
       "12029            0         0        1        0       0           1  \n",
       "3533             0         0        1        0       0           1  \n",
       "172090           0         0        1        0       0           1  \n",
       "122856           0         0        1        0       0           1  \n",
       "21547            0         0        1        0       0           1  \n",
       "107369           0         0        1        0       0           1  \n",
       "27932            0         0        1        0       0           1  \n",
       "109113           0         0        1        0       0           1  \n",
       "49407            0         0        1        0       0           1  \n",
       "84428            0         0        1        0       0           1  \n",
       "87732            0         0        1        0       0           1  \n",
       "77584            0         0        1        0       0           1  \n",
       "131138           0         0        1        1       0           1  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['arrhyt_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "41254e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33695, '1': 484})\n"
     ]
    }
   ],
   "source": [
    "## ENDOCRINE [C19] - no dm / endo\n",
    "\n",
    "## text\n",
    "text = ['acromegaly', 'adrenal', 'addisons', 'conns syn', 'cushings synd', 'cushings disease', 'thyroid', 'graves disease',\n",
    "       'hashimoto', 'polycystic ovary', 'prolactin', 'pituitar', 'androgen', 'testosterone', 'gonadism', 'gonadal']\n",
    "\n",
    "spec['endo_text'] = np.where(groups['text'].str.contains('endocrin'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['endo_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['endo_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## outputs\n",
    "print('text counts:')\n",
    "print(Counter(spec['endo_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "197c41c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32897, '1': 1282})\n"
     ]
    }
   ],
   "source": [
    "#### DIABETES - all / dm\n",
    "\n",
    "## text\n",
    "text = ['diabet', 'mellitus', 'hypoglycemia', 'hypoglycaemi', 'hyperglycemi', 'hyperglycaemi', 'insulin', 'glucagon',\n",
    "        'islet cell'\n",
    "       ]\n",
    "\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains('diabetes'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['dm_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['dm_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains(\"insipidus\"), \"0\", spec['dm_text'])\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains('growth factor'), \"0\", spec['dm_text'])\n",
    "spec['dm_text'] = np.where(groups['text'].str.contains(' igf'), \"1\", spec['dm_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['dm_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6ac8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34015, '1': 164})\n"
     ]
    }
   ],
   "source": [
    "#### DIABETES - insulin / insulin\n",
    "\n",
    "spec['insulin_text'] = np.where(groups['text'].str.contains('insulin'), \"1\", \"0\")\n",
    "\n",
    "\n",
    "spec['insulin_text'] = np.where(groups['text'].str.contains('growth factor'), \"0\", spec['insulin_text'])\n",
    "spec['insulin_text'] = np.where(groups['text'].str.contains(' igf'), \"0\", spec['insulin_text'])\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['insulin_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f15d1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67296</th>\n",
       "      <td>predictive models for diabetic retinopathy from non-image teleretinal screening data &lt;b&gt;introduction:&lt;/b&gt; timely diabetic retinopathy detection remains a problem in medically underserved settings in the us; diabetic patients in these locales have limited access to eye specialists teleretinal screening programs have been introduced to address this problem &lt;b&gt;methods:&lt;/b&gt; using data on ethnicity, gender, age, hemoglobin a1c, insulin dependence, time since last eye examination, subjective diabetes control, and years with diabetes from 27,116 diabetic patients participating in a los angeles county teleretinal screening program, we compared different machine learning methods for predicting retinopathy the dataset exhibited a class imbalance &lt;b&gt;results:&lt;/b&gt; six classifiers learned on the data were predictive of retinopathy the best model had an auc of 0754, sensitivity of 58% and specificity of 80% &lt;b&gt;discussion:&lt;/b&gt; successfully detecting retinopathy from diabetic patientsroutinely collected clinical data could help clinicians in medically underserved areas identify unscreened diabetic patients who are at risk of developing retinopathy this work is a step towards that goal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34025</th>\n",
       "      <td>obesity in qatar: a case-control study on the identification of associated risk factors obesity is an emerging public health problem in the western world as well as in the gulf region qatar, a tiny wealthy county, is among the top-ranked obese countries with a high obesity rate among its population compared to qatars severity of this health crisis, only a limited number of studies focused on the systematic identification of potential risk factors using multimodal datasets this study aims to develop machine learning ml models to distinguish healthy from obese individuals and reveal potential risk factors associated with obesity in qatar we designed a case-control study focused on 500 qatari subjects, comprising 250 obese and 250 healthy individuals- the later forming the control group we obtained the most extensive collection of clinical measurements for the qatari population from the qatar biobank qbb repertoire, including i physio-clinical biomarkers, ii spirometry, iii vicorder, iv dxa scan composition, and v dxa scan densitometry readings we developed several machine learning ml models to distinguish healthy from obese individuals and applied multiple feature selection techniques to identify potential risk factors associated with obesity the proposed ml model achieved over 90% accuracy, thereby outperforming the existing state of the art models the outcome from the ablation study on multimodal clinical datasets revealed physio-clinical measurements as the most influential risk factors in distinguishing healthy versus obese subjects furthermore, multiple feature ranking techniques confirmed known obesity risk factors c-peptide, insulin, albumin, uric acid and identified potential risk factors linked to obesity-related comorbidities such as diabetes eg, hba1c, glucose, liver function eg, alkaline phosphatase, gamma-glutamyl transferase, lipid profile eg, triglyceride, low density lipoprotein cholesterol, high density lipoprotein cholesterol, etc most of the dxa measurements eg, bone area, bone mineral composition, bone mineral density, etc were significantly &lt;i&gt;p&lt;/i&gt;-value &lt; 005 higher in the obese group overall, the net effect of hypothesized protective factors of obesity on bone mass seems to have surpassed the hypothesized harmful factors all the identified factors warrant further investigation in a clinical setup to understand their role in obesity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150524</th>\n",
       "      <td>detection of hypoglycemic episodes in children with type 1 diabetes using an optimal bayesian neural network algorithm hypoglycemia or low blood glucose is a common and serious side effect of insulin therapy in patients with diabetes hypomon is a non-invasive monitor that measures some physiological parameters continuously to provide detection of hypoglycemic episodes in type 1 diabetes mellitus patients t1dm based on heart rate, corrected qt interval of the ecg signal and skin impedance, a bayesian neural network detection algorithm has been developed to recognize the presence of hypoglycemic episodes from a clinical study of 25 children with t1dm, associated with hypoglycemic episodes, their heart rates increased 1152+/-0157 vs 1035+/-0108, p&lt;00001, their corrected qt intervals increased 1088+/-0086 vs 1020+/-0062, p&lt;00001 and their skin impedances reduced significantly 0679+/-0195 vs 0837+/-0203, p&lt;00001 the overall data were organized into a training set 14 cases and a test set 14 cases randomly selected using an optimal bayesian neural network with 11 hidden nodes, and an algorithm developed from the training set, a sensitivity of 08346 and specificity of 06388 were achieved for the test set</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122952</th>\n",
       "      <td>a glucose model based on support vector regression for the prediction of hypoglycemic events under free-living conditions the prevention of hypoglycemic events is of paramount importance in the daily management of insulin-treated diabetes the use of short-term prediction algorithms of the subcutaneous sc glucose concentration may contribute significantly toward this direction the literature suggests that, although the recent glucose profile is a prominent predictor of hypoglycemia, the overall patients context greatly impacts its accurate estimation the objective of this study is to evaluate the performance of a support vector for regression svr sc glucose method on hypoglycemia prediction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150510</th>\n",
       "      <td>controlling blood glucose levels in diabetics by neural network predictor in this study we develop a system that uses some variables such as, level of exercise, stress, food intake, injected insulin and blood glucose level in previous intervals, as input and accurately predicts the blood glucose level in the next interval the system is split up to make separate prediction of blood glucose level in the morning, afternoon, evening and night, using data from one patient covering a period of 77 days we have used rbf neural network, and compared our result with mlp neural network that was implemented by the others the assessment of the analysis resulted in a root mean square error of 004+/-00004 mmol/l</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64728</th>\n",
       "      <td>minimizing postprandial hypoglycemia in type 1 diabetes patients using multiple insulin injections and capillary blood glucose self-monitoring with machine learning techniques diabetic patients treated with intensive insulin therapies require a tight glycemic control and may benefit from advanced tools to predict blood glucose bg concentration levels and hypo/hyperglycemia events prediction systems using machine learning techniques have mainly focused on applications for sensor augmented pump sap therapy in contrast, insulin bolus calculators that rely on bg prediction for multiple daily insulin mdi injections for patients under self-monitoring blood glucose smbg are scarce because of insufficient data sources and limited prediction capability of forecasting models</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24524</th>\n",
       "      <td>the use of machine learning techniques to determine the predictive value of inflammatory biomarkers in the development of type 2 diabetes mellitus &lt;b&gt;&lt;i&gt;background:&lt;/i&gt;&lt;/b&gt; certain inflammatory biomarkers, such as interleukin-6, interleukin-1, c-reactive protein crp, and fibrinogen, are prototypical acute-phase parameters that can also be predictors of cardiovascular disease however, this inflammatory response can also be linked to the development of type 2 diabetes mellitus t2dm &lt;b&gt;&lt;i&gt;methods:&lt;/i&gt;&lt;/b&gt; we performed a cross-sectional, retrospective study of hypertensive patients in an outpatient setting demographic, clinical, and laboratory parameters, such as the homeostatic model assessment of insulin resistance homa-ir, crp, and fibrinogen, were recorded the outcome was progression to overt t2dm over the 12-year observation period &lt;b&gt;&lt;i&gt;results:&lt;/i&gt;&lt;/b&gt; a total of 3,472 hypertensive patients were screened, but 1,576 individuals without t2dm were ultimately included in the analyses patients with elevated fibrinogen, crp, and insulin resistance had a significantly greater incidence of progression to t2dm during follow-up, 199 patients progressed to t2dm multivariate logistic regression analyses showed that body mass index odds ratio or 104, 95% confidence interval ci: 101-107, homa-ir or 113, 95% ci: 108-116, age or 105, 95% ci: 103-107, logcrp or 137, 95% ci: 114-155, and fibrinogen or 144, 95% ci: 123-166 were the most important predictors of progression to t2dm the area under the receiver operating characteristic curve auc of this model was 076 using machine learning methods, we built a model that included homa-ir, fibrinogen, and logcrp that was more accurate than the logistic regression model, with an auc of 09 &lt;b&gt;&lt;i&gt;conclusion:&lt;/i&gt;&lt;/b&gt; our results suggest that inflammatory biomarkers and homa-ir have a strong prognostic value in predicting progression to t2dm machine learning methods can provide more accurate results to better understand the implications of these features in terms of progression to t2dm a successful therapeutic approach based on these features can avoid progression to t2dm and thus improve long-term survival</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172023</th>\n",
       "      <td>simulation studies on neural predictive control of glucose using the subcutaneous route a novel strategy for closed-loop control of glucose using subcutaneous sc tissue glucose measurement and sc infusion of monomeric insulin analogues was developed and evaluated in a simulation study the proposed control strategy is an amalgamation of a neural network and nonlinear model predictive control npc technique a radial basis function neural network was used for off-line system identification of nonlinear auto regressive model with exogenous inputs narx model of the glucoregulatory system the explicit narx model obtained from the off-line identification procedure was then used to predict the effects of future control actions numerical studies were carried out using a comprehensive model of glucose regulation the system identification procedure enabled construction of a parsimonious network from the stimulated data, and consequently, design of a controller using multiple-step-ahead predictions of the previously identified model according to the simulation results, stable control is achievable in the presence of large noise levels and for unknown or variable physiological or technical time delays in conclusion, the simulation results suggest that closed-loop control of glucose will be achievable using sc glucose measurement and sc insulin administration however, the control limitations due to the sc insulin administration makes additional action of the patient at meal time necessary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55279</th>\n",
       "      <td>a hybrid approach for modeling type 2 diabetes mellitus progression type 2 diabetes mellitus t2dm is a chronic, progressive metabolic disorder characterized by hyperglycemia resulting from abnormalities in insulin secretion, insulin action, or both it is associated with an increased risk of developing vascular complication of micro as well as macro nature because of its inconspicuous and heterogeneous character, the management of t2dm is very complex modeling physiological processes over time demonstrating the patients evolving health condition is imperative to comprehending the patients current status of health, projecting its likely dynamics and assessing the requisite care and treatment measures in future hidden markov model hmm is an effective approach for such prognostic modeling however, the nature of the clinical setting, together with the format of the electronic medical records emrs data, in particular the sparse and irregularly sampled clinical data which is well understood to present significant challenges, has confounded standard hmm in the present study, we proposed an approximation technique based on newtons divided difference method nddm as a component with hmm to determine the risk of developing diabetes in an individual over different time horizons using irregular and sparsely sampled emrs data the proposed method is capable of exploiting available sequences of clinical measurements obtained from a longitudinal sample of patients for effective imputation and improved prediction performance furthermore, results demonstrated that the discrimination capability of our proposed method, in prognosticating diabetes risk, is superior to the standard hmm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171982</th>\n",
       "      <td>neural predictive controller for insulin delivery using the subcutaneous route a neural predictive controller for closed-loop control of glucose using subcutaneous sc tissue glucose measurement and sc infusion of monomeric insulin analogs was developed and evaluated in a simulation study the proposed control strategy is based on off-line system identification using neural networks nns and nonlinear model predictive controller design the system identification framework combines the concept of nonlinear autoregressive model with exogenous inputs narx system representation, regularization approach for constructing radial basis function nns, and validation methods for nonlinear systems numerical studies on system identification and closed-loop control of glucose were carried out using a comprehensive model of glucose regulation and a pharmacokinetic model for the absorption of monomeric insulin analogs from the sc depot the system identification procedure enabled construction of a parsimonious network from the simulated data, and consequently, design of a controller using multiple-step-ahead predictions of the previously identified model according to the simulation results, stable control is achievable in the presence of large noise levels, for unknown or variable time delays as well as for slow time variations of the controlled process however, the control limitations due to the sc insulin administration makes additional action from the patient at meal time necessary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165248</th>\n",
       "      <td>integrating model-based decision support in a multi-modal reasoning system for managing type 1 diabetic patients we present a multi-modal reasoning mmr methodology that integrates case-based reasoning cbr, rule-based reasoning rbr and model-based reasoning mbr, meant to provide physicians with a reliable decision support tool in the context of type 1 diabetes mellitus management in particular, we have implemented a decision support system that is able to jointly exploit a probabilistic model of the glucose-insulin system at the steady state, a rbr system for suggestion generation and a cbr system for patients profiling the integration of the cbr, rbr and mbr paradigms allows for an optimized exploitation of all the available information, and for the definition of a therapy properly tailored to the patients needs, overcoming the single approaches limitations the system has been tested both on simulated and on real patientsdata</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131592</th>\n",
       "      <td>artificial neural networks for closed loop control of in silico and ad hoc type 1 diabetes the closed loop control of blood glucose levels might help to reduce many short- and long-term complications of type 1 diabetes continuous glucose monitoring and insulin pump systems have facilitated the development of the artificial pancreas in this paper, artificial neural networks are used for both the identification of patient dynamics and the glycaemic regulation a subcutaneous glucose measuring system together with a lispro insulin subcutaneous pump were used to gather clinical data for each patient undergoing treatment, and a corresponding in silico and ad hoc neural network model was derived for each patient to represent their particular glucose-insulin relationship based on this nonlinear neural network model, an ad hoc neural network controller was designed to close the feedback loop for glycaemic regulation of the in silico patient both the neural network model and the controller were tested for each patient under simulation, and the results obtained show a good performance during food intake and variable exercise conditions</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51536</th>\n",
       "      <td>prediction of nocturnal hypoglycemia in adults with type 1 diabetes under multiple daily injections using continuous glucose monitoring and physical activity monitor 1 background: nocturnal hypoglycemia nh is one of the most challenging side effects of multiple doses of insulin mdi therapy in type 1 diabetes t1d this work aimed to investigate the feasibility of a machine-learning-based prediction model to anticipate nh in t1d patients on mdi 2 methods: ten t1d adults were studied during 12 weeks information regarding t1d management, continuous glucose monitoring cgm, and from a physical activity tracker were obtained under free-living conditions at home supervised machine-learning algorithms were applied to the data, and prediction models were created to forecast the occurrence of nh individualized prediction models were generated using multilayer perceptron mlp and a support vector machine svm 3 results: population outcomes indicated that more than 70% of the nh may be avoided with the proposed methodology the predictions performed by the svm achieved the best population outcomes, with a sensitivity and specificity of 7875% and 8215%, respectively 4 conclusions: our study supports the feasibility of using ml techniques to address the prediction of nocturnal hypoglycemia in the daily life of patients with t1d on mdi, using cgm and a physical activity tracker</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50047</th>\n",
       "      <td>machine learning-based adherence detection of type 2 diabetes patients on once-daily basal insulin injections lack of treatment adherence can lead to life-threatening health complications for people with type 2 diabetes t2d recent improvements and availability in continuous glucose monitoring cgm technology have enabled various possibilities to monitor diabetes treatment detection of missed once-daily basal insulin injections can be used to provide feedback to patients, thus improving their diabetes management in this study, we explore how &lt;i&gt;machine learning&lt;/i&gt; ml based on cgm data can be used for detecting &lt;i&gt;adherence&lt;/i&gt; to once-daily basal insulin injections</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61892</th>\n",
       "      <td>development of a plasma screening panel for pediatric nonalcoholic fatty liver disease using metabolomics nonalcoholic fatty liver disease nafld is the most common chronic liver disease in children, but diagnosis is challenging due to limited availability of noninvasive biomarkers machine learning applied to high-resolution metabolomics and clinical phenotype data offers a novel framework for developing a nafld screening panel in youth here, untargeted metabolomics by liquid chromatography-mass spectrometry was performed on plasma samples from a combined cross-sectional sample of children and adolescents ages 2-25 years old with nafld n = 222 and without nafld n = 337, confirmed by liver biopsy or magnetic resonance imaging anthropometrics, blood lipids, liver enzymes, and glucose and insulin metabolism were also assessed a machine learning approach was applied to the metabolomics and clinical phenotype data sets, which were split into training and test sets, and included dimension reduction, feature selection, and classification model development the selected metabolite features were the amino acids serine, leucine/isoleucine, and tryptophan; three putatively annotated compounds dihydrothymine and two phospholipids; and two unknowns the selected clinical phenotype variables were waist circumference, whole-body insulin sensitivity index wbisi based on the oral glucose tolerance test, and blood triglycerides the highest performing classification model was random forest, which had an area under the receiver operating characteristic curve auroc of 094, sensitivity of 73%, and specificity of 97% for detecting nafld cases a second classification model was developed using the homeostasis model assessment of insulin resistance substituted for the wbisi similarly, the highest performing classification model was random forest, which had an auroc of 092, sensitivity of 73%, and specificity of 94% &lt;i&gt;conclusion:&lt;/i&gt; the identified screening panel consisting of both metabolomics and clinical features has promising potential for screening for nafld in youth further development of this panel and independent validation testing in other cohorts are warranted</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26410</th>\n",
       "      <td>the challenge of predicting blood glucose concentration changes in patients with type i diabetes patients with type i diabetes t1d must take insulin injections to prevent the serious long term effects of hyperglycemia they must also be careful not to inject too much insulin because this could induce potentially fatal hypoglycemia patients therefore follow a regimen that determines how much insulin to inject at each time, based on various measurements we can produce an effective regimen if we can accurately predict a patients future blood glucose bg values from his/her current features this study explores the challenges of predicting future bg by applying a number of machine learning algorithms, as well as various data preprocessing variations corresponding to 312 learner, preprocessed-dataset combinations, to a new t1d dataset that contains 29,601 entries from 47 different patients our most accurate predictor, a weighted ensemble of two gaussian process regression models, achieved a cross-validation &lt;math xmlns=http://wwww3org/1998/math/mathml&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt; loss of 27 mmol/l 4865 mg/dl this result was unexpectedly poor given that one can obtain an &lt;math xmlns=http://wwww3org/1998/math/mathml&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt; of 29 mmol/l 5243 mg/dl using the naive approach of simply predicting the patients average bg these results suggest that the diabetes diary data that is typically collected may be insufficient to produce accurate bg prediction models; additional data may be necessary to build accurate bg prediction models over hours</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77553</th>\n",
       "      <td>prediction of glucose metabolism disorder risk using a machine learning algorithm: pilot study a 75-g oral glucose tolerance test ogtt provides important information about glucose metabolism, although the test is expensive and invasive complete ogtt information, such as 1-hour and 2-hour postloading plasma glucose and immunoreactive insulin levels, may be useful for predicting the future risk of diabetes or glucose metabolism disorders gmd, which includes both diabetes and prediabetes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22965</th>\n",
       "      <td>forecasting of glucose levels and hypoglycemic events: head-to-head comparison of linear and nonlinear data-driven algorithms based on continuous glucose monitoring data only in type 1 diabetes management, the availability of algorithms capable of accurately forecasting future blood glucose bg concentrations and hypoglycemic episodes could enable proactive therapeutic actions, eg, the consumption of carbohydrates to mitigate, or even avoid, an impending critical event the only input of this kind of algorithm is often continuous glucose monitoring cgm sensor data, because other signals such as injected insulin, ingested carbs, and physical activity are frequently unavailable several predictive algorithms fed by cgm data only have been proposed in the literature, but they were assessed using datasets originated by different experimental protocols, making a comparison of their relative merits difficult the aim of the present work was to perform a head-to-head comparison of thirty different linear and nonlinear predictive algorithms using the same dataset, given by 124 cgm traces collected over 10 days with the newest dexcom g6 sensor available on the market and considering a 30-min prediction horizon we considered the state-of-the art methods, investigating, in particular, linear black-box methods autoregressive; autoregressive moving-average; and autoregressive integrated moving-average, arima and nonlinear machine-learning methods support vector regression, svr; regression random forest; feed-forward neural network, fnn; and long short-term memory neural network for each method, the prediction accuracy and hypoglycemia detection capabilities were assessed using either population or individualized model parameters as far as prediction accuracy is concerned, the results show that the best linear algorithm individualized arima provides accuracy comparable to that of the best nonlinear algorithm individualized fnn, with root mean square errors of 2215 and 2152 mg/dl, respectively as far as hypoglycemia detection is concerned, the best linear algorithm individualized arima provided precision = 64%, recall = 82%, and one false alarm/day, comparable to the best nonlinear technique population svr: precision = 63%, recall = 69%, and 05 false alarms/day in general, the head-to-head comparison of the thirty algorithms fed by cgm data only made using a wide dataset shows that individualized linear models are more effective than population ones, while no significant advantages seem to emerge when employing nonlinear methodologies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168614</th>\n",
       "      <td>an intelligent diabetes software prototype: predicting blood glucose levels and recommending regimen changes maintaining optimal blood glucose bg control is difficult for type 1 diabetes mellitus t1dm patients when typical daily regimens of food, insulin and exercise are altered artificial intelligence ai systems consisting of treatment algorithms calibrated through large datasets of patient specific information may offer a solution such a system can predict bg level changes resulting from regimen disturbances and recommend regimen changes for compensation a software prototype based on neural network, fuzzy logic, and expert system concepts was developed and evaluated to determine feasibility and efficacy of a patient specific prediction model bg data are the primary driver for adapting existing functions to patient specific prediction algorithms mean absolute percent error mape between actual and predicted bg values from inputs of daily insulin, food, and exercise information for an t1dm test subject was 105% using a calibrated model the prototype is limited by the requirement for a rigid testing schedule, human error and situational circumstances such as alcohol consumption, illness, infection, stress, and significant hormonal imbalances no significant conclusions regarding model validity can be drawn due to limited evaluation process and subject sample size, although the prototype has demonstrated viability as a learning tool for diabetes patients increased impetus for further development of this prototype and similar ai models may materialize when more effective diagnostic and data capture tools become available to reduce testing and improve accuracy of the model with more input data</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135608</th>\n",
       "      <td>neural network-based real-time prediction of glucose in patients with insulin-dependent diabetes continuous glucose monitoring cgm technologies report measurements of interstitial glucose concentration every 5 min cgm technologies have the potential to be utilized for prediction of prospective glucose concentrations with subsequent optimization of glycemic control this article outlines a feed-forward neural network model nnm utilized for real-time prediction of glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  \\\n",
       "67296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  predictive models for diabetic retinopathy from non-image teleretinal screening data <b>introduction:</b> timely diabetic retinopathy detection remains a problem in medically underserved settings in the us; diabetic patients in these locales have limited access to eye specialists teleretinal screening programs have been introduced to address this problem <b>methods:</b> using data on ethnicity, gender, age, hemoglobin a1c, insulin dependence, time since last eye examination, subjective diabetes control, and years with diabetes from 27,116 diabetic patients participating in a los angeles county teleretinal screening program, we compared different machine learning methods for predicting retinopathy the dataset exhibited a class imbalance <b>results:</b> six classifiers learned on the data were predictive of retinopathy the best model had an auc of 0754, sensitivity of 58% and specificity of 80% <b>discussion:</b> successfully detecting retinopathy from diabetic patientsroutinely collected clinical data could help clinicians in medically underserved areas identify unscreened diabetic patients who are at risk of developing retinopathy this work is a step towards that goal   \n",
       "34025                                                                                                                                                                         obesity in qatar: a case-control study on the identification of associated risk factors obesity is an emerging public health problem in the western world as well as in the gulf region qatar, a tiny wealthy county, is among the top-ranked obese countries with a high obesity rate among its population compared to qatars severity of this health crisis, only a limited number of studies focused on the systematic identification of potential risk factors using multimodal datasets this study aims to develop machine learning ml models to distinguish healthy from obese individuals and reveal potential risk factors associated with obesity in qatar we designed a case-control study focused on 500 qatari subjects, comprising 250 obese and 250 healthy individuals- the later forming the control group we obtained the most extensive collection of clinical measurements for the qatari population from the qatar biobank qbb repertoire, including i physio-clinical biomarkers, ii spirometry, iii vicorder, iv dxa scan composition, and v dxa scan densitometry readings we developed several machine learning ml models to distinguish healthy from obese individuals and applied multiple feature selection techniques to identify potential risk factors associated with obesity the proposed ml model achieved over 90% accuracy, thereby outperforming the existing state of the art models the outcome from the ablation study on multimodal clinical datasets revealed physio-clinical measurements as the most influential risk factors in distinguishing healthy versus obese subjects furthermore, multiple feature ranking techniques confirmed known obesity risk factors c-peptide, insulin, albumin, uric acid and identified potential risk factors linked to obesity-related comorbidities such as diabetes eg, hba1c, glucose, liver function eg, alkaline phosphatase, gamma-glutamyl transferase, lipid profile eg, triglyceride, low density lipoprotein cholesterol, high density lipoprotein cholesterol, etc most of the dxa measurements eg, bone area, bone mineral composition, bone mineral density, etc were significantly <i>p</i>-value < 005 higher in the obese group overall, the net effect of hypothesized protective factors of obesity on bone mass seems to have surpassed the hypothesized harmful factors all the identified factors warrant further investigation in a clinical setup to understand their role in obesity   \n",
       "150524                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    detection of hypoglycemic episodes in children with type 1 diabetes using an optimal bayesian neural network algorithm hypoglycemia or low blood glucose is a common and serious side effect of insulin therapy in patients with diabetes hypomon is a non-invasive monitor that measures some physiological parameters continuously to provide detection of hypoglycemic episodes in type 1 diabetes mellitus patients t1dm based on heart rate, corrected qt interval of the ecg signal and skin impedance, a bayesian neural network detection algorithm has been developed to recognize the presence of hypoglycemic episodes from a clinical study of 25 children with t1dm, associated with hypoglycemic episodes, their heart rates increased 1152+/-0157 vs 1035+/-0108, p<00001, their corrected qt intervals increased 1088+/-0086 vs 1020+/-0062, p<00001 and their skin impedances reduced significantly 0679+/-0195 vs 0837+/-0203, p<00001 the overall data were organized into a training set 14 cases and a test set 14 cases randomly selected using an optimal bayesian neural network with 11 hidden nodes, and an algorithm developed from the training set, a sensitivity of 08346 and specificity of 06388 were achieved for the test set   \n",
       "122952                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a glucose model based on support vector regression for the prediction of hypoglycemic events under free-living conditions the prevention of hypoglycemic events is of paramount importance in the daily management of insulin-treated diabetes the use of short-term prediction algorithms of the subcutaneous sc glucose concentration may contribute significantly toward this direction the literature suggests that, although the recent glucose profile is a prominent predictor of hypoglycemia, the overall patients context greatly impacts its accurate estimation the objective of this study is to evaluate the performance of a support vector for regression svr sc glucose method on hypoglycemia prediction   \n",
       "150510                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 controlling blood glucose levels in diabetics by neural network predictor in this study we develop a system that uses some variables such as, level of exercise, stress, food intake, injected insulin and blood glucose level in previous intervals, as input and accurately predicts the blood glucose level in the next interval the system is split up to make separate prediction of blood glucose level in the morning, afternoon, evening and night, using data from one patient covering a period of 77 days we have used rbf neural network, and compared our result with mlp neural network that was implemented by the others the assessment of the analysis resulted in a root mean square error of 004+/-00004 mmol/l   \n",
       "64728                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             minimizing postprandial hypoglycemia in type 1 diabetes patients using multiple insulin injections and capillary blood glucose self-monitoring with machine learning techniques diabetic patients treated with intensive insulin therapies require a tight glycemic control and may benefit from advanced tools to predict blood glucose bg concentration levels and hypo/hyperglycemia events prediction systems using machine learning techniques have mainly focused on applications for sensor augmented pump sap therapy in contrast, insulin bolus calculators that rely on bg prediction for multiple daily insulin mdi injections for patients under self-monitoring blood glucose smbg are scarce because of insufficient data sources and limited prediction capability of forecasting models   \n",
       "24524                                                                                                                                                                                                                                                                                                                                                                                                            the use of machine learning techniques to determine the predictive value of inflammatory biomarkers in the development of type 2 diabetes mellitus <b><i>background:</i></b> certain inflammatory biomarkers, such as interleukin-6, interleukin-1, c-reactive protein crp, and fibrinogen, are prototypical acute-phase parameters that can also be predictors of cardiovascular disease however, this inflammatory response can also be linked to the development of type 2 diabetes mellitus t2dm <b><i>methods:</i></b> we performed a cross-sectional, retrospective study of hypertensive patients in an outpatient setting demographic, clinical, and laboratory parameters, such as the homeostatic model assessment of insulin resistance homa-ir, crp, and fibrinogen, were recorded the outcome was progression to overt t2dm over the 12-year observation period <b><i>results:</i></b> a total of 3,472 hypertensive patients were screened, but 1,576 individuals without t2dm were ultimately included in the analyses patients with elevated fibrinogen, crp, and insulin resistance had a significantly greater incidence of progression to t2dm during follow-up, 199 patients progressed to t2dm multivariate logistic regression analyses showed that body mass index odds ratio or 104, 95% confidence interval ci: 101-107, homa-ir or 113, 95% ci: 108-116, age or 105, 95% ci: 103-107, logcrp or 137, 95% ci: 114-155, and fibrinogen or 144, 95% ci: 123-166 were the most important predictors of progression to t2dm the area under the receiver operating characteristic curve auc of this model was 076 using machine learning methods, we built a model that included homa-ir, fibrinogen, and logcrp that was more accurate than the logistic regression model, with an auc of 09 <b><i>conclusion:</i></b> our results suggest that inflammatory biomarkers and homa-ir have a strong prognostic value in predicting progression to t2dm machine learning methods can provide more accurate results to better understand the implications of these features in terms of progression to t2dm a successful therapeutic approach based on these features can avoid progression to t2dm and thus improve long-term survival   \n",
       "172023                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         simulation studies on neural predictive control of glucose using the subcutaneous route a novel strategy for closed-loop control of glucose using subcutaneous sc tissue glucose measurement and sc infusion of monomeric insulin analogues was developed and evaluated in a simulation study the proposed control strategy is an amalgamation of a neural network and nonlinear model predictive control npc technique a radial basis function neural network was used for off-line system identification of nonlinear auto regressive model with exogenous inputs narx model of the glucoregulatory system the explicit narx model obtained from the off-line identification procedure was then used to predict the effects of future control actions numerical studies were carried out using a comprehensive model of glucose regulation the system identification procedure enabled construction of a parsimonious network from the stimulated data, and consequently, design of a controller using multiple-step-ahead predictions of the previously identified model according to the simulation results, stable control is achievable in the presence of large noise levels and for unknown or variable physiological or technical time delays in conclusion, the simulation results suggest that closed-loop control of glucose will be achievable using sc glucose measurement and sc insulin administration however, the control limitations due to the sc insulin administration makes additional action of the patient at meal time necessary   \n",
       "55279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          a hybrid approach for modeling type 2 diabetes mellitus progression type 2 diabetes mellitus t2dm is a chronic, progressive metabolic disorder characterized by hyperglycemia resulting from abnormalities in insulin secretion, insulin action, or both it is associated with an increased risk of developing vascular complication of micro as well as macro nature because of its inconspicuous and heterogeneous character, the management of t2dm is very complex modeling physiological processes over time demonstrating the patients evolving health condition is imperative to comprehending the patients current status of health, projecting its likely dynamics and assessing the requisite care and treatment measures in future hidden markov model hmm is an effective approach for such prognostic modeling however, the nature of the clinical setting, together with the format of the electronic medical records emrs data, in particular the sparse and irregularly sampled clinical data which is well understood to present significant challenges, has confounded standard hmm in the present study, we proposed an approximation technique based on newtons divided difference method nddm as a component with hmm to determine the risk of developing diabetes in an individual over different time horizons using irregular and sparsely sampled emrs data the proposed method is capable of exploiting available sequences of clinical measurements obtained from a longitudinal sample of patients for effective imputation and improved prediction performance furthermore, results demonstrated that the discrimination capability of our proposed method, in prognosticating diabetes risk, is superior to the standard hmm   \n",
       "171982                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   neural predictive controller for insulin delivery using the subcutaneous route a neural predictive controller for closed-loop control of glucose using subcutaneous sc tissue glucose measurement and sc infusion of monomeric insulin analogs was developed and evaluated in a simulation study the proposed control strategy is based on off-line system identification using neural networks nns and nonlinear model predictive controller design the system identification framework combines the concept of nonlinear autoregressive model with exogenous inputs narx system representation, regularization approach for constructing radial basis function nns, and validation methods for nonlinear systems numerical studies on system identification and closed-loop control of glucose were carried out using a comprehensive model of glucose regulation and a pharmacokinetic model for the absorption of monomeric insulin analogs from the sc depot the system identification procedure enabled construction of a parsimonious network from the simulated data, and consequently, design of a controller using multiple-step-ahead predictions of the previously identified model according to the simulation results, stable control is achievable in the presence of large noise levels, for unknown or variable time delays as well as for slow time variations of the controlled process however, the control limitations due to the sc insulin administration makes additional action from the patient at meal time necessary   \n",
       "165248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        integrating model-based decision support in a multi-modal reasoning system for managing type 1 diabetic patients we present a multi-modal reasoning mmr methodology that integrates case-based reasoning cbr, rule-based reasoning rbr and model-based reasoning mbr, meant to provide physicians with a reliable decision support tool in the context of type 1 diabetes mellitus management in particular, we have implemented a decision support system that is able to jointly exploit a probabilistic model of the glucose-insulin system at the steady state, a rbr system for suggestion generation and a cbr system for patients profiling the integration of the cbr, rbr and mbr paradigms allows for an optimized exploitation of all the available information, and for the definition of a therapy properly tailored to the patients needs, overcoming the single approaches limitations the system has been tested both on simulated and on real patientsdata   \n",
       "131592                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             artificial neural networks for closed loop control of in silico and ad hoc type 1 diabetes the closed loop control of blood glucose levels might help to reduce many short- and long-term complications of type 1 diabetes continuous glucose monitoring and insulin pump systems have facilitated the development of the artificial pancreas in this paper, artificial neural networks are used for both the identification of patient dynamics and the glycaemic regulation a subcutaneous glucose measuring system together with a lispro insulin subcutaneous pump were used to gather clinical data for each patient undergoing treatment, and a corresponding in silico and ad hoc neural network model was derived for each patient to represent their particular glucose-insulin relationship based on this nonlinear neural network model, an ad hoc neural network controller was designed to close the feedback loop for glycaemic regulation of the in silico patient both the neural network model and the controller were tested for each patient under simulation, and the results obtained show a good performance during food intake and variable exercise conditions   \n",
       "51536                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                prediction of nocturnal hypoglycemia in adults with type 1 diabetes under multiple daily injections using continuous glucose monitoring and physical activity monitor 1 background: nocturnal hypoglycemia nh is one of the most challenging side effects of multiple doses of insulin mdi therapy in type 1 diabetes t1d this work aimed to investigate the feasibility of a machine-learning-based prediction model to anticipate nh in t1d patients on mdi 2 methods: ten t1d adults were studied during 12 weeks information regarding t1d management, continuous glucose monitoring cgm, and from a physical activity tracker were obtained under free-living conditions at home supervised machine-learning algorithms were applied to the data, and prediction models were created to forecast the occurrence of nh individualized prediction models were generated using multilayer perceptron mlp and a support vector machine svm 3 results: population outcomes indicated that more than 70% of the nh may be avoided with the proposed methodology the predictions performed by the svm achieved the best population outcomes, with a sensitivity and specificity of 7875% and 8215%, respectively 4 conclusions: our study supports the feasibility of using ml techniques to address the prediction of nocturnal hypoglycemia in the daily life of patients with t1d on mdi, using cgm and a physical activity tracker   \n",
       "50047                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    machine learning-based adherence detection of type 2 diabetes patients on once-daily basal insulin injections lack of treatment adherence can lead to life-threatening health complications for people with type 2 diabetes t2d recent improvements and availability in continuous glucose monitoring cgm technology have enabled various possibilities to monitor diabetes treatment detection of missed once-daily basal insulin injections can be used to provide feedback to patients, thus improving their diabetes management in this study, we explore how <i>machine learning</i> ml based on cgm data can be used for detecting <i>adherence</i> to once-daily basal insulin injections   \n",
       "61892                                                                                                                                                                                                                                                                                                                                                                                                development of a plasma screening panel for pediatric nonalcoholic fatty liver disease using metabolomics nonalcoholic fatty liver disease nafld is the most common chronic liver disease in children, but diagnosis is challenging due to limited availability of noninvasive biomarkers machine learning applied to high-resolution metabolomics and clinical phenotype data offers a novel framework for developing a nafld screening panel in youth here, untargeted metabolomics by liquid chromatography-mass spectrometry was performed on plasma samples from a combined cross-sectional sample of children and adolescents ages 2-25 years old with nafld n = 222 and without nafld n = 337, confirmed by liver biopsy or magnetic resonance imaging anthropometrics, blood lipids, liver enzymes, and glucose and insulin metabolism were also assessed a machine learning approach was applied to the metabolomics and clinical phenotype data sets, which were split into training and test sets, and included dimension reduction, feature selection, and classification model development the selected metabolite features were the amino acids serine, leucine/isoleucine, and tryptophan; three putatively annotated compounds dihydrothymine and two phospholipids; and two unknowns the selected clinical phenotype variables were waist circumference, whole-body insulin sensitivity index wbisi based on the oral glucose tolerance test, and blood triglycerides the highest performing classification model was random forest, which had an area under the receiver operating characteristic curve auroc of 094, sensitivity of 73%, and specificity of 97% for detecting nafld cases a second classification model was developed using the homeostasis model assessment of insulin resistance substituted for the wbisi similarly, the highest performing classification model was random forest, which had an auroc of 092, sensitivity of 73%, and specificity of 94% <i>conclusion:</i> the identified screening panel consisting of both metabolomics and clinical features has promising potential for screening for nafld in youth further development of this panel and independent validation testing in other cohorts are warranted   \n",
       "26410                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        the challenge of predicting blood glucose concentration changes in patients with type i diabetes patients with type i diabetes t1d must take insulin injections to prevent the serious long term effects of hyperglycemia they must also be careful not to inject too much insulin because this could induce potentially fatal hypoglycemia patients therefore follow a regimen that determines how much insulin to inject at each time, based on various measurements we can produce an effective regimen if we can accurately predict a patients future blood glucose bg values from his/her current features this study explores the challenges of predicting future bg by applying a number of machine learning algorithms, as well as various data preprocessing variations corresponding to 312 learner, preprocessed-dataset combinations, to a new t1d dataset that contains 29,601 entries from 47 different patients our most accurate predictor, a weighted ensemble of two gaussian process regression models, achieved a cross-validation <math xmlns=http://wwww3org/1998/math/mathml><mrow><mi>e</mi><mi>r</mi><msub><mi>r</mi><mrow><mi>l</mi><mn>1</mn></mrow></msub></mrow></math> loss of 27 mmol/l 4865 mg/dl this result was unexpectedly poor given that one can obtain an <math xmlns=http://wwww3org/1998/math/mathml><mrow><mi>e</mi><mi>r</mi><msub><mi>r</mi><mrow><mi>l</mi><mn>1</mn></mrow></msub></mrow></math> of 29 mmol/l 5243 mg/dl using the naive approach of simply predicting the patients average bg these results suggest that the diabetes diary data that is typically collected may be insufficient to produce accurate bg prediction models; additional data may be necessary to build accurate bg prediction models over hours   \n",
       "77553                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           prediction of glucose metabolism disorder risk using a machine learning algorithm: pilot study a 75-g oral glucose tolerance test ogtt provides important information about glucose metabolism, although the test is expensive and invasive complete ogtt information, such as 1-hour and 2-hour postloading plasma glucose and immunoreactive insulin levels, may be useful for predicting the future risk of diabetes or glucose metabolism disorders gmd, which includes both diabetes and prediabetes   \n",
       "22965   forecasting of glucose levels and hypoglycemic events: head-to-head comparison of linear and nonlinear data-driven algorithms based on continuous glucose monitoring data only in type 1 diabetes management, the availability of algorithms capable of accurately forecasting future blood glucose bg concentrations and hypoglycemic episodes could enable proactive therapeutic actions, eg, the consumption of carbohydrates to mitigate, or even avoid, an impending critical event the only input of this kind of algorithm is often continuous glucose monitoring cgm sensor data, because other signals such as injected insulin, ingested carbs, and physical activity are frequently unavailable several predictive algorithms fed by cgm data only have been proposed in the literature, but they were assessed using datasets originated by different experimental protocols, making a comparison of their relative merits difficult the aim of the present work was to perform a head-to-head comparison of thirty different linear and nonlinear predictive algorithms using the same dataset, given by 124 cgm traces collected over 10 days with the newest dexcom g6 sensor available on the market and considering a 30-min prediction horizon we considered the state-of-the art methods, investigating, in particular, linear black-box methods autoregressive; autoregressive moving-average; and autoregressive integrated moving-average, arima and nonlinear machine-learning methods support vector regression, svr; regression random forest; feed-forward neural network, fnn; and long short-term memory neural network for each method, the prediction accuracy and hypoglycemia detection capabilities were assessed using either population or individualized model parameters as far as prediction accuracy is concerned, the results show that the best linear algorithm individualized arima provides accuracy comparable to that of the best nonlinear algorithm individualized fnn, with root mean square errors of 2215 and 2152 mg/dl, respectively as far as hypoglycemia detection is concerned, the best linear algorithm individualized arima provided precision = 64%, recall = 82%, and one false alarm/day, comparable to the best nonlinear technique population svr: precision = 63%, recall = 69%, and 05 false alarms/day in general, the head-to-head comparison of the thirty algorithms fed by cgm data only made using a wide dataset shows that individualized linear models are more effective than population ones, while no significant advantages seem to emerge when employing nonlinear methodologies   \n",
       "168614                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               an intelligent diabetes software prototype: predicting blood glucose levels and recommending regimen changes maintaining optimal blood glucose bg control is difficult for type 1 diabetes mellitus t1dm patients when typical daily regimens of food, insulin and exercise are altered artificial intelligence ai systems consisting of treatment algorithms calibrated through large datasets of patient specific information may offer a solution such a system can predict bg level changes resulting from regimen disturbances and recommend regimen changes for compensation a software prototype based on neural network, fuzzy logic, and expert system concepts was developed and evaluated to determine feasibility and efficacy of a patient specific prediction model bg data are the primary driver for adapting existing functions to patient specific prediction algorithms mean absolute percent error mape between actual and predicted bg values from inputs of daily insulin, food, and exercise information for an t1dm test subject was 105% using a calibrated model the prototype is limited by the requirement for a rigid testing schedule, human error and situational circumstances such as alcohol consumption, illness, infection, stress, and significant hormonal imbalances no significant conclusions regarding model validity can be drawn due to limited evaluation process and subject sample size, although the prototype has demonstrated viability as a learning tool for diabetes patients increased impetus for further development of this prototype and similar ai models may materialize when more effective diagnostic and data capture tools become available to reduce testing and improve accuracy of the model with more input data   \n",
       "135608                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          neural network-based real-time prediction of glucose in patients with insulin-dependent diabetes continuous glucose monitoring cgm technologies report measurements of interstitial glucose concentration every 5 min cgm technologies have the potential to be utilized for prediction of prospective glucose concentrations with subsequent optimization of glycemic control this article outlines a feed-forward neural network model nnm utilized for real-time prediction of glucose   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "67296         0       0       0           0          0        0       0   \n",
       "34025         0       0       0           0          0        0       0   \n",
       "150524        0       0       0           0          0        0       0   \n",
       "122952        0       0       0           0          0        0       0   \n",
       "150510        0       0       0           0          0        0       0   \n",
       "64728         0       0       0           0          0        0       0   \n",
       "24524         0       0       0           0          0        0       0   \n",
       "172023        0       0       0           0          0        0       0   \n",
       "55279         0       0       0           0          0        0       0   \n",
       "171982        0       0       0           0          0        0       0   \n",
       "165248        0       0       0           0          0        0       0   \n",
       "131592        0       0       0           0          0        0       0   \n",
       "51536         0       0       0           0          0        0       0   \n",
       "50047         0       0       0           0          0        0       0   \n",
       "61892         0       0       0           0          0        0       0   \n",
       "26410         0       0       0           0          0        0       0   \n",
       "77553         0       0       0           0          0        0       0   \n",
       "22965         0       0       0           0          0        0       0   \n",
       "168614        0       0       1           0          0        0       0   \n",
       "135608        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "67296            0            0         0           0        0       0   \n",
       "34025            0            0         0           0        0       0   \n",
       "150524           0            0         0           0        0       0   \n",
       "122952           0            0         1           0        0       0   \n",
       "150510           0            0         0           0        0       0   \n",
       "64728            0            0         0           0        0       0   \n",
       "24524            0            0         0           0        0       0   \n",
       "172023           0            0         1           0        0       0   \n",
       "55279            0            0         0           0        0       0   \n",
       "171982           0            0         1           0        0       0   \n",
       "165248           0            0         0           0        0       0   \n",
       "131592           0            0         1           0        0       0   \n",
       "51536            0            0         0           0        0       0   \n",
       "50047            0            0         0           0        0       0   \n",
       "61892            0            0         0           0        0       0   \n",
       "26410            0            0         0           0        0       0   \n",
       "77553            0            0         0           0        0       0   \n",
       "22965            0            0         0           0        0       0   \n",
       "168614           0            0         0           0        0       0   \n",
       "135608           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "67296            0             0           0            0         0   \n",
       "34025            0             0           0            0         0   \n",
       "150524           0             0           0            0         0   \n",
       "122952           0             0           0            0         0   \n",
       "150510           0             0           0            0         0   \n",
       "64728            0             0           0            0         0   \n",
       "24524            0             0           0            0         0   \n",
       "172023           0             0           0            0         0   \n",
       "55279            0             0           0            0         0   \n",
       "171982           0             0           0            0         0   \n",
       "165248           0             0           0            0         0   \n",
       "131592           0             0           0            0         0   \n",
       "51536            0             0           0            0         0   \n",
       "50047            0             0           0            0         0   \n",
       "61892            0             0           0            0         0   \n",
       "26410            0             0           0            0         0   \n",
       "77553            0             0           0            0         0   \n",
       "22965            0             0           0            0         0   \n",
       "168614           0             0           0            0         0   \n",
       "135608           0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "67296           0           0            0           0            0   \n",
       "34025           0           0            0           0            0   \n",
       "150524          0           0            0           0            0   \n",
       "122952          0           0            0           0            0   \n",
       "150510          0           0            0           0            0   \n",
       "64728           0           0            0           0            0   \n",
       "24524           0           0            0           0            0   \n",
       "172023          0           0            0           0            0   \n",
       "55279           0           0            0           0            0   \n",
       "171982          0           0            0           0            0   \n",
       "165248          0           0            0           0            0   \n",
       "131592          0           0            0           0            0   \n",
       "51536           0           0            0           0            0   \n",
       "50047           0           0            0           0            0   \n",
       "61892           0           0            0           0            0   \n",
       "26410           0           0            0           0            0   \n",
       "77553           0           0            0           0            0   \n",
       "22965           0           0            0           0            0   \n",
       "168614          0           0            0           0            0   \n",
       "135608          0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "67296           0            0        0         0          0       0        0   \n",
       "34025           0            0        0         0          0       0        1   \n",
       "150524          0            0        0         0          0       0        0   \n",
       "122952          0            0        0         0          0       0        0   \n",
       "150510          0            0        0         0          0       0        0   \n",
       "64728           0            0        0         0          0       0        0   \n",
       "24524           0            0        0         0          0       0        0   \n",
       "172023          0            0        0         0          0       0        0   \n",
       "55279           0            0        0         0          0       0        0   \n",
       "171982          0            0        0         0          0       0        0   \n",
       "165248          0            0        0         0          0       0        0   \n",
       "131592          0            0        0         0          0       0        1   \n",
       "51536           0            0        0         0          0       0        0   \n",
       "50047           0            0        0         0          0       0        0   \n",
       "61892           0            0        0         0          0       0        1   \n",
       "26410           0            0        0         0          0       0        0   \n",
       "77553           0            0        0         0          0       0        0   \n",
       "22965           0            0        0         0          0       0        0   \n",
       "168614          0            0        0         0          0       0        0   \n",
       "135608          0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "67296          0          0        0       0         0          0        0   \n",
       "34025          0          0        0       0         1          0        0   \n",
       "150524         0          0        0       0         0          0        0   \n",
       "122952         0          0        0       0         0          0        0   \n",
       "150510         0          0        0       0         0          0        0   \n",
       "64728          0          0        0       0         0          0        0   \n",
       "24524          0          0        0       0         0          0        0   \n",
       "172023         0          0        0       0         0          0        0   \n",
       "55279          0          0        0       0         0          0        0   \n",
       "171982         0          0        0       0         0          0        0   \n",
       "165248         0          0        0       0         0          0        0   \n",
       "131592         0          0        0       0         0          0        0   \n",
       "51536          0          0        0       0         0          0        0   \n",
       "50047          0          0        0       0         0          0        0   \n",
       "61892          0          0        0       0         0          0        0   \n",
       "26410          0          0        0       0         0          0        0   \n",
       "77553          0          0        0       0         0          0        0   \n",
       "22965          0          0        0       0         0          0        0   \n",
       "168614         0          0        0       0         0          0        0   \n",
       "135608         0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "67296            0         0        0        0       0           0         0   \n",
       "34025            0         0        0        0       0           0         0   \n",
       "150524           0         0        1        0       0           1         0   \n",
       "122952           0         0        0        0       0           0         0   \n",
       "150510           0         0        0        0       0           0         0   \n",
       "64728            0         0        0        0       0           0         0   \n",
       "24524            0         0        1        0       0           0         0   \n",
       "172023           0         0        0        0       0           0         0   \n",
       "55279            0         0        0        0       0           0         0   \n",
       "171982           0         0        0        0       0           0         0   \n",
       "165248           0         0        0        0       0           0         0   \n",
       "131592           0         0        0        0       0           0         0   \n",
       "51536            0         0        0        0       0           0         0   \n",
       "50047            0         0        0        0       0           0         0   \n",
       "61892            0         0        0        0       0           0         0   \n",
       "26410            0         0        0        0       0           0         0   \n",
       "77553            0         0        0        0       0           0         0   \n",
       "22965            0         0        0        0       0           0         0   \n",
       "168614           0         0        0        0       0           0         0   \n",
       "135608           0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text  \n",
       "67296        1            1  \n",
       "34025        1            1  \n",
       "150524       1            1  \n",
       "122952       1            1  \n",
       "150510       1            1  \n",
       "64728        1            1  \n",
       "24524        1            1  \n",
       "172023       1            1  \n",
       "55279        1            1  \n",
       "171982       1            1  \n",
       "165248       1            1  \n",
       "131592       1            1  \n",
       "51536        1            1  \n",
       "50047        1            1  \n",
       "61892        1            1  \n",
       "26410        1            1  \n",
       "77553        1            1  \n",
       "22965        1            1  \n",
       "168614       1            1  \n",
       "135608       1            1  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['insulin_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a3ff1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33794, '1': 385})\n"
     ]
    }
   ],
   "source": [
    "#### DM RETINOPATHIES / retina\n",
    "\n",
    "## text\n",
    "spec['retina_text'] = np.where(groups['text'].str.contains('diabetic retin'), \"1\", \"0\")\n",
    "\n",
    "spec['retina_text'] = np.where((groups['text'].str.contains(\"diabet\")) &\n",
    "                             (groups['text'].str.contains(\"retina\")) , \"1\", spec['retina_text'])\n",
    "spec['retina_text'] = np.where((groups['text'].str.contains(\"diabet\")) &\n",
    "                             (groups['text'].str.contains(\"retino\")) , \"1\", spec['retina_text'])\n",
    "spec['retina_text'] = np.where((groups['text'].str.contains(\"diabet\")) &\n",
    "                             (groups['text'].str.contains(\"eye\")) , \"1\", spec['retina_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['retina_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f408a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54347</th>\n",
       "      <td>automatic detection of rare pathologies in fundus photographs using few-shot learning in the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy dr screening networks through deep learning, these datasets were used to train automatic detectors for dr and a few other frequent pathologies, with the goal to automate screening one challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy the reason is that standard deep learning requires too many examples of these conditions however, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category this paper presents a new few-shot learning framework that extends convolutional neural networks cnns, trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection it is based on the observation that cnns often perceive photographs containing the same anomalies as similar, even though these cnns were trained to detect unrelated conditions this observation was based on the t-sne visualization tool, which we decided to incorporate in our probabilistic model experiments on a dataset of 164,660 screening examinations from the ophdiat screening network show that 37 conditions, out of 41, can be detected with an area under the roc curve auc greater than 08 average auc: 0938 in particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and siamese networks, another few-shot learning solution we expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23793</th>\n",
       "      <td>precise higher-order reflectivity and morphology models for early diagnosis of diabetic retinopathy using oct images this study proposes a novel computer assisted diagnostic cad system for early diagnosis of diabetic retinopathy dr using optical coherence tomography oct b-scans the cad system is based on fusing novel oct markers that describe both the morphology/anatomy and the reflectivity of retinal layers to improve dr diagnosis this system separates retinal layers automatically using a segmentation approach based on an adaptive appearance and their prior shape information high-order morphological and novel reflectivity markers are extracted from individual segmented layers namely, the morphological markers are layer thickness and tortuosity while the reflectivity markers are the 1st-order reflectivity of the layer in addition to local and global high-order reflectivity based on markov-gibbs random field mgrf and gray-level co-occurrence matrix glcm, respectively the extracted image-derived markers are represented using cumulative distribution function cdf descriptors the constructed cdfs are then described using their statistical measures, ie, the 10th through 90th percentiles with a 10% increment for individual layer classification, each extracted descriptor of a given layer is fed to a support vector machine svm classifier with a linear kernel the results of the four classifiers are then fused using a backpropagation neural network bnn to diagnose each retinal layer for global subject diagnosis, classification outputs probabilities of the twelve layers are fused using another bnn to make the final diagnosis of the b-scan this system is validated and tested on 130 patients, with two scans for both eyes ie 260 oct images, with a balanced number of normal and dr subjects using different validation metrics: 2-folds, 4-folds, 10-folds, and leave-one-subject-out loso cross-validation approaches the performance of the proposed system was evaluated using sensitivity, specificity, f1-score, and accuracy metrics the systems performance after the fusion of these different markers showed better performance compared with individual markers and other machine learning fusion methods namely, it achieved formula: see text, formula: see text, formula: see text, and formula: see text, respectively, using the loso cross-validation technique the reported results, based on the integration of morphology and reflectivity markers and by using state-of-the-art machine learning classifications, demonstrate the ability of the proposed system to diagnose the dr early</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74208</th>\n",
       "      <td>diagnostic accuracy of a device for the automated detection of diabetic retinopathy in a primary care setting to determine the diagnostic accuracy in a real-world primary care setting of a deep learning-enhanced device for automated detection of diabetic retinopathy dr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137086</th>\n",
       "      <td>active learning for an efficient training strategy of computer-aided diagnosis systems: application to diabetic retinopathy screening the performance of computer-aided diagnosis cad systems can be highly influenced by the training strategy cad systems are traditionally trained using available labeled data, extracted from a specific data distribution or from public databases due to the wide variability of medical data, these databases might not be representative enough when the cad system is applied to data extracted from a different clinical setting, diminishing the performance or requiring more labeled samples in order to get better data generalization in this work, we propose the incorporation of an active learning approach in the training phase of cad systems for reducing the number of required training samples while maximizing the system performance the benefit of this approach has been evaluated using a specific cad system for diabetic retinopathy screening the results show that 1 using a training set obtained from a different data source results in a considerable reduction of the cad performance; and 2 using active learning the selected training set can be reduced from 1000 to 200 samples while maintaining an area under the receiver operating characteristic curve of 0856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69238</th>\n",
       "      <td>topological data analysis of high resolution diabetic retinopathy images diabetic retinopathy is a complication of diabetes that produces changes in the blood vessel structure in the retina, which can cause severe vision problems and even blindness in this paper, we demonstrate that by identifying topological features in very high resolution retinal images, we can construct a classifier that discriminates between healthy patients and those with diabetic retinopathy using summary statistics of these features topological data analysis identifies the features as connected components and holes in the images and describes the extent to which they persist across the image these features are encoded in persistence diagrams, summaries of which can be used to discrimate between diabetic and healthy patients the method has the potential to be an effective automated screening tool, with high sensitivity and specificity</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67277</th>\n",
       "      <td>microaneurysms segmentation with a u-net based on recurrent residual convolutional neural network microaneurysms mas play an important role in the diagnosis of clinical diabetic retinopathy at the early stage annotation of mas manually by experts is laborious and so it is essential to develop automatic segmentation methods automatic ma segmentation remains a challenging task mainly due to the low local contrast of the image and the small size of mas a deep learning-based method called u-net has become one of the most popular methods for the medical image segmentation task we propose an architecture for u-net, named deep recurrent u-net dru-net, obtained by combining the deep residual model and recurrent convolutional operations into u-net in the ma segmentation task, dru-net can accumulate effective features much better than the typical u-net the proposed method is evaluated on two publicly available datasets: e-ophtha and idrid our results show that the proposed dru-net achieves the best performance with 09999 accuracy value and 09943 area under curve auc value on the e-ophtha dataset and on the idrid dataset, it has achieved 0987 auc value to our knowledge, this is the first result of segmenting mas on this dataset compared with other methods, such as u-net, fcnn, and resu-net, our architecture dru-net achieves state-of-the-art performance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64727</th>\n",
       "      <td>classification of diabetes-related retinal diseases using a deep learning approach in optical coherence tomography spectral domain optical coherence tomography sd-oct is a volumetric imaging technique that allows measuring patterns between layers such as small amounts of fluid since 2012, automatic medical image analysis performance has steadily increased through the use of deep learning models that automatically learn relevant features for specific tasks, instead of designing visual features manually nevertheless, providing insights and interpretation of the predictions made by the model is still a challenge this paper describes a deep learning model able to detect medically interpretable information in relevant images from a volume to classify diabetes-related retinal diseases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14171</th>\n",
       "      <td>analysis and comparison of two artificial intelligence diabetic retinopathy screening algorithms in a pilot study: idx-dr and retinalyze the prevalence of diabetic retinopathy dr is expected to increase this will put an increasing strain on health care resources recently, artificial intelligence-based, autonomous dr screening systems have been developed a direct comparison between different systems is often difficult and only two such comparisons have been published so far as different screening solutions are now available commercially, with more in the pipeline, choosing a system is not a simple matter based on the images gathered in a local dr screening program we performed a retrospective comparison of idx-dr and retinalyze</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67936</th>\n",
       "      <td>supervised machine learning based multi-task artificial intelligence classification of retinopathies artificial intelligence ai classification holds promise as a novel and affordable screening tool for clinical management of ocular diseases rural and underserved areas, which suffer from lack of access to experienced ophthalmologists may particularly benefit from this technology quantitative optical coherence tomography angiography octa imaging provides excellent capability to identify subtle vascular distortions, which are useful for classifying retinovascular diseases however, application of ai for differentiation and classification of multiple eye diseases is not yet established in this study, we demonstrate supervised machine learning based multi-task octa classification we sought 1 to differentiate normal from diseased ocular conditions, 2 to differentiate different ocular disease conditions from each other, and 3 to stage the severity of each ocular condition quantitative octa features, including blood vessel tortuosity bvt, blood vascular caliber bvc, vessel perimeter index vpi, blood vessel density bvd, foveal avascular zone faz area faz-a, and faz contour irregularity faz-ci were fully automatically extracted from the octa images a stepwise backward elimination approach was employed to identify sensitive octa features and optimal-feature-combinations for the multi-task classification for proof-of-concept demonstration, diabetic retinopathy dr and sickle cell retinopathy scr were used to validate the supervised machine leaning classifier the presented ai classification methodology is applicable and can be readily extended to other ocular diseases, holding promise to enable a mass-screening platform for clinical deployment and telemedicine</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105801</th>\n",
       "      <td>computer-assisted identification of proliferative diabetic retinopathy in color retinal images advanced proliferative stage of diabetic retinopathy dr is indicated by the growth of thin, fragile and highly unregulated vessels, neovascularization nv in order to identify proliferative diabetic retinopathy pdr, our approach models the micro-pattern of local variations using texture based analysis and quantifies the structural changes in vessel patterns in localized patches, to map them to the confidence score of being neovascular using supervised learning framework rule-based criteria on patch-level neovascularity scores in an image is used for the decision of absence or presence of pdr evaluated using 3 datasets, our method achieves 96% sensitivity and 926% specificity for localizing nv image-level identification of pdr achieves high sensitivity of 9672% at 796% specificity and high specificity of 9650% at 7322% sensitivity our approach could have potential application in dr grading where it can localize nve regions and identify pdr images for immediate intervention</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87247</th>\n",
       "      <td>microaneurysm detection using fully convolutional neural networks diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough microaneurysms are the earliest clinical signs of diabetic retinopathy this paper presents an automatic method for detecting microaneurysms in fundus photographies</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114910</th>\n",
       "      <td>dream: diabetic retinopathy analysis using machine learning this paper presents a computer-aided screening system dream that analyzes fundus images with varying illumination and fields of view, and generates a severity grade for diabetic retinopathy dr using machine learning classifiers such as the gaussian mixture model gmm, k-nearest neighbor knn, support vector machine svm, and adaboost are analyzed for classifying retinopathy lesions from nonlesions gmm and knn classifiers are found to be the best classifiers for bright and red lesion classification, respectively a main contribution of this paper is the reduction in the number of features used for lesion classification by feature ranking using adaboost where 30 top features are selected out of 78 a novel two-step hierarchical classification approach is proposed where the nonlesions or false positives are rejected in the first step in the second step, the bright lesions are classified as hard exudates and cotton wool spots, and the red lesions are classified as hemorrhages and micro-aneurysms this lesion classification problem deals with unbalanced datasets and svm or combination classifiers derived from svm using the dempster-shafer theory are found to incur more classification error than the gmm and knn classifiers due to the data imbalance the dr severity grading system is tested on 1200 images from the publicly available messidor dataset the dream system achieves 100% sensitivity, 5316% specificity, and 0904 auc, compared to the best reported 96% sensitivity, 51% specificity, and 0875 auc, for classifying images as with or without dr the feature reduction further reduces the average computation time for dr severity per image from 5954 to 346 s</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22692</th>\n",
       "      <td>deep learning-based diabetic retinopathy severity grading system employing quadrant ensemble model the diabetic retinopathy accounts in the deterioration of retinal blood vessels leading to a serious compilation affecting the eyes the automated dr diagnosis frameworks are critically important for the early identification and detection of these eye-related problems, helping the ophthalmic experts in providing the second opinion for effectual treatment the deep learning techniques have evolved as an improvement over the conventional approaches, which are dependent on the handcrafted feature extraction to address the issue of proficient dr discrimination, the authors have proposed a quadrant ensemble automated dr grading approach by implementing inceptionresnet-v2 deep neural network framework the presented model incorporates histogram equalization, optical disc localization, and quadrant cropping along with the data augmentation step for improving the network performance a superior accuracy performance of 9333% is observed for the proposed framework, and a significant reduction of 0325 is noticed in the cross-entropy loss function for messidor benchmark dataset; however, its validation utilizing the latest idrid dataset establishes its generalization ability the accuracy improvement of 1358% is observed when the proposed qeirv-2 model is compared with the classical inception-v3 cnn model to justify the viability of the proposed framework, its performance is compared with the existing state-of-the-art approaches and 2523% of accuracy improvement is observed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91414</th>\n",
       "      <td>exudate detection for diabetic retinopathy with convolutional neural networks exudate detection is an essential task for computer-aid diagnosis of diabetic retinopathy dr, so as to monitor the progress of dr in this paper, deep convolutional neural network cnn is adopted to achieve pixel-wise exudate identification the cnn model is first trained with expert labeled exudates image patches and then saved as off-line classifier in order to achieve pixel-level accuracy meanwhile reduce computational time, potential exudate candidate points are first extracted with morphological ultimate opening algorithm then the local region 64 × 64 surrounding the candidate points are forwarded to the trained cnn model for classification/identification a pixel-wise accuracy of 9192%, sensitivity of 8885% and specificity of 96% is achieved with the proposed cnn architecture on the test database</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12962</th>\n",
       "      <td>explain: explanatory artificial intelligence for diabetic retinopathy diagnosis in recent years, artificial intelligence ai has proven its relevance for medical decision support however, the black-box nature of successful ai algorithms still holds back their wide-spread deployment in this paper, we describe an explanatory artificial intelligence xai that reaches the same level of performance as black-box ai, for the task of classifying diabetic retinopathy dr severity using color fundus photography cfp this algorithm, called explain, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations the novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box ai algorithms: the concepts of lesions and lesion categories emerge by themselves for improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image the advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences explain is evaluated at the image level and at the pixel level on various cfp image datasets we expect this new framework, which jointly offers high classification performance and explainability, to facilitate ai deployment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101242</th>\n",
       "      <td>a risk based neural network approach for predictive modeling of blood glucose dynamics for type 1 diabetes patients, maintaining the blood glucose bg at normal values is a challenging task due to eg variable insulin reactions, diets, lifestyles, emotional conditions, etc hyperglycemic and hypoglycemic events can generate various complications eg diabetic ketoacidosis, retinopathy, neuropathy, etc, so predicting bg values in time is of great importance for diabetes self-management herein, we propose a non-linear autoregressive neural network approach, based on the minimal dataset available from a continuous glucose monitoring cgm sensor, with an integrated measure of intra-patient bg variability the method kept the balance between accuracy and complexity, allowing a fast response with no additional effort or discomfort for the patient</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55728</th>\n",
       "      <td>automatic parallel detection of neovascularization from retinal images using ensemble of extreme learning machine&lt;sup&gt;&lt;/sup&gt; retinopathy screening is a non-invasive method to collect retinal images and neovascularization detection from retinal images plays a significant role on the identification and classification of diabetes retinopathy in this paper, an automatic parallel detection framework for neovascularization with color retinal images using ensemble of extreme learning machine is proposed the framework employs two map-reduce jobs to extract features and trains extreme learning machine models ensemble methods such as bagging, subspace partitioning and cross validating are used to increase the accuracy the framework is evaluated with retinal images from messidor database experimental results show the framework can improve the detection accuracy, as well as speedup the processing time to 22 times on average</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85890</th>\n",
       "      <td>automated quality assessment of colour fundus images for diabetic retinopathy screening in telemedicine fundus images obtained in a telemedicine program are acquired at different sites that are captured by people who have varying levels of experience these result in a relatively high percentage of images which are later marked as unreadable by graders unreadable images require a recapture which is time and cost intensive an automated method that determines the image quality during acquisition is an effective alternative to determine the image quality during acquisition, we describe here an automated method for the assessment of image quality in the context of diabetic retinopathy the method explicitly applies machine learning techniques to access the image and to determine acceptand rejectcategories rejectcategory image requires a recapture a deep convolution neural network is trained to grade the images automatically a large representative set of 7000 colour fundus images was used for the experiment which was obtained from the eyepacs that were made available by the california healthcare foundation three retinal image analysis experts were employed to categorise these images into acceptand rejectclasses based on the precise definition of image quality in the context of dr the network was trained using 3428 images the method shows an accuracy of 100% to successfully categorise acceptand rejectimages, which is about 2% higher than the traditional machine learning method on a clinical trial, the proposed method shows 97% agreement with human grader the method can be easily incorporated with the fundus image capturing system in the acquisition centre and can guide the photographer whether a recapture is necessary or not</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122998</th>\n",
       "      <td>the reading of components of diabetic retinopathy: an evolutionary approach for filtering normal digital fundus imaging in screening and population based studies in any diabetic retinopathy screening program, about two-thirds of patients have no retinopathy however, on average, it takes a human expert about one and a half times longer to decide an image is normal than to recognize an abnormal case with obvious features in this work, we present an automated system for filtering out normal cases to facilitate a more effective use of grading time the key aim with any such tool is to achieve high sensitivity and specificity to ensure patientssafety and service efficiency there are many challenges to overcome, given the variation of images and characteristics to identify the system combines computed evidence obtained from various processing stages, including segmentation of candidate regions, classification and contextual analysis through hidden markov models furthermore, evolutionary algorithms are employed to optimize the hidden markov models, feature selection and heterogeneous ensemble classifiers in order to evaluate its capability of identifying normal images across diverse populations, a population-oriented study was undertaken comparing the softwares output to grading by humans in addition, population based studies collect large numbers of images on subjects expected to have no abnormality these studies expect timely and cost-effective grading altogether 9954 previously unseen images taken from various populations were tested all test images were masked so the automated system had not been exposed to them before this system was trained using image subregions taken from about 400 sample images sensitivities of 922% and specificities of 904% were achieved varying between populations and population clusters of all images the automated system decided to be normal, 982% were true normal when compared to the manual grading results these results demonstrate scalability and strong potential of such an integrated computational intelligence system as an effective tool to assist a grading service</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42078</th>\n",
       "      <td>from local to global: a graph framework for retinal artery/vein classification fundus photography has been widely used for inspecting eye disorders by ophthalmologists or computer algorithms biomarkers related to retinal vessels plays an essential role to detect early diabetes to quantify vascular biomarkers or the corresponding changes, an accurate artery and vein classification is necessary in this work, we propose a new framework to boost local vessel classification with a global vascular network model using graph convolution we compare our proposed method with two traditional state-of-the-art methods on a testing dataset of 750 images from the maastricht study after incorporating global information, our model achieves the best accuracy of 8645% compared to 855% from convolutional neural networks cnn and 829% from handcrafted pixel feature classification hpfc our model also obtains the best area under receiver operating characteristic curve auc of 095, compared to 093 from cnn and 090 from hpfc the new classification framework has the advantage of easy deployment on top of local classification features it corrects the local classification error by minimizing global classification error and it brings free additional classification performance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "54347                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               automatic detection of rare pathologies in fundus photographs using few-shot learning in the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy dr screening networks through deep learning, these datasets were used to train automatic detectors for dr and a few other frequent pathologies, with the goal to automate screening one challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy the reason is that standard deep learning requires too many examples of these conditions however, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category this paper presents a new few-shot learning framework that extends convolutional neural networks cnns, trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection it is based on the observation that cnns often perceive photographs containing the same anomalies as similar, even though these cnns were trained to detect unrelated conditions this observation was based on the t-sne visualization tool, which we decided to incorporate in our probabilistic model experiments on a dataset of 164,660 screening examinations from the ophdiat screening network show that 37 conditions, out of 41, can be detected with an area under the roc curve auc greater than 08 average auc: 0938 in particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and siamese networks, another few-shot learning solution we expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology   \n",
       "23793   precise higher-order reflectivity and morphology models for early diagnosis of diabetic retinopathy using oct images this study proposes a novel computer assisted diagnostic cad system for early diagnosis of diabetic retinopathy dr using optical coherence tomography oct b-scans the cad system is based on fusing novel oct markers that describe both the morphology/anatomy and the reflectivity of retinal layers to improve dr diagnosis this system separates retinal layers automatically using a segmentation approach based on an adaptive appearance and their prior shape information high-order morphological and novel reflectivity markers are extracted from individual segmented layers namely, the morphological markers are layer thickness and tortuosity while the reflectivity markers are the 1st-order reflectivity of the layer in addition to local and global high-order reflectivity based on markov-gibbs random field mgrf and gray-level co-occurrence matrix glcm, respectively the extracted image-derived markers are represented using cumulative distribution function cdf descriptors the constructed cdfs are then described using their statistical measures, ie, the 10th through 90th percentiles with a 10% increment for individual layer classification, each extracted descriptor of a given layer is fed to a support vector machine svm classifier with a linear kernel the results of the four classifiers are then fused using a backpropagation neural network bnn to diagnose each retinal layer for global subject diagnosis, classification outputs probabilities of the twelve layers are fused using another bnn to make the final diagnosis of the b-scan this system is validated and tested on 130 patients, with two scans for both eyes ie 260 oct images, with a balanced number of normal and dr subjects using different validation metrics: 2-folds, 4-folds, 10-folds, and leave-one-subject-out loso cross-validation approaches the performance of the proposed system was evaluated using sensitivity, specificity, f1-score, and accuracy metrics the systems performance after the fusion of these different markers showed better performance compared with individual markers and other machine learning fusion methods namely, it achieved formula: see text, formula: see text, formula: see text, and formula: see text, respectively, using the loso cross-validation technique the reported results, based on the integration of morphology and reflectivity markers and by using state-of-the-art machine learning classifications, demonstrate the ability of the proposed system to diagnose the dr early   \n",
       "74208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    diagnostic accuracy of a device for the automated detection of diabetic retinopathy in a primary care setting to determine the diagnostic accuracy in a real-world primary care setting of a deep learning-enhanced device for automated detection of diabetic retinopathy dr   \n",
       "137086                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               active learning for an efficient training strategy of computer-aided diagnosis systems: application to diabetic retinopathy screening the performance of computer-aided diagnosis cad systems can be highly influenced by the training strategy cad systems are traditionally trained using available labeled data, extracted from a specific data distribution or from public databases due to the wide variability of medical data, these databases might not be representative enough when the cad system is applied to data extracted from a different clinical setting, diminishing the performance or requiring more labeled samples in order to get better data generalization in this work, we propose the incorporation of an active learning approach in the training phase of cad systems for reducing the number of required training samples while maximizing the system performance the benefit of this approach has been evaluated using a specific cad system for diabetic retinopathy screening the results show that 1 using a training set obtained from a different data source results in a considerable reduction of the cad performance; and 2 using active learning the selected training set can be reduced from 1000 to 200 samples while maintaining an area under the receiver operating characteristic curve of 0856   \n",
       "69238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        topological data analysis of high resolution diabetic retinopathy images diabetic retinopathy is a complication of diabetes that produces changes in the blood vessel structure in the retina, which can cause severe vision problems and even blindness in this paper, we demonstrate that by identifying topological features in very high resolution retinal images, we can construct a classifier that discriminates between healthy patients and those with diabetic retinopathy using summary statistics of these features topological data analysis identifies the features as connected components and holes in the images and describes the extent to which they persist across the image these features are encoded in persistence diagrams, summaries of which can be used to discrimate between diabetic and healthy patients the method has the potential to be an effective automated screening tool, with high sensitivity and specificity   \n",
       "67277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              microaneurysms segmentation with a u-net based on recurrent residual convolutional neural network microaneurysms mas play an important role in the diagnosis of clinical diabetic retinopathy at the early stage annotation of mas manually by experts is laborious and so it is essential to develop automatic segmentation methods automatic ma segmentation remains a challenging task mainly due to the low local contrast of the image and the small size of mas a deep learning-based method called u-net has become one of the most popular methods for the medical image segmentation task we propose an architecture for u-net, named deep recurrent u-net dru-net, obtained by combining the deep residual model and recurrent convolutional operations into u-net in the ma segmentation task, dru-net can accumulate effective features much better than the typical u-net the proposed method is evaluated on two publicly available datasets: e-ophtha and idrid our results show that the proposed dru-net achieves the best performance with 09999 accuracy value and 09943 area under curve auc value on the e-ophtha dataset and on the idrid dataset, it has achieved 0987 auc value to our knowledge, this is the first result of segmenting mas on this dataset compared with other methods, such as u-net, fcnn, and resu-net, our architecture dru-net achieves state-of-the-art performance   \n",
       "64727                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            classification of diabetes-related retinal diseases using a deep learning approach in optical coherence tomography spectral domain optical coherence tomography sd-oct is a volumetric imaging technique that allows measuring patterns between layers such as small amounts of fluid since 2012, automatic medical image analysis performance has steadily increased through the use of deep learning models that automatically learn relevant features for specific tasks, instead of designing visual features manually nevertheless, providing insights and interpretation of the predictions made by the model is still a challenge this paper describes a deep learning model able to detect medically interpretable information in relevant images from a volume to classify diabetes-related retinal diseases   \n",
       "14171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 analysis and comparison of two artificial intelligence diabetic retinopathy screening algorithms in a pilot study: idx-dr and retinalyze the prevalence of diabetic retinopathy dr is expected to increase this will put an increasing strain on health care resources recently, artificial intelligence-based, autonomous dr screening systems have been developed a direct comparison between different systems is often difficult and only two such comparisons have been published so far as different screening solutions are now available commercially, with more in the pipeline, choosing a system is not a simple matter based on the images gathered in a local dr screening program we performed a retrospective comparison of idx-dr and retinalyze   \n",
       "67936                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  supervised machine learning based multi-task artificial intelligence classification of retinopathies artificial intelligence ai classification holds promise as a novel and affordable screening tool for clinical management of ocular diseases rural and underserved areas, which suffer from lack of access to experienced ophthalmologists may particularly benefit from this technology quantitative optical coherence tomography angiography octa imaging provides excellent capability to identify subtle vascular distortions, which are useful for classifying retinovascular diseases however, application of ai for differentiation and classification of multiple eye diseases is not yet established in this study, we demonstrate supervised machine learning based multi-task octa classification we sought 1 to differentiate normal from diseased ocular conditions, 2 to differentiate different ocular disease conditions from each other, and 3 to stage the severity of each ocular condition quantitative octa features, including blood vessel tortuosity bvt, blood vascular caliber bvc, vessel perimeter index vpi, blood vessel density bvd, foveal avascular zone faz area faz-a, and faz contour irregularity faz-ci were fully automatically extracted from the octa images a stepwise backward elimination approach was employed to identify sensitive octa features and optimal-feature-combinations for the multi-task classification for proof-of-concept demonstration, diabetic retinopathy dr and sickle cell retinopathy scr were used to validate the supervised machine leaning classifier the presented ai classification methodology is applicable and can be readily extended to other ocular diseases, holding promise to enable a mass-screening platform for clinical deployment and telemedicine   \n",
       "105801                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        computer-assisted identification of proliferative diabetic retinopathy in color retinal images advanced proliferative stage of diabetic retinopathy dr is indicated by the growth of thin, fragile and highly unregulated vessels, neovascularization nv in order to identify proliferative diabetic retinopathy pdr, our approach models the micro-pattern of local variations using texture based analysis and quantifies the structural changes in vessel patterns in localized patches, to map them to the confidence score of being neovascular using supervised learning framework rule-based criteria on patch-level neovascularity scores in an image is used for the decision of absence or presence of pdr evaluated using 3 datasets, our method achieves 96% sensitivity and 926% specificity for localizing nv image-level identification of pdr achieves high sensitivity of 9672% at 796% specificity and high specificity of 9650% at 7322% sensitivity our approach could have potential application in dr grading where it can localize nve regions and identify pdr images for immediate intervention   \n",
       "87247                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   microaneurysm detection using fully convolutional neural networks diabetic retinopathy is a microvascular complication of diabetes that can lead to sight loss if treated not early enough microaneurysms are the earliest clinical signs of diabetic retinopathy this paper presents an automatic method for detecting microaneurysms in fundus photographies   \n",
       "114910                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               dream: diabetic retinopathy analysis using machine learning this paper presents a computer-aided screening system dream that analyzes fundus images with varying illumination and fields of view, and generates a severity grade for diabetic retinopathy dr using machine learning classifiers such as the gaussian mixture model gmm, k-nearest neighbor knn, support vector machine svm, and adaboost are analyzed for classifying retinopathy lesions from nonlesions gmm and knn classifiers are found to be the best classifiers for bright and red lesion classification, respectively a main contribution of this paper is the reduction in the number of features used for lesion classification by feature ranking using adaboost where 30 top features are selected out of 78 a novel two-step hierarchical classification approach is proposed where the nonlesions or false positives are rejected in the first step in the second step, the bright lesions are classified as hard exudates and cotton wool spots, and the red lesions are classified as hemorrhages and micro-aneurysms this lesion classification problem deals with unbalanced datasets and svm or combination classifiers derived from svm using the dempster-shafer theory are found to incur more classification error than the gmm and knn classifiers due to the data imbalance the dr severity grading system is tested on 1200 images from the publicly available messidor dataset the dream system achieves 100% sensitivity, 5316% specificity, and 0904 auc, compared to the best reported 96% sensitivity, 51% specificity, and 0875 auc, for classifying images as with or without dr the feature reduction further reduces the average computation time for dr severity per image from 5954 to 346 s   \n",
       "22692                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     deep learning-based diabetic retinopathy severity grading system employing quadrant ensemble model the diabetic retinopathy accounts in the deterioration of retinal blood vessels leading to a serious compilation affecting the eyes the automated dr diagnosis frameworks are critically important for the early identification and detection of these eye-related problems, helping the ophthalmic experts in providing the second opinion for effectual treatment the deep learning techniques have evolved as an improvement over the conventional approaches, which are dependent on the handcrafted feature extraction to address the issue of proficient dr discrimination, the authors have proposed a quadrant ensemble automated dr grading approach by implementing inceptionresnet-v2 deep neural network framework the presented model incorporates histogram equalization, optical disc localization, and quadrant cropping along with the data augmentation step for improving the network performance a superior accuracy performance of 9333% is observed for the proposed framework, and a significant reduction of 0325 is noticed in the cross-entropy loss function for messidor benchmark dataset; however, its validation utilizing the latest idrid dataset establishes its generalization ability the accuracy improvement of 1358% is observed when the proposed qeirv-2 model is compared with the classical inception-v3 cnn model to justify the viability of the proposed framework, its performance is compared with the existing state-of-the-art approaches and 2523% of accuracy improvement is observed   \n",
       "91414                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          exudate detection for diabetic retinopathy with convolutional neural networks exudate detection is an essential task for computer-aid diagnosis of diabetic retinopathy dr, so as to monitor the progress of dr in this paper, deep convolutional neural network cnn is adopted to achieve pixel-wise exudate identification the cnn model is first trained with expert labeled exudates image patches and then saved as off-line classifier in order to achieve pixel-level accuracy meanwhile reduce computational time, potential exudate candidate points are first extracted with morphological ultimate opening algorithm then the local region 64 × 64 surrounding the candidate points are forwarded to the trained cnn model for classification/identification a pixel-wise accuracy of 9192%, sensitivity of 8885% and specificity of 96% is achieved with the proposed cnn architecture on the test database   \n",
       "12962                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               explain: explanatory artificial intelligence for diabetic retinopathy diagnosis in recent years, artificial intelligence ai has proven its relevance for medical decision support however, the black-box nature of successful ai algorithms still holds back their wide-spread deployment in this paper, we describe an explanatory artificial intelligence xai that reaches the same level of performance as black-box ai, for the task of classifying diabetic retinopathy dr severity using color fundus photography cfp this algorithm, called explain, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations the novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box ai algorithms: the concepts of lesions and lesion categories emerge by themselves for improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image the advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences explain is evaluated at the image level and at the pixel level on various cfp image datasets we expect this new framework, which jointly offers high classification performance and explainability, to facilitate ai deployment   \n",
       "101242                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  a risk based neural network approach for predictive modeling of blood glucose dynamics for type 1 diabetes patients, maintaining the blood glucose bg at normal values is a challenging task due to eg variable insulin reactions, diets, lifestyles, emotional conditions, etc hyperglycemic and hypoglycemic events can generate various complications eg diabetic ketoacidosis, retinopathy, neuropathy, etc, so predicting bg values in time is of great importance for diabetes self-management herein, we propose a non-linear autoregressive neural network approach, based on the minimal dataset available from a continuous glucose monitoring cgm sensor, with an integrated measure of intra-patient bg variability the method kept the balance between accuracy and complexity, allowing a fast response with no additional effort or discomfort for the patient    \n",
       "55728                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    automatic parallel detection of neovascularization from retinal images using ensemble of extreme learning machine<sup></sup> retinopathy screening is a non-invasive method to collect retinal images and neovascularization detection from retinal images plays a significant role on the identification and classification of diabetes retinopathy in this paper, an automatic parallel detection framework for neovascularization with color retinal images using ensemble of extreme learning machine is proposed the framework employs two map-reduce jobs to extract features and trains extreme learning machine models ensemble methods such as bagging, subspace partitioning and cross validating are used to increase the accuracy the framework is evaluated with retinal images from messidor database experimental results show the framework can improve the detection accuracy, as well as speedup the processing time to 22 times on average   \n",
       "85890                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               automated quality assessment of colour fundus images for diabetic retinopathy screening in telemedicine fundus images obtained in a telemedicine program are acquired at different sites that are captured by people who have varying levels of experience these result in a relatively high percentage of images which are later marked as unreadable by graders unreadable images require a recapture which is time and cost intensive an automated method that determines the image quality during acquisition is an effective alternative to determine the image quality during acquisition, we describe here an automated method for the assessment of image quality in the context of diabetic retinopathy the method explicitly applies machine learning techniques to access the image and to determine acceptand rejectcategories rejectcategory image requires a recapture a deep convolution neural network is trained to grade the images automatically a large representative set of 7000 colour fundus images was used for the experiment which was obtained from the eyepacs that were made available by the california healthcare foundation three retinal image analysis experts were employed to categorise these images into acceptand rejectclasses based on the precise definition of image quality in the context of dr the network was trained using 3428 images the method shows an accuracy of 100% to successfully categorise acceptand rejectimages, which is about 2% higher than the traditional machine learning method on a clinical trial, the proposed method shows 97% agreement with human grader the method can be easily incorporated with the fundus image capturing system in the acquisition centre and can guide the photographer whether a recapture is necessary or not   \n",
       "122998                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  the reading of components of diabetic retinopathy: an evolutionary approach for filtering normal digital fundus imaging in screening and population based studies in any diabetic retinopathy screening program, about two-thirds of patients have no retinopathy however, on average, it takes a human expert about one and a half times longer to decide an image is normal than to recognize an abnormal case with obvious features in this work, we present an automated system for filtering out normal cases to facilitate a more effective use of grading time the key aim with any such tool is to achieve high sensitivity and specificity to ensure patientssafety and service efficiency there are many challenges to overcome, given the variation of images and characteristics to identify the system combines computed evidence obtained from various processing stages, including segmentation of candidate regions, classification and contextual analysis through hidden markov models furthermore, evolutionary algorithms are employed to optimize the hidden markov models, feature selection and heterogeneous ensemble classifiers in order to evaluate its capability of identifying normal images across diverse populations, a population-oriented study was undertaken comparing the softwares output to grading by humans in addition, population based studies collect large numbers of images on subjects expected to have no abnormality these studies expect timely and cost-effective grading altogether 9954 previously unseen images taken from various populations were tested all test images were masked so the automated system had not been exposed to them before this system was trained using image subregions taken from about 400 sample images sensitivities of 922% and specificities of 904% were achieved varying between populations and population clusters of all images the automated system decided to be normal, 982% were true normal when compared to the manual grading results these results demonstrate scalability and strong potential of such an integrated computational intelligence system as an effective tool to assist a grading service    \n",
       "42078                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 from local to global: a graph framework for retinal artery/vein classification fundus photography has been widely used for inspecting eye disorders by ophthalmologists or computer algorithms biomarkers related to retinal vessels plays an essential role to detect early diabetes to quantify vascular biomarkers or the corresponding changes, an accurate artery and vein classification is necessary in this work, we propose a new framework to boost local vessel classification with a global vascular network model using graph convolution we compare our proposed method with two traditional state-of-the-art methods on a testing dataset of 750 images from the maastricht study after incorporating global information, our model achieves the best accuracy of 8645% compared to 855% from convolutional neural networks cnn and 829% from handcrafted pixel feature classification hpfc our model also obtains the best area under receiver operating characteristic curve auc of 095, compared to 093 from cnn and 090 from hpfc the new classification framework has the advantage of easy deployment on top of local classification features it corrects the local classification error by minimizing global classification error and it brings free additional classification performance   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "54347         0       0       0           0          0        0       0   \n",
       "23793         0       0       0           0          0        0       0   \n",
       "74208         0       0       0           0          0        0       0   \n",
       "137086        0       0       0           0          0        0       0   \n",
       "69238         0       0       0           0          0        0       0   \n",
       "67277         0       0       0           0          0        0       0   \n",
       "64727         0       0       0           0          0        0       0   \n",
       "14171         0       0       0           0          0        0       0   \n",
       "67936         0       0       0           0          0        0       0   \n",
       "105801        0       0       0           0          0        0       0   \n",
       "87247         0       0       0           0          0        0       0   \n",
       "114910        0       0       0           0          0        0       0   \n",
       "22692         0       0       0           0          0        0       0   \n",
       "91414         0       0       1           0          0        0       0   \n",
       "12962         0       0       0           0          0        0       0   \n",
       "101242        0       0       0           0          0        0       0   \n",
       "55728         0       0       0           0          0        0       0   \n",
       "85890         0       0       0           0          0        0       0   \n",
       "122998        0       0       1           0          0        0       0   \n",
       "42078         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "54347            0            0         0           0        0       0   \n",
       "23793            0            0         0           0        0       0   \n",
       "74208            0            0         0           0        0       0   \n",
       "137086           0            0         0           0        0       0   \n",
       "69238            0            0         0           0        0       0   \n",
       "67277            0            0         0           0        0       0   \n",
       "64727            0            0         0           0        0       0   \n",
       "14171            0            0         0           0        0       0   \n",
       "67936            0            0         0           0        0       0   \n",
       "105801           0            0         0           0        0       0   \n",
       "87247            0            0         0           0        0       0   \n",
       "114910           0            0         0           0        0       0   \n",
       "22692            0            0         0           0        0       0   \n",
       "91414            0            0         0           0        0       0   \n",
       "12962            0            0         0           0        0       0   \n",
       "101242           0            0         0           0        0       0   \n",
       "55728            0            0         0           0        0       0   \n",
       "85890            0            0         0           0        0       0   \n",
       "122998           0            0         0           0        0       0   \n",
       "42078            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "54347            0             0           0            0         0   \n",
       "23793            0             0           0            0         0   \n",
       "74208            0             0           0            0         0   \n",
       "137086           0             0           0            0         0   \n",
       "69238            0             0           0            0         0   \n",
       "67277            0             0           0            0         0   \n",
       "64727            0             0           0            0         0   \n",
       "14171            0             0           0            0         0   \n",
       "67936            0             0           0            0         0   \n",
       "105801           0             0           0            0         0   \n",
       "87247            0             0           0            0         0   \n",
       "114910           0             0           0            0         0   \n",
       "22692            0             0           0            0         0   \n",
       "91414            0             0           0            0         0   \n",
       "12962            0             0           0            0         0   \n",
       "101242           0             0           0            0         0   \n",
       "55728            0             0           0            0         0   \n",
       "85890            0             0           0            0         0   \n",
       "122998           0             0           0            0         0   \n",
       "42078            0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "54347           0           0            0           0            0   \n",
       "23793           0           0            0           0            0   \n",
       "74208           0           0            0           0            0   \n",
       "137086          0           0            0           0            0   \n",
       "69238           0           0            0           0            0   \n",
       "67277           0           0            0           0            0   \n",
       "64727           0           0            0           0            0   \n",
       "14171           0           0            0           0            0   \n",
       "67936           0           0            0           0            0   \n",
       "105801          0           0            0           0            0   \n",
       "87247           0           0            0           0            0   \n",
       "114910          0           0            0           0            0   \n",
       "22692           0           0            0           0            0   \n",
       "91414           0           0            0           0            0   \n",
       "12962           0           0            0           0            0   \n",
       "101242          0           0            0           0            0   \n",
       "55728           0           0            0           0            0   \n",
       "85890           0           0            0           0            0   \n",
       "122998          0           0            0           0            0   \n",
       "42078           0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "54347           0            0        0         0          0       0        0   \n",
       "23793           0            0        0         0          0       0        0   \n",
       "74208           0            0        0         0          0       0        0   \n",
       "137086          0            0        0         0          0       0        0   \n",
       "69238           0            0        0         0          0       0        0   \n",
       "67277           0            0        0         0          0       0        0   \n",
       "64727           0            0        0         0          0       0        0   \n",
       "14171           0            0        0         0          0       0        0   \n",
       "67936           0            0        0         0          0       0        0   \n",
       "105801          0            0        0         0          0       0        0   \n",
       "87247           0            0        0         0          0       0        0   \n",
       "114910          0            0        0         0          0       0        0   \n",
       "22692           0            0        0         0          0       0        0   \n",
       "91414           0            0        0         0          0       0        0   \n",
       "12962           0            0        0         0          0       0        0   \n",
       "101242          0            0        0         0          0       0        0   \n",
       "55728           0            0        0         0          0       0        0   \n",
       "85890           0            0        0         0          0       0        0   \n",
       "122998          0            0        0         0          0       0        0   \n",
       "42078           0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "54347          0          0        0       0         0          1        0   \n",
       "23793          0          0        0       0         0          0        0   \n",
       "74208          0          0        0       0         0          0        0   \n",
       "137086         0          0        0       0         0          0        0   \n",
       "69238          0          0        0       0         0          0        0   \n",
       "67277          0          0        0       0         0          0        0   \n",
       "64727          0          0        0       0         0          0        0   \n",
       "14171          0          0        0       0         0          0        0   \n",
       "67936          0          0        0       0         0          0        0   \n",
       "105801         0          0        0       0         0          0        0   \n",
       "87247          0          0        0       0         0          0        0   \n",
       "114910         0          0        0       0         0          0        0   \n",
       "22692          0          0        0       0         0          0        0   \n",
       "91414          0          0        0       0         0          0        0   \n",
       "12962          0          0        0       0         0          0        0   \n",
       "101242         0          0        0       0         0          1        0   \n",
       "55728          0          0        0       0         0          0        0   \n",
       "85890          0          0        0       0         0          0        0   \n",
       "122998         0          0        0       0         0          0        0   \n",
       "42078          0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "54347            0         0        0        0       0           0         0   \n",
       "23793            0         0        0        0       0           0         0   \n",
       "74208            0         0        0        0       0           0         0   \n",
       "137086           0         0        0        0       0           0         0   \n",
       "69238            0         0        0        0       0           0         0   \n",
       "67277            0         0        0        0       0           0         0   \n",
       "64727            0         0        0        0       0           0         0   \n",
       "14171            0         0        0        0       0           0         0   \n",
       "67936            0         0        1        0       0           0         0   \n",
       "105801           0         0        0        0       0           0         0   \n",
       "87247            0         0        0        0       0           0         0   \n",
       "114910           0         0        0        0       0           0         0   \n",
       "22692            0         0        0        0       0           0         0   \n",
       "91414            0         0        0        0       0           0         0   \n",
       "12962            0         0        0        0       0           0         0   \n",
       "101242           0         0        0        0       0           0         0   \n",
       "55728            0         0        0        0       0           0         0   \n",
       "85890            0         0        0        0       0           0         0   \n",
       "122998           0         0        0        0       0           0         0   \n",
       "42078            0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text  \n",
       "54347        1            0           1  \n",
       "23793        1            0           1  \n",
       "74208        1            0           1  \n",
       "137086       1            0           1  \n",
       "69238        1            0           1  \n",
       "67277        1            0           1  \n",
       "64727        1            0           1  \n",
       "14171        1            0           1  \n",
       "67936        1            0           1  \n",
       "105801       1            0           1  \n",
       "87247        1            0           1  \n",
       "114910       1            0           1  \n",
       "22692        1            0           1  \n",
       "91414        1            0           1  \n",
       "12962        1            0           1  \n",
       "101242       1            1           1  \n",
       "55728        1            0           1  \n",
       "85890        1            0           1  \n",
       "122998       1            0           1  \n",
       "42078        1            0           1  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['retina_text']=='1'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bd07100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32751, '1': 1428})\n"
     ]
    }
   ],
   "source": [
    "## OPHTHALMOLOGY [C11] / eye\n",
    "\n",
    "## text\n",
    "text = ['ophth', 'retina', 'retino', 'retinitis', 'eye disease', 'uveitis', 'iritis', 'conjunctiv', 'cornea', 'blephar',\n",
    "       'optic nerve', 'optic atrophy', 'optic disk', 'optic disc', 'optic neuropathy', 'choroid', 'blindness', 'macular',\n",
    "       'strabismus', 'ocular', 'glaucoma']\n",
    "\n",
    "spec['eye_text'] = np.where(groups['text'].str.contains('eye disease'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['eye_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['eye_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "spec['eye_text'] = np.where((groups['text'].str.contains(\"eye\")) &\n",
    "                             (groups['text'].str.contains(\"optic\")) , \"1\", spec['eye_text'])\n",
    "spec['eye_text'] = np.where((groups['text'].str.contains(\"eye\")) &\n",
    "                             (groups['text'].str.contains(\"fundus\")) , \"1\", spec['eye_text'])\n",
    "spec['eye_text'] = np.where((groups['text'].str.contains(\"eye\")) &\n",
    "                             (groups['text'].str.contains(\"fundal\")) , \"1\", spec['eye_text'])\n",
    "    \n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['eye_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb7f15c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105801</th>\n",
       "      <td>computer-assisted identification of proliferative diabetic retinopathy in color retinal images advanced proliferative stage of diabetic retinopathy dr is indicated by the growth of thin, fragile and highly unregulated vessels, neovascularization nv in order to identify proliferative diabetic retinopathy pdr, our approach models the micro-pattern of local variations using texture based analysis and quantifies the structural changes in vessel patterns in localized patches, to map them to the confidence score of being neovascular using supervised learning framework rule-based criteria on patch-level neovascularity scores in an image is used for the decision of absence or presence of pdr evaluated using 3 datasets, our method achieves 96% sensitivity and 926% specificity for localizing nv image-level identification of pdr achieves high sensitivity of 9672% at 796% specificity and high specificity of 9650% at 7322% sensitivity our approach could have potential application in dr grading where it can localize nve regions and identify pdr images for immediate intervention</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28237</th>\n",
       "      <td>corneal edema visualization with optical coherence tomography using deep learning: proof of concept optical coherence tomography oct is essential for the diagnosis and follow-up of corneal edema, but assessment can be challenging in minimal or localized edema the objective was to develop and validate a novel automated tool to detect and visualize corneal edema with oct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>the bayesian additive regression trees formula for safe machine learning-based intraocular lens predictions &lt;b&gt;purpose:&lt;/b&gt; our work introduces a highly accurate, safe, and sufficiently explicable machine-learning artificial intelligence model of intraocular lens power iol translating into better post-surgical outcomes for patients with cataracts we also demonstrate its improved predictive accuracy over previous formulas &lt;b&gt;methods:&lt;/b&gt; we collected retrospective eye measurement data on 5,331 eyes from 3,276 patients across multiple centers who received a lens implantation during cataract surgery the dependent measure is the post-operative manifest spherical equivalent error from intended and the independent variables are the patient- and eye-specific characteristics this dataset was split so that one subset was for formula construction and the other for validating our new formula data excluded fellow eyes, so as not to confound the prediction with bilateral eyes &lt;b&gt;results:&lt;/b&gt; our formula is three times more precise than reported studies with a median absolute iol error of 0204 diopters d when converted to absolute predictive refraction errors on the cornea, the median error is 0137 d which is close to the iol manufacturer tolerance these estimates are validated out-of-sample and thus are expected to reflect the future performance of our prediction formula, especially since our data were collected from a wide variety of patients, clinics, and manufacturers &lt;b&gt;conclusion:&lt;/b&gt; the increased precision of iol power calculations has the potential to optimize patient positive refractive outcomes our model also provides uncertainty plots that can be used in tandem with the clinicians expertise and previous formula output, further enhancing the safety &lt;b&gt;translational relavance:&lt;/b&gt; our new machine learning process has the potential to significantly improve patient iol refractive outcomes safely</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52855</th>\n",
       "      <td>predicting the likelihood of need for future keratoplasty intervention using artificial intelligence to apply artificial intelligence ai for automated identification of corneal condition and prediction of the likelihood of need for future keratoplasty intervention from optical coherence tomography oct-based corneal parameters</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60425</th>\n",
       "      <td>utility of a public-available artificial intelligence in diagnosis of polypoidal choroidal vasculopathy to investigate the feasibility of training an artificial intelligence ai on a public-available ai platform to diagnose polypoidal choroidal vasculopathy pcv using indocyanine green angiography icga</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "105801                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            computer-assisted identification of proliferative diabetic retinopathy in color retinal images advanced proliferative stage of diabetic retinopathy dr is indicated by the growth of thin, fragile and highly unregulated vessels, neovascularization nv in order to identify proliferative diabetic retinopathy pdr, our approach models the micro-pattern of local variations using texture based analysis and quantifies the structural changes in vessel patterns in localized patches, to map them to the confidence score of being neovascular using supervised learning framework rule-based criteria on patch-level neovascularity scores in an image is used for the decision of absence or presence of pdr evaluated using 3 datasets, our method achieves 96% sensitivity and 926% specificity for localizing nv image-level identification of pdr achieves high sensitivity of 9672% at 796% specificity and high specificity of 9650% at 7322% sensitivity our approach could have potential application in dr grading where it can localize nve regions and identify pdr images for immediate intervention   \n",
       "28237                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  corneal edema visualization with optical coherence tomography using deep learning: proof of concept optical coherence tomography oct is essential for the diagnosis and follow-up of corneal edema, but assessment can be challenging in minimal or localized edema the objective was to develop and validate a novel automated tool to detect and visualize corneal edema with oct   \n",
       "22522   the bayesian additive regression trees formula for safe machine learning-based intraocular lens predictions <b>purpose:</b> our work introduces a highly accurate, safe, and sufficiently explicable machine-learning artificial intelligence model of intraocular lens power iol translating into better post-surgical outcomes for patients with cataracts we also demonstrate its improved predictive accuracy over previous formulas <b>methods:</b> we collected retrospective eye measurement data on 5,331 eyes from 3,276 patients across multiple centers who received a lens implantation during cataract surgery the dependent measure is the post-operative manifest spherical equivalent error from intended and the independent variables are the patient- and eye-specific characteristics this dataset was split so that one subset was for formula construction and the other for validating our new formula data excluded fellow eyes, so as not to confound the prediction with bilateral eyes <b>results:</b> our formula is three times more precise than reported studies with a median absolute iol error of 0204 diopters d when converted to absolute predictive refraction errors on the cornea, the median error is 0137 d which is close to the iol manufacturer tolerance these estimates are validated out-of-sample and thus are expected to reflect the future performance of our prediction formula, especially since our data were collected from a wide variety of patients, clinics, and manufacturers <b>conclusion:</b> the increased precision of iol power calculations has the potential to optimize patient positive refractive outcomes our model also provides uncertainty plots that can be used in tandem with the clinicians expertise and previous formula output, further enhancing the safety <b>translational relavance:</b> our new machine learning process has the potential to significantly improve patient iol refractive outcomes safely   \n",
       "52855                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              predicting the likelihood of need for future keratoplasty intervention using artificial intelligence to apply artificial intelligence ai for automated identification of corneal condition and prediction of the likelihood of need for future keratoplasty intervention from optical coherence tomography oct-based corneal parameters   \n",
       "60425                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        utility of a public-available artificial intelligence in diagnosis of polypoidal choroidal vasculopathy to investigate the feasibility of training an artificial intelligence ai on a public-available ai platform to diagnose polypoidal choroidal vasculopathy pcv using indocyanine green angiography icga   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "105801        0       0       0           0          0        0       0   \n",
       "28237         0       0       0           0          0        0       0   \n",
       "22522         0       0       0           0          0        0       0   \n",
       "52855         0       0       0           0          0        0       0   \n",
       "60425         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "105801           0            0         0           0        0       0   \n",
       "28237            0            0         0           0        0       0   \n",
       "22522            0            0         0           0        0       0   \n",
       "52855            0            0         0           0        0       0   \n",
       "60425            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "105801           0             0           0            0         0   \n",
       "28237            0             0           0            0         0   \n",
       "22522            0             0           0            0         0   \n",
       "52855            0             0           0            0         0   \n",
       "60425            0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "105801          0           0            0           0            0   \n",
       "28237           0           0            0           0            0   \n",
       "22522           0           0            0           0            0   \n",
       "52855           0           0            0           0            0   \n",
       "60425           0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "105801          0            0        0         0          0       0        0   \n",
       "28237           0            0        0         0          0       0        0   \n",
       "22522           0            0        0         0          0       0        0   \n",
       "52855           0            0        0         0          0       0        0   \n",
       "60425           0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "105801         0          0        0       0         0          0        0   \n",
       "28237          0          0        0       0         0          0        0   \n",
       "22522          0          0        0       0         0          0        0   \n",
       "52855          0          0        0       0         0          0        0   \n",
       "60425          0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "105801           0         0        0        0       0           0         0   \n",
       "28237            0         0        0        0       0           0         0   \n",
       "22522            0         0        0        0       0           0         0   \n",
       "52855            0         0        0        0       0           0         0   \n",
       "60425            0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text  \n",
       "105801       1            0           1        1  \n",
       "28237        0            0           0        1  \n",
       "22522        0            0           0        1  \n",
       "52855        0            0           0        1  \n",
       "60425        0            0           0        1  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['eye_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f2a93c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33435, '1': 744})\n"
     ]
    }
   ],
   "source": [
    "## HAEMATOLOGIC [C15] / haem\n",
    "\n",
    "## text\n",
    "text = ['haematological cancer', 'hematological cancer', 'haematological malig', 'hematological malig', 'myelodysplas',\n",
    "       'myeloprolif', 'lymphoprolif', 'leukaemia', 'leukemia', 'myelofibro', 'thrombocythemia', 'polycythemia vera',\n",
    "       'polycythemia rubra vera', 'thrombocythaemia', 'polycythaemia vera', 'polycythaemia rubra vera', 'lymphoma',\n",
    "       'myeloma', ' gvhd', 'stem cell transpl', 'bone marrow aspirate',\n",
    "       'haematolog', 'anemia', 'anaemia', 'hemoglobin', 'haemoglobin', 'sickle cell', 'thalassemia', 'thalassaemia',\n",
    "       'sickle crisis', 'clotting disorder', 'coagulation disorder', 'coagulopathy', 'hemophilia', 'haemophilia',\n",
    "       'von willebrand', 'disseminated intrasvascular', 'thrombocytopeni', 'hemoly', 'haemoly', 'cryoglob', 'thrombim',\n",
    "       'bone marrow', 'coagulation']\n",
    "\n",
    "spec['haem_text'] = np.where(groups['text'].str.contains('hematolog'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['haem_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['haem_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['haem_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf21fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33140, '1': 1039})\n"
     ]
    }
   ],
   "source": [
    "## GYNAE/OBSTETRIC [C13] / obs\n",
    "\n",
    "## text\n",
    "text = ['obstetric', 'fetal', 'foetal', 'foetus', 'fetus', 'gestation', 'pregnan', 'endometriosis', 'ovarian', 'gynecolog', 'uterine', 'uterus'\n",
    "       'cervix', 'pap smear', 'cervical cancer', 'cervical carcinoma', ' vagina ', 'vaginal', 'vaginosis', 'macrosomia', 'colposcop',\n",
    "       'gynaecolog', 'menopaus', 'eclamp', ' iugr ', 'caesarean', 'endometrial']\n",
    "\n",
    "spec['obs_text'] = np.where(groups['text'].str.contains('cesarean'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['obs_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['obs_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['obs_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "901255f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33427, '1': 752})\n"
     ]
    }
   ],
   "source": [
    "## NEPHROLOGY [C12] AND UROLOGY / renal\n",
    "\n",
    "## text\n",
    "text = [' renal ', 'kidney', 'hemodialysis', 'haemodialysis', 'hemofilt', 'haemofilt', 'nephro', 'nephrit', 'glomerulus',\n",
    "       'bladder', 'urethral']\n",
    "\n",
    "spec['renal_text'] = np.where(groups['text'].str.contains('renovasc'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['renal_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['renal_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['renal_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f8a4f628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88378</th>\n",
       "      <td>a lasso method to identify protein signature predicting post-transplant renal graft survival identifying novel biomarkers to predict renal graft survival is important in post-transplant clinical practice serum creatinine, currently the most popular surrogate biomarker, offers limited information of the underlying allograft profiles it is known to perform unsatisfactorily to predict renal function in this paper, we apply a lasso machine-learning algorithm in the cox proportional hazards model to identify promising proteins that are associated with the hazard of allograft loss after renal transplantation, motivated by a clinical pilot study that collected 47 patients receiving renal transplants at the university of michigan hospital we assess the association of 17 proteins previously identified by cibrik et al 5 with allograft rejection in our regularized cox regression analysis, where the lasso variable selection method is applied to select important proteins that predict the hazard of allograft loss we also develop a post-selection inference to further investigate the statistical significance of the proteins on the hazard of allograft loss, and conclude that two proteins kim-1 and vegf-r2 are important protein markers for risk prediction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32516</th>\n",
       "      <td>coupled mass-spectrometry-based lipidomics machine learning approach for early detection of clear cell renal cell carcinoma a discovery-based lipid profiling study of serum samples from a cohort that included patients with clear cell renal cell carcinoma ccrcc stages i, ii, iii, and iv &lt;i&gt;n&lt;/i&gt; = 112 and controls &lt;i&gt;n&lt;/i&gt; = 52 was performed using ultraperformance liquid chromatography coupled to quadrupole-time-of-flight mass spectrometry and machine learning techniques multivariate models based on support vector machines and the lasso variable selection method yielded two discriminant lipid panels for ccrcc detection and early diagnosis a 16-lipid panel allowed discriminating ccrcc patients from controls with 957% accuracy in a training set under cross-validation and 771% accuracy in an independent test set a second model trained to discriminate early i and ii from late iii and iv stage ccrcc yielded a panel of 26 compounds that classified stage i patients from an independent test set with 821% accuracy thirteen species, including cholic acid, undecylenic acid, lauric acid, lpc16:0/0:0, and pc18:2/18:2, identified with level 1 exhibited significantly lower levels in samples from ccrcc patients compared to controls moreover, 3α-hydroxy-5α-androstan-17-one 3-sulfate, &lt;i&gt;cis&lt;/i&gt;-5-dodecenoic acid, arachidonic acid, &lt;i&gt;cis&lt;/i&gt;-13-docosenoic acid, pi16:0/18:1, pc16:0/18:2, and pco-16:0/20:4 contributed to discriminate early from late ccrcc stage patients the results are auspicious for early ccrcc diagnosis after validation of the panels in larger and different cohorts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118293</th>\n",
       "      <td>an active learning approach for rapid characterization of endothelial cells in human tumors currently, no available pathological or molecular measures of tumor angiogenesis predict response to antiangiogenic therapies used in clinical practice recognizing that tumor endothelial cells ec and ec activation and survival signaling are the direct targets of these therapies, we sought to develop an automated platform for quantifying activity of critical signaling pathways and other biological events in ec of patient tumors by histopathology computer image analysis of ec in highly heterogeneous human tumors by a statistical classifier trained using examples selected by human experts performed poorly due to subjectivity and selection bias we hypothesized that the analysis can be optimized by a more active process to aid experts in identifying informative training examples to test this hypothesis, we incorporated a novel active learning al algorithm into farsight image analysis software that aids the expert by seeking out informative examples for the operator to label the resulting farsight-al system identified ec with specificity and sensitivity consistently greater than 09 and outperformed traditional supervised classification algorithms the system modeled individual operator preferences and generated reproducible results using the results of ec classification, we also quantified proliferation ki67 and activity in important signal transduction pathways map kinase, stat3 in immunostained human clear cell renal cell carcinoma and other tumors farsight-al enables characterization of ec in conventionally preserved human tumors in a more automated process suitable for testing and validating in clinical trials the results of our study support a unique opportunity for quantifying angiogenesis in a manner that can now be tested for its ability to identify novel predictive and response biomarkers</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50125</th>\n",
       "      <td>weakly-supervised convolutional neural networks of renal tumor segmentation in abdominal cta images renal cancer is one of the 10 most common cancers in human beings the laparoscopic partial nephrectomy lpn is an effective way to treat renal cancer localization and delineation of the renal tumor from pre-operative ct angiography cta is an important step for lpn surgery planning recently, with the development of the technique of deep learning, deep neural networks can be trained to provide accurate pixel-wise renal tumor segmentation in cta images however, constructing the training dataset with a large amount of pixel-wise annotations is a time-consuming task for the radiologists therefore, weakly-supervised approaches attract more interest in research</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51684</th>\n",
       "      <td>noninvasive fuhrman grading of clear cell renal cell carcinoma using computed tomography radiomic features and machine learning to identify optimal classification methods for computed tomography ct radiomics-based preoperative prediction of clear cell renal cell carcinoma ccrcc grade</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "88378                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    a lasso method to identify protein signature predicting post-transplant renal graft survival identifying novel biomarkers to predict renal graft survival is important in post-transplant clinical practice serum creatinine, currently the most popular surrogate biomarker, offers limited information of the underlying allograft profiles it is known to perform unsatisfactorily to predict renal function in this paper, we apply a lasso machine-learning algorithm in the cox proportional hazards model to identify promising proteins that are associated with the hazard of allograft loss after renal transplantation, motivated by a clinical pilot study that collected 47 patients receiving renal transplants at the university of michigan hospital we assess the association of 17 proteins previously identified by cibrik et al 5 with allograft rejection in our regularized cox regression analysis, where the lasso variable selection method is applied to select important proteins that predict the hazard of allograft loss we also develop a post-selection inference to further investigate the statistical significance of the proteins on the hazard of allograft loss, and conclude that two proteins kim-1 and vegf-r2 are important protein markers for risk prediction   \n",
       "32516                                                                                                                                                                                                                                                                                                                                       coupled mass-spectrometry-based lipidomics machine learning approach for early detection of clear cell renal cell carcinoma a discovery-based lipid profiling study of serum samples from a cohort that included patients with clear cell renal cell carcinoma ccrcc stages i, ii, iii, and iv <i>n</i> = 112 and controls <i>n</i> = 52 was performed using ultraperformance liquid chromatography coupled to quadrupole-time-of-flight mass spectrometry and machine learning techniques multivariate models based on support vector machines and the lasso variable selection method yielded two discriminant lipid panels for ccrcc detection and early diagnosis a 16-lipid panel allowed discriminating ccrcc patients from controls with 957% accuracy in a training set under cross-validation and 771% accuracy in an independent test set a second model trained to discriminate early i and ii from late iii and iv stage ccrcc yielded a panel of 26 compounds that classified stage i patients from an independent test set with 821% accuracy thirteen species, including cholic acid, undecylenic acid, lauric acid, lpc16:0/0:0, and pc18:2/18:2, identified with level 1 exhibited significantly lower levels in samples from ccrcc patients compared to controls moreover, 3α-hydroxy-5α-androstan-17-one 3-sulfate, <i>cis</i>-5-dodecenoic acid, arachidonic acid, <i>cis</i>-13-docosenoic acid, pi16:0/18:1, pc16:0/18:2, and pco-16:0/20:4 contributed to discriminate early from late ccrcc stage patients the results are auspicious for early ccrcc diagnosis after validation of the panels in larger and different cohorts   \n",
       "118293  an active learning approach for rapid characterization of endothelial cells in human tumors currently, no available pathological or molecular measures of tumor angiogenesis predict response to antiangiogenic therapies used in clinical practice recognizing that tumor endothelial cells ec and ec activation and survival signaling are the direct targets of these therapies, we sought to develop an automated platform for quantifying activity of critical signaling pathways and other biological events in ec of patient tumors by histopathology computer image analysis of ec in highly heterogeneous human tumors by a statistical classifier trained using examples selected by human experts performed poorly due to subjectivity and selection bias we hypothesized that the analysis can be optimized by a more active process to aid experts in identifying informative training examples to test this hypothesis, we incorporated a novel active learning al algorithm into farsight image analysis software that aids the expert by seeking out informative examples for the operator to label the resulting farsight-al system identified ec with specificity and sensitivity consistently greater than 09 and outperformed traditional supervised classification algorithms the system modeled individual operator preferences and generated reproducible results using the results of ec classification, we also quantified proliferation ki67 and activity in important signal transduction pathways map kinase, stat3 in immunostained human clear cell renal cell carcinoma and other tumors farsight-al enables characterization of ec in conventionally preserved human tumors in a more automated process suitable for testing and validating in clinical trials the results of our study support a unique opportunity for quantifying angiogenesis in a manner that can now be tested for its ability to identify novel predictive and response biomarkers    \n",
       "50125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    weakly-supervised convolutional neural networks of renal tumor segmentation in abdominal cta images renal cancer is one of the 10 most common cancers in human beings the laparoscopic partial nephrectomy lpn is an effective way to treat renal cancer localization and delineation of the renal tumor from pre-operative ct angiography cta is an important step for lpn surgery planning recently, with the development of the technique of deep learning, deep neural networks can be trained to provide accurate pixel-wise renal tumor segmentation in cta images however, constructing the training dataset with a large amount of pixel-wise annotations is a time-consuming task for the radiologists therefore, weakly-supervised approaches attract more interest in research   \n",
       "51684                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 noninvasive fuhrman grading of clear cell renal cell carcinoma using computed tomography radiomic features and machine learning to identify optimal classification methods for computed tomography ct radiomics-based preoperative prediction of clear cell renal cell carcinoma ccrcc grade   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "88378         0       0       0           0          0        0       0   \n",
       "32516         0       0       0           0          0        0       0   \n",
       "118293        0       0       0           0          0        1       0   \n",
       "50125         0       0       0           0          0        0       0   \n",
       "51684         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "88378            0            0         0           0        0       0   \n",
       "32516            0            0         0           0        1       0   \n",
       "118293           0            0         0           0        1       0   \n",
       "50125            0            0         0           0        1       0   \n",
       "51684            0            0         0           0        1       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "88378            0             0           0            0         0   \n",
       "32516            0             0           0            0         0   \n",
       "118293           0             0           0            0         0   \n",
       "50125            0             0           0            0         0   \n",
       "51684            0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "88378           0           0            0           0            0   \n",
       "32516           0           0            1           0            0   \n",
       "118293          0           0            1           0            0   \n",
       "50125           0           0            1           0            0   \n",
       "51684           0           0            1           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "88378           0            0        0         0          0       0        0   \n",
       "32516           0            0        0         0          0       0        0   \n",
       "118293          0            0        0         0          0       0        0   \n",
       "50125           0            0        0         0          0       0        0   \n",
       "51684           0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "88378          0          0        0       0         0          0        0   \n",
       "32516          0          0        0       0         0          0        0   \n",
       "118293         0          0        0       0         0          0        0   \n",
       "50125          0          0        0       0         0          0        0   \n",
       "51684          0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "88378            0         0        0        0       0           0         0   \n",
       "32516            0         0        0        0       0           0         0   \n",
       "118293           0         0        0        0       0           0         0   \n",
       "50125            0         0        0        0       0           0         0   \n",
       "51684            0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text haem_text obs_text renal_text  \n",
       "88378        0            0           0        0         0        0          1  \n",
       "32516        0            0           0        0         0        0          1  \n",
       "118293       0            0           0        0         0        0          1  \n",
       "50125        0            0           0        0         0        0          1  \n",
       "51684        0            0           0        0         0        0          1  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['renal_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c49679a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33917, '1': 262})\n"
     ]
    }
   ],
   "source": [
    "## ACUTE & CHRONIC KIDNEY DISEASE / ACKD\n",
    "\n",
    "## text\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"acute kidney\"), \"1\", \"0\")\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"acute renal\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"kidney failure\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"renal failure\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"chronic kidney disease\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"chronic renal disease\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"stage kidney\"), \"1\", spec['ackd_text'])\n",
    "spec['ackd_text'] = np.where(groups['text'].str.contains(\"stage renal\"), \"1\", spec['ackd_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['ackd_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64e9093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 32370, '1': 1809})\n"
     ]
    }
   ],
   "source": [
    "## PAEDIATRICS / paeds\n",
    "\n",
    "## text\n",
    "text = ['paedia', 'pedia', 'neonate', 'neonatal', 'teenage', 'youth', 'children', 'childhood', 'infant', \n",
    "       'newborn', 'baby', 'babies', 'toddler']\n",
    "\n",
    "spec['paeds_text'] = np.where(groups['text'].str.contains(' child '), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['paeds_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['paeds_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['paeds_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6f602e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21566</th>\n",
       "      <td>machine learning models on adc features to assess brain changes of children with pierre robin sequence in order to evaluate brain changes in young children with pierre robin sequence prs using machine learning based on apparent diffusion coefficient adc features, we retrospectively enrolled a total of 60 cases 42 in the training dataset and 18 in the testing dataset which included 30 prs and 30 controls from the childrens hospital affiliated to the nanjing medical university from january 2017-december 2019 there were 21 and nine prs cases in each dataset, with the remainder belonging to the control group in the same age range a total of 105 adc features were extracted from magnetic resonance imaging mri data features were pruned using least absolute shrinkage and selection operator lasso regression and seven adc features were developed as the optimal signatures for training machine learning models support vector machine svm achieved an area under the receiver operating characteristic curve auc of 099 for the training set and 085 for the testing set the auc of the multivariable logistic regression mlr and the adaboost for the training and validation dataset were 098/084 and 094/069, respectively based on the adc features, the two groups of cases ie, the prs group and the control group could be well-distinguished by the machine learning models, indicating that there is a significant difference in brain development between children with prs and normal controls</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>a simple and robust methylation test for risk stratification of patients with juvenile myelomonocytic leukemia juvenile myelomonocytic leukemia jmml is a rare myelodysplastic/myeloproliferative neoplasm that develops during infancy and early childhood the array-based international consensus definition of dna methylation has recently classified patients with jmml into the following three groups: high methylation hm, intermediate methylation im, and low methylation lm to develop a simple and robust methylation clinical test, 137 patients with jmml have been analyzed using the digital restriction enzyme analysis of methylation dream, which is a next-generation sequencing based methylation analysis unsupervised consensus clustering of the discovery cohort n=99 using the dream data has identified hm and lm subgroups hm_dream, n=35; lm_dream; n=64 of the 98 cases that could be compared with the international consensus classification, 90 cases of hm n=30 and lm n=60 had 100% concordance with the dream clustering results for the remaining eight cases classified as the im group, four cases were classified into the hm_dream group and four cases into the lm_dream group a machine-learning classifier has been successfully constructed using a support vector machine svm, which divided the validation cohort n=38 into hm hm_svm; n=18 and lm lm_svm; n=20 groups patients with the hm_svm profile had a significantly poorer 5-year overall survival rate than those with the lm_svm profile in conclusion, a robust methylation test has been developed using the dream analysis for patients with jmml this simple and straightforward test can be easily incorporated in diagnosis to generate a methylation classification for patients so that they can receive risk-adapted treatment in the context of future clinical trials</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121207</th>\n",
       "      <td>automated down syndrome detection using facial photographs down syndrome, the most common single cause of human birth defects, produces alterations in physical growth and mental retardation; its early detection is crucial children with down syndrome generally have distinctive facial characteristics, which brings an opportunity for the computer-aided diagnosis of down syndrome using photographs of patients in this study, we propose a novel strategy based on machine learning techniques to detect down syndrome automatically a modified constrained local model is used to locate facial landmarks then geometric features and texture features based on local binary patterns are extracted around each landmark finally, down syndrome is detected using a variety of classifiers the best performance achieved 946% accuracy, 933% precision and 955% recall by using support vector machine with radial basis function kernel the results indicate that our method could assist in down syndrome screening effectively in a simple, non-invasive way</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>machine learning for detection of correct peripherally inserted central catheter tip position from radiology reports in infants  in critically ill infants, the position of a peripherally inserted central catheter picc must be confirmed frequently, as the tip may move from its original position and run the risk of hyperosmolar vascular damage or extravasation into surrounding spaces automated detection of picc tip position holds great promise for alerting bedside clinicians to noncentral piccs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67163</th>\n",
       "      <td>monitoring disease progression with a quantitative severity scale for retinopathy of prematurity using deep learning retinopathy of prematurity rop is a leading cause of childhood blindness worldwide, but clinical diagnosis is subjective and qualitative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text  \\\n",
       "21566                                                                                                                                                                                                                                                                                                                                                   machine learning models on adc features to assess brain changes of children with pierre robin sequence in order to evaluate brain changes in young children with pierre robin sequence prs using machine learning based on apparent diffusion coefficient adc features, we retrospectively enrolled a total of 60 cases 42 in the training dataset and 18 in the testing dataset which included 30 prs and 30 controls from the childrens hospital affiliated to the nanjing medical university from january 2017-december 2019 there were 21 and nine prs cases in each dataset, with the remainder belonging to the control group in the same age range a total of 105 adc features were extracted from magnetic resonance imaging mri data features were pruned using least absolute shrinkage and selection operator lasso regression and seven adc features were developed as the optimal signatures for training machine learning models support vector machine svm achieved an area under the receiver operating characteristic curve auc of 099 for the training set and 085 for the testing set the auc of the multivariable logistic regression mlr and the adaboost for the training and validation dataset were 098/084 and 094/069, respectively based on the adc features, the two groups of cases ie, the prs group and the control group could be well-distinguished by the machine learning models, indicating that there is a significant difference in brain development between children with prs and normal controls   \n",
       "2306    a simple and robust methylation test for risk stratification of patients with juvenile myelomonocytic leukemia juvenile myelomonocytic leukemia jmml is a rare myelodysplastic/myeloproliferative neoplasm that develops during infancy and early childhood the array-based international consensus definition of dna methylation has recently classified patients with jmml into the following three groups: high methylation hm, intermediate methylation im, and low methylation lm to develop a simple and robust methylation clinical test, 137 patients with jmml have been analyzed using the digital restriction enzyme analysis of methylation dream, which is a next-generation sequencing based methylation analysis unsupervised consensus clustering of the discovery cohort n=99 using the dream data has identified hm and lm subgroups hm_dream, n=35; lm_dream; n=64 of the 98 cases that could be compared with the international consensus classification, 90 cases of hm n=30 and lm n=60 had 100% concordance with the dream clustering results for the remaining eight cases classified as the im group, four cases were classified into the hm_dream group and four cases into the lm_dream group a machine-learning classifier has been successfully constructed using a support vector machine svm, which divided the validation cohort n=38 into hm hm_svm; n=18 and lm lm_svm; n=20 groups patients with the hm_svm profile had a significantly poorer 5-year overall survival rate than those with the lm_svm profile in conclusion, a robust methylation test has been developed using the dream analysis for patients with jmml this simple and straightforward test can be easily incorporated in diagnosis to generate a methylation classification for patients so that they can receive risk-adapted treatment in the context of future clinical trials   \n",
       "121207                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                automated down syndrome detection using facial photographs down syndrome, the most common single cause of human birth defects, produces alterations in physical growth and mental retardation; its early detection is crucial children with down syndrome generally have distinctive facial characteristics, which brings an opportunity for the computer-aided diagnosis of down syndrome using photographs of patients in this study, we propose a novel strategy based on machine learning techniques to detect down syndrome automatically a modified constrained local model is used to locate facial landmarks then geometric features and texture features based on local binary patterns are extracted around each landmark finally, down syndrome is detected using a variety of classifiers the best performance achieved 946% accuracy, 933% precision and 955% recall by using support vector machine with radial basis function kernel the results indicate that our method could assist in down syndrome screening effectively in a simple, non-invasive way    \n",
       "4356                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            machine learning for detection of correct peripherally inserted central catheter tip position from radiology reports in infants  in critically ill infants, the position of a peripherally inserted central catheter picc must be confirmed frequently, as the tip may move from its original position and run the risk of hyperosmolar vascular damage or extravasation into surrounding spaces automated detection of picc tip position holds great promise for alerting bedside clinicians to noncentral piccs   \n",
       "67163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               monitoring disease progression with a quantitative severity scale for retinopathy of prematurity using deep learning retinopathy of prematurity rop is a leading cause of childhood blindness worldwide, but clinical diagnosis is subjective and qualitative   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "21566         0       0       0           0          0        0       0   \n",
       "2306          0       0       0           0          0        0       0   \n",
       "121207        0       0       0           0          0        0       0   \n",
       "4356          0       0       0           0          0        0       0   \n",
       "67163         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "21566            0            0         0           0        0       0   \n",
       "2306             0            0         0           0        1       0   \n",
       "121207           0            0         0           0        0       0   \n",
       "4356             0            0         0           0        0       0   \n",
       "67163            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "21566            0             0           0            0         0   \n",
       "2306             0             0           0            0         0   \n",
       "121207           0             0           0            0         0   \n",
       "4356             0             0           0            0         0   \n",
       "67163            0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "21566           0           0            0           0            0   \n",
       "2306            0           0            0           0            1   \n",
       "121207          0           0            0           0            0   \n",
       "4356            0           0            0           0            0   \n",
       "67163           0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "21566           0            0        0         0          0       0        0   \n",
       "2306            0            0        0         0          0       0        0   \n",
       "121207          0            0        0         0          0       0        0   \n",
       "4356            0            0        0         0          0       0        0   \n",
       "67163           0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "21566          0          0        0       0         0          1        0   \n",
       "2306           0          0        0       0         0          0        0   \n",
       "121207         0          0        0       0         0          0        0   \n",
       "4356           0          0        0       0         0          0        0   \n",
       "67163          0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "21566            0         0        0        0       0           0         0   \n",
       "2306             0         0        0        0       0           0         0   \n",
       "121207           0         0        0        0       0           0         0   \n",
       "4356             0         0        0        0       0           0         0   \n",
       "67163            0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text haem_text obs_text  \\\n",
       "21566        0            0           0        0         0        0   \n",
       "2306         0            0           0        0         1        0   \n",
       "121207       0            0           0        0         0        0   \n",
       "4356         0            0           0        0         0        0   \n",
       "67163        0            0           0        1         0        0   \n",
       "\n",
       "       renal_text ackd_text paeds_text  \n",
       "21566           0         0          1  \n",
       "2306            0         0          1  \n",
       "121207          0         0          1  \n",
       "4356            0         0          1  \n",
       "67163           0         0          1  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['paeds_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2c6cce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33852, '1': 327})\n"
     ]
    }
   ],
   "source": [
    "## STOMATOGNATHIC, DENTAL [C07]  / dent\n",
    "\n",
    "## text\n",
    "text = [' dental', 'dentist', 'dentition', 'teeth', 'tooth', 'canine', 'incisor', 'molars', 'maxilla', 'mandibul', 'mandible',\n",
    "       'stomatognathic', 'gingiva', 'buccal', 'peridont']\n",
    "\n",
    "spec['dent_text'] = np.where(groups['text'].str.contains('maxillofacial'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['dent_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['dent_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "## output\n",
    "print('text counts:')\n",
    "print(Counter(spec['dent_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "68ed791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>artificial intelligence and infrared thermography as auxiliary tools in the diagnosis of temporomandibular disorder to assess three machine learning ml attribute extraction methods: radiomic, semantic and radiomic-semantic association on temporomandibular disorder tmd detection using infrared thermography it; and to determine which ml classifier, knn, svm and mlp, is the most efficient for this purpose</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9362</th>\n",
       "      <td>caries and restoration detection using bitewing film based on transfer learning with cnns caries is a dental disease caused by bacterial infection if the cause of the caries is detected early, the treatment will be relatively easy, which in turn prevents caries from spreading the current common procedure of dentists is to first perform radiographic examination on the patient and mark the lesions manually however, the work of judging lesions and markings requires professional experience and is very time-consuming and repetitive taking advantage of the rapid development of artificial intelligence imaging research and technical methods will help dentists make accurate markings and improve medical treatments it can also shorten the judgment time of professionals in addition to the use of gaussian high-pass filter and otsus threshold image enhancement technology, this research solves the problem that the original cutting technology cannot extract certain single teeth, and it proposes a caries and lesions area analysis model based on convolutional neural networks cnn, which can identify caries and restorations from the bitewing images moreover, it provides dentists with more accurate objective judgment data to achieve the purpose of automatic diagnosis and treatment planning as a technology for assisting precision medicine a standardized database established following a defined set of steps is also proposed in this study there are three main steps to generate the image of a single tooth from a bitewing image, which can increase the accuracy of the analysis model the steps include 1 preprocessing of the dental image to obtain a high-quality binarization, 2 a dental image cropping procedure to obtain individually separated tooth samples, and 3 a dental image masking step which masks the fine broken teeth from the sample and enhances the quality of the training among the current four common neural networks, namely, alexnet, googlenet, vgg19, and resnet50, experimental results show that the proposed alexnet model in this study for restoration and caries judgments has an accuracy as high as 9556% and 9030%, respectively these are promising results that lead to the possibility of developing an automatic judgment method of bitewing film</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34369</th>\n",
       "      <td>three-dimensional mandibular motion trajectory-tracking system based on bp neural network the aim of this study was to develop a prototype three-dimensional optical motion capture system based on binocular stereo vision, back-propagation bp neural network and 3d compen-sation method for accurate and real-time recording of mandibular movement a specialized 3d method of compensation to eliminate the involuntary vibration motions by human heart beating and respiration a kind of binocular visual 3d measurement method based on projection line and a calibration method based on bp neural network is proposed to solve the problem of the high complexity of camera calibration process and the low accuracy of 3d measurement the accuracy of the proposed system is systematically evaluated by means of electric platform and clinical trials, and the root-mean-square is 00773 mm finally, comparisons with state-of-the-art methods demonstrate that our system has higher reliability and accuracy meanwhile, the motion trajectory-tracking system is expected to be used in the diagnosis of clinical oral diseases and digital design of restoration</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143555</th>\n",
       "      <td>construction of a dental caries prediction model by data mining recently, the distribution of dental caries has been shown to be skewed, and precise prediction models cannot be obtained using all the data we applied a balancing technique to obtain more appropriate and robust models, and compared their accuracy with that of the conventional model the data were obtained from annual oral check-ups for schoolchildren conducted in japan five hundred children were followed from ages 5 to 8, and the three-year follow-up data were used the variables used were salivary levels of mutans streptococci and lactobacilli, 3-min stimulated saliva volume, salivary ph, fluoride usage, and frequency of consumption of sweet snacks and beverages initially, conventional models were constructed by logistic regression analysis, neural network a kind of prediction method, and decision analysis next, the balancing technique was used to construct new models, we randomly sampled the same number of subjects with and without new dental caries by repeated sampling, 10 models were constructed for each method application of the balancing technique resulted in the most robust model, with 073 sensitivity and 077 specificity obtained by c 50 analysis for data with a skewed distribution, the balancing method could be one of the important techniques for obtaining a suitable and robust prediction model for dental caries</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>development of an exosomal gene signature to detect residual disease in dogs with osteosarcoma using a novel xenograft platform and machine learning osteosarcoma has a guarded prognosis a major hurdle in developing more effective osteosarcoma therapies is the lack of disease-specific biomarkers to predict risk, prognosis, or therapeutic response exosomes are secreted extracellular microvesicles emerging as powerful diagnostic tools however, their clinical application is precluded by challenges in identifying disease-associated cargo from the vastly larger background of normal exosome cargo we developed a method using canine osteosarcoma in mouse xenografts to distinguish tumor-derived from host-response exosomal messenger rnas mrnas the model allows for the identification of canine osteosarcoma-specific gene signatures by rna sequencing and a species-differentiating bioinformatics pipeline an osteosarcoma-associated signature consisting of five gene transcripts ska2, neu1, paf1, psmg2, and nob1 was validated in dogs with spontaneous osteosarcoma by real-time quantitative reverse transcription pcr qrt-pcr, while a machine learning model assigned dogs into healthy or disease groups serum/plasma exosomes were isolated from 53 dogs in distinct clinical groups healthy, osteosarcoma, other bone tumor, or non-neoplastic disease pre-treatment samples from osteosarcoma cases were used as the training set, and a validation set from post-treatment samples was used for testing, classifying as osteosarcoma detected or osteosarcoma-not detected dogs in a validation set whose post-treatment samples were classified as osteosarcoma-not detected had longer remissions, up to 15 months after treatment in conclusion, we identified a gene signature predictive of molecular remissions with potential applications in the early detection and minimal residual disease settings these results provide proof of concept for our discovery platform and its utilization in future studies to inform cancer risk, diagnosis, prognosis, and therapeutic response</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           text  \\\n",
       "1562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      artificial intelligence and infrared thermography as auxiliary tools in the diagnosis of temporomandibular disorder to assess three machine learning ml attribute extraction methods: radiomic, semantic and radiomic-semantic association on temporomandibular disorder tmd detection using infrared thermography it; and to determine which ml classifier, knn, svm and mlp, is the most efficient for this purpose   \n",
       "9362    caries and restoration detection using bitewing film based on transfer learning with cnns caries is a dental disease caused by bacterial infection if the cause of the caries is detected early, the treatment will be relatively easy, which in turn prevents caries from spreading the current common procedure of dentists is to first perform radiographic examination on the patient and mark the lesions manually however, the work of judging lesions and markings requires professional experience and is very time-consuming and repetitive taking advantage of the rapid development of artificial intelligence imaging research and technical methods will help dentists make accurate markings and improve medical treatments it can also shorten the judgment time of professionals in addition to the use of gaussian high-pass filter and otsus threshold image enhancement technology, this research solves the problem that the original cutting technology cannot extract certain single teeth, and it proposes a caries and lesions area analysis model based on convolutional neural networks cnn, which can identify caries and restorations from the bitewing images moreover, it provides dentists with more accurate objective judgment data to achieve the purpose of automatic diagnosis and treatment planning as a technology for assisting precision medicine a standardized database established following a defined set of steps is also proposed in this study there are three main steps to generate the image of a single tooth from a bitewing image, which can increase the accuracy of the analysis model the steps include 1 preprocessing of the dental image to obtain a high-quality binarization, 2 a dental image cropping procedure to obtain individually separated tooth samples, and 3 a dental image masking step which masks the fine broken teeth from the sample and enhances the quality of the training among the current four common neural networks, namely, alexnet, googlenet, vgg19, and resnet50, experimental results show that the proposed alexnet model in this study for restoration and caries judgments has an accuracy as high as 9556% and 9030%, respectively these are promising results that lead to the possibility of developing an automatic judgment method of bitewing film   \n",
       "34369                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          three-dimensional mandibular motion trajectory-tracking system based on bp neural network the aim of this study was to develop a prototype three-dimensional optical motion capture system based on binocular stereo vision, back-propagation bp neural network and 3d compen-sation method for accurate and real-time recording of mandibular movement a specialized 3d method of compensation to eliminate the involuntary vibration motions by human heart beating and respiration a kind of binocular visual 3d measurement method based on projection line and a calibration method based on bp neural network is proposed to solve the problem of the high complexity of camera calibration process and the low accuracy of 3d measurement the accuracy of the proposed system is systematically evaluated by means of electric platform and clinical trials, and the root-mean-square is 00773 mm finally, comparisons with state-of-the-art methods demonstrate that our system has higher reliability and accuracy meanwhile, the motion trajectory-tracking system is expected to be used in the diagnosis of clinical oral diseases and digital design of restoration   \n",
       "143555                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             construction of a dental caries prediction model by data mining recently, the distribution of dental caries has been shown to be skewed, and precise prediction models cannot be obtained using all the data we applied a balancing technique to obtain more appropriate and robust models, and compared their accuracy with that of the conventional model the data were obtained from annual oral check-ups for schoolchildren conducted in japan five hundred children were followed from ages 5 to 8, and the three-year follow-up data were used the variables used were salivary levels of mutans streptococci and lactobacilli, 3-min stimulated saliva volume, salivary ph, fluoride usage, and frequency of consumption of sweet snacks and beverages initially, conventional models were constructed by logistic regression analysis, neural network a kind of prediction method, and decision analysis next, the balancing technique was used to construct new models, we randomly sampled the same number of subjects with and without new dental caries by repeated sampling, 10 models were constructed for each method application of the balancing technique resulted in the most robust model, with 073 sensitivity and 077 specificity obtained by c 50 analysis for data with a skewed distribution, the balancing method could be one of the important techniques for obtaining a suitable and robust prediction model for dental caries   \n",
       "4494                                                                                                                                                                                                                     development of an exosomal gene signature to detect residual disease in dogs with osteosarcoma using a novel xenograft platform and machine learning osteosarcoma has a guarded prognosis a major hurdle in developing more effective osteosarcoma therapies is the lack of disease-specific biomarkers to predict risk, prognosis, or therapeutic response exosomes are secreted extracellular microvesicles emerging as powerful diagnostic tools however, their clinical application is precluded by challenges in identifying disease-associated cargo from the vastly larger background of normal exosome cargo we developed a method using canine osteosarcoma in mouse xenografts to distinguish tumor-derived from host-response exosomal messenger rnas mrnas the model allows for the identification of canine osteosarcoma-specific gene signatures by rna sequencing and a species-differentiating bioinformatics pipeline an osteosarcoma-associated signature consisting of five gene transcripts ska2, neu1, paf1, psmg2, and nob1 was validated in dogs with spontaneous osteosarcoma by real-time quantitative reverse transcription pcr qrt-pcr, while a machine learning model assigned dogs into healthy or disease groups serum/plasma exosomes were isolated from 53 dogs in distinct clinical groups healthy, osteosarcoma, other bone tumor, or non-neoplastic disease pre-treatment samples from osteosarcoma cases were used as the training set, and a validation set from post-treatment samples was used for testing, classifying as osteosarcoma detected or osteosarcoma-not detected dogs in a validation set whose post-treatment samples were classified as osteosarcoma-not detected had longer remissions, up to 15 months after treatment in conclusion, we identified a gene signature predictive of molecular remissions with potential applications in the early detection and minimal residual disease settings these results provide proof of concept for our discovery platform and its utilization in future studies to inform cancer risk, diagnosis, prognosis, and therapeutic response   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "1562          0       0       0           0          0        0       0   \n",
       "9362          0       0       1           0          0        0       0   \n",
       "34369         0       0       0           0          0        0       0   \n",
       "143555        0       0       1           0          0        0       0   \n",
       "4494          0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "1562             0            0         0           0        0       0   \n",
       "9362             0            0         0           0        0       0   \n",
       "34369            0            0         0           0        0       0   \n",
       "143555           0            0         0           0        0       0   \n",
       "4494             0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "1562             0             0           0            0         0   \n",
       "9362             0             0           0            0         0   \n",
       "34369            0             0           0            0         0   \n",
       "143555           0             0           0            0         0   \n",
       "4494             0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "1562            0           0            0           0            0   \n",
       "9362            0           0            0           0            0   \n",
       "34369           0           0            0           0            0   \n",
       "143555          0           0            0           0            0   \n",
       "4494            0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "1562            0            0        0         0          0       0        0   \n",
       "9362            0            0        0         0          0       0        0   \n",
       "34369           0            0        0         0          0       0        0   \n",
       "143555          0            0        0         0          0       0        0   \n",
       "4494            0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "1562           0          0        0       0         0          0        0   \n",
       "9362           0          0        0       0         0          0        0   \n",
       "34369          0          0        0       0         0          0        0   \n",
       "143555         0          0        0       0         0          0        0   \n",
       "4494           0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "1562             0         0        0        0       0           0         0   \n",
       "9362             0         0        0        0       0           0         0   \n",
       "34369            0         0        0        0       0           0         0   \n",
       "143555           0         0        0        0       0           0         0   \n",
       "4494             0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text haem_text obs_text  \\\n",
       "1562         0            0           0        0         0        0   \n",
       "9362         0            0           0        0         0        0   \n",
       "34369        0            0           0        1         0        0   \n",
       "143555       0            0           0        0         0        0   \n",
       "4494         0            0           0        0         0        0   \n",
       "\n",
       "       renal_text ackd_text paeds_text dent_text  \n",
       "1562            0         0          0         1  \n",
       "9362            0         0          0         1  \n",
       "34369           0         0          0         1  \n",
       "143555          0         0          1         1  \n",
       "4494            0         0          0         1  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['dent_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b039222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34020, '1': 159})\n"
     ]
    }
   ],
   "source": [
    "## AUDIOLOGY [C09] / audio\n",
    "\n",
    "## text\n",
    "text = ['audiology', ' ear disease', 'earache', 'labyrinth', 'otitis', 'otosclerosis', 'cochlear', 'tympanic memb',\n",
    "       'otoscop', 'acoustic neuroma', 'meniere', 'hearing loss', 'hearing impairment', 'cholesteatoma', 'otoacoustic', 'deafness', ' deaf ',\n",
    "       'middle ear', 'outer ear', 'inner ear', 'otolog', 'paroxysmal positional vertigo']\n",
    "\n",
    "spec['audio_text'] = np.where(groups['text'].str.contains('hearing aid'), \"1\", \"0\")\n",
    "\n",
    "for x in text:\n",
    "    spec['audio_text'] = np.where(groups['text'].str.contains(x), \"1\", spec['audio_text']) #if yes then 1, if no, keep current\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['audio_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4229ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>the use of explainable artificial intelligence to explore types of fenestral otosclerosis misdiagnosed when using temporal bone high-resolution computed tomography the purpose of this study was to explore the common characteristics of fenestral otosclerosis os which are misdiagnosed, and develop a deep learning model for the diagnosis of fenestral os based on temporal bone high-resolution computed tomography scans</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166658</th>\n",
       "      <td>the application of bionic wavelet transform to speech signal processing in cochlear implants using neural network simulations cochlear implants cis restore partial hearing to people with severe to profound sensorineural deafness; but there is still a marked performance gap in speech recognition between those who have received cochlear implant and people with a normal hearing capability one of the factors that may lead to this performance gap is the inadequate signal processing method used in cis this paper investigates the application of an improved signal-processing method called bionic wavelet transform bwt this method is based upon the auditory model and allows for signal processing comparing the neural network simulations on the same experimental materials processed by wavelet transform wt and bwt, the application of bwt to speech signal processing in ci has a number of advantages, including: improvement in recognition rates for both consonants and vowels, reduction of the number of required channels, reduction of the average stimulation duration for words, and high noise tolerance consonant recognition results in 15 normal hearing subjects show that the bwt produces significantly better performance than the wt t = -436276, p = 000065 the bwt has great potential to reduce the performance gap between ci listeners and people with a normal hearing capability in the future</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>environmental noise classification with inception-dense blocks for hearing aids hearing aids are increasingly essential for people with hearing loss for this purpose, environmental noise estimation and classification are some of the required technologies however, some noise classifiers utilize multiple audio features, which cause intense computation in addition, such noise classifiers employ inputs of different time lengths, which may affect classification performance thus, this paper proposes a model architecture for noise classification, and performs experiments with three different audio segment time lengths the proposed model attains fewer floating-point operations and parameters by utilizing the log-scaled mel-spectrogram as an input feature the proposed models are evaluated with classification accuracy, computational complexity, trainable parameters, and inference time on the urbansound8k dataset and hans dataset the experimental results showed that the proposed model outperforms other models on two datasets furthermore, compared with other models, the proposed model reduces model complexity and inference time while maintaining classification accuracy as a result, the proposed noise classification for hearing aids offers less computational complexity without compromising performance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103805</th>\n",
       "      <td>coregistered photoacoustic and ultrasound imaging and classification of ovarian cancer: ex vivo and in vivo studies most ovarian cancers are diagnosed at advanced stages due to the lack of efficacious screening techniques photoacoustic tomography pat has a potential to image tumor angiogenesis and detect early neovascular changes of the ovary we have developed a coregistered pat and ultrasound us prototype system for real-time assessment of ovarian masses features extracted from pat and us angular beams, envelopes, and images were input to a logistic classifier and a support vector machine svm classifier to diagnose ovaries as benign or malignant a total of 25 excised ovaries of 15 patients were studied and the logistic and svm classifiers achieved sensitivities of 704 and 877%, and specificities of 956 and 979%, respectively furthermore, the ovaries of two patients were noninvasively imaged using the pat/us system before surgical excision by using five significant features and the logistic classifier, 12 out of 14 images 86% sensitivity from a malignant ovarian mass and all 17 images 100% specificity from a benign mass were accurately classified; the svm correctly classified 10 out of 14 malignant images 71% sensitivity and all 17 benign images 100% specificity these initial results demonstrate the clinical potential of the pat/us technique for ovarian cancer diagnosis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12219</th>\n",
       "      <td>identifying genetic risk variants associated with noise-induced hearing loss based on a novel strategy for evaluating individual susceptibility the overall genetic profile for noise-induced hearing loss nihl remains elusive herein we proposed a novel machine learning ml based strategy to evaluate individual susceptibility to nihl and identify the underlying genetic risk variants based on a subsample of participants with extreme phenotypes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       text  \\\n",
       "9512                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      the use of explainable artificial intelligence to explore types of fenestral otosclerosis misdiagnosed when using temporal bone high-resolution computed tomography the purpose of this study was to explore the common characteristics of fenestral otosclerosis os which are misdiagnosed, and develop a deep learning model for the diagnosis of fenestral os based on temporal bone high-resolution computed tomography scans   \n",
       "166658  the application of bionic wavelet transform to speech signal processing in cochlear implants using neural network simulations cochlear implants cis restore partial hearing to people with severe to profound sensorineural deafness; but there is still a marked performance gap in speech recognition between those who have received cochlear implant and people with a normal hearing capability one of the factors that may lead to this performance gap is the inadequate signal processing method used in cis this paper investigates the application of an improved signal-processing method called bionic wavelet transform bwt this method is based upon the auditory model and allows for signal processing comparing the neural network simulations on the same experimental materials processed by wavelet transform wt and bwt, the application of bwt to speech signal processing in ci has a number of advantages, including: improvement in recognition rates for both consonants and vowels, reduction of the number of required channels, reduction of the average stimulation duration for words, and high noise tolerance consonant recognition results in 15 normal hearing subjects show that the bwt produces significantly better performance than the wt t = -436276, p = 000065 the bwt has great potential to reduce the performance gap between ci listeners and people with a normal hearing capability in the future   \n",
       "5540                                                                                          environmental noise classification with inception-dense blocks for hearing aids hearing aids are increasingly essential for people with hearing loss for this purpose, environmental noise estimation and classification are some of the required technologies however, some noise classifiers utilize multiple audio features, which cause intense computation in addition, such noise classifiers employ inputs of different time lengths, which may affect classification performance thus, this paper proposes a model architecture for noise classification, and performs experiments with three different audio segment time lengths the proposed model attains fewer floating-point operations and parameters by utilizing the log-scaled mel-spectrogram as an input feature the proposed models are evaluated with classification accuracy, computational complexity, trainable parameters, and inference time on the urbansound8k dataset and hans dataset the experimental results showed that the proposed model outperforms other models on two datasets furthermore, compared with other models, the proposed model reduces model complexity and inference time while maintaining classification accuracy as a result, the proposed noise classification for hearing aids offers less computational complexity without compromising performance   \n",
       "103805     coregistered photoacoustic and ultrasound imaging and classification of ovarian cancer: ex vivo and in vivo studies most ovarian cancers are diagnosed at advanced stages due to the lack of efficacious screening techniques photoacoustic tomography pat has a potential to image tumor angiogenesis and detect early neovascular changes of the ovary we have developed a coregistered pat and ultrasound us prototype system for real-time assessment of ovarian masses features extracted from pat and us angular beams, envelopes, and images were input to a logistic classifier and a support vector machine svm classifier to diagnose ovaries as benign or malignant a total of 25 excised ovaries of 15 patients were studied and the logistic and svm classifiers achieved sensitivities of 704 and 877%, and specificities of 956 and 979%, respectively furthermore, the ovaries of two patients were noninvasively imaged using the pat/us system before surgical excision by using five significant features and the logistic classifier, 12 out of 14 images 86% sensitivity from a malignant ovarian mass and all 17 images 100% specificity from a benign mass were accurately classified; the svm correctly classified 10 out of 14 malignant images 71% sensitivity and all 17 benign images 100% specificity these initial results demonstrate the clinical potential of the pat/us technique for ovarian cancer diagnosis   \n",
       "12219                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            identifying genetic risk variants associated with noise-induced hearing loss based on a novel strategy for evaluating individual susceptibility the overall genetic profile for noise-induced hearing loss nihl remains elusive herein we proposed a novel machine learning ml based strategy to evaluate individual susceptibility to nihl and identify the underlying genetic risk variants based on a subsample of participants with extreme phenotypes   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "9512          0       0       0           0          0        0       0   \n",
       "166658        0       0       0           0          0        0       0   \n",
       "5540          0       0       0           0          0        1       0   \n",
       "103805        0       0       0           0          0        0       0   \n",
       "12219         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "9512             0            0         0           0        0       0   \n",
       "166658           0            0         0           0        0       0   \n",
       "5540             0            0         0           0        0       0   \n",
       "103805           0            0         0           0        1       0   \n",
       "12219            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "9512             0             0           0            0         0   \n",
       "166658           0             0           0            0         0   \n",
       "5540             0             0           0            0         0   \n",
       "103805           0             0           0            0         0   \n",
       "12219            0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "9512            0           0            0           0            0   \n",
       "166658          0           0            0           0            0   \n",
       "5540            0           0            0           0            0   \n",
       "103805          0           0            0           1            0   \n",
       "12219           0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "9512            0            0        0         0          0       0        0   \n",
       "166658          0            0        0         0          0       0        0   \n",
       "5540            0            0        0         0          0       0        0   \n",
       "103805          0            0        0         0          0       0        0   \n",
       "12219           0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "9512           0          0        0       0         0          0        0   \n",
       "166658         0          0        0       0         0          0        0   \n",
       "5540           0          0        0       0         0          0        0   \n",
       "103805         0          0        0       0         0          0        0   \n",
       "12219          0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "9512             0         0        0        0       0           0         0   \n",
       "166658           0         0        0        0       0           0         0   \n",
       "5540             0         0        0        0       0           0         0   \n",
       "103805           0         0        0        0       0           0         0   \n",
       "12219            0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text haem_text obs_text  \\\n",
       "9512         0            0           0        0         0        0   \n",
       "166658       0            0           0        0         0        0   \n",
       "5540         0            0           0        0         0        0   \n",
       "103805       0            0           0        0         0        1   \n",
       "12219        0            0           0        0         0        0   \n",
       "\n",
       "       renal_text ackd_text paeds_text dent_text audio_text  \n",
       "9512            0         0          0         0          1  \n",
       "166658          0         0          0         0          1  \n",
       "5540            0         0          0         0          1  \n",
       "103805          0         0          0         0          1  \n",
       "12219           0         0          0         0          1  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['audio_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "527f953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34079, '1': 100})\n"
     ]
    }
   ],
   "source": [
    "## BRAIN COMPUTER / bci\n",
    "\n",
    "## text\n",
    "spec['bci_text'] = np.where(groups['text'].str.contains(\"brain control\"), \"1\", \"0\")\n",
    "spec['bci_text'] = np.where(groups['text'].str.contains(\"brain computer\"), \"1\", \"0\")\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['bci_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "93e6f3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33915, '1': 264})\n"
     ]
    }
   ],
   "source": [
    "## PROSTHESIS CONTROL / prosth\n",
    "\n",
    "## text\n",
    "spec['prosth_text'] = np.where(groups['text'].str.contains(\"prosthetic\"), \"1\", \"0\")\n",
    "spec['prosth_text'] = np.where(groups['text'].str.contains(\"prosthesis\"), \"1\", spec['prosth_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['prosth_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3dce9999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>bci_text</th>\n",
       "      <th>prosth_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62466</th>\n",
       "      <td>applying deep artificial neural network approach to maxillofacial prostheses coloration maxillofacial prosthetic rehabilitation replaces missing structures to recover the function and aesthetics relating to facial defects or injuries deep learning is rapidly expanding with respect to applications in medical fields in this study, we apply the artificial neural network ann-based deep learning approach to coloration support for fabricating maxillofacial prostheses</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32881</th>\n",
       "      <td>development of a shoulder disarticulation prosthesis system intuitively controlled with the trunk surface electromyogram we developed an intuitively operational shoulder disarticulation prosthesis system that can be used without long-term training the developed system consisted of four degrees of freedom joints, as well as a user adapting control system based on a machine learning technique and surface electromyogram emg of the trunk we measured the surface emg of the trunk of healthy subjects at multiple points and analyzed through principal component analysis to identify the proper emg measurement portion of the trunk, which was determined to be distributed in the chest and back additionally, evaluation experiments demonstrated the capability of four healthy subjects to grasp and move objects in the horizontal as well as the vertical directions, using our developed system controlled via the emg of the chest and back moreover, we also quantitatively confirmed the ability of a bilateral shoulder disarticulation amputee to complete the evaluation experiment similar to healthy subjects</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39944</th>\n",
       "      <td>automated detection of periprosthetic joint infections and data elements using natural language processing periprosthetic joint infection pji data elements are contained in both structured and unstructured documents in electronic health records and require manual data collection the goal of this study is to develop a natural language processing nlp algorithm to replicate manual chart review for pji data elements</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142817</th>\n",
       "      <td>decoding of individuated finger movements using surface electromyography upper limb prostheses are increasingly resembling the limbs they seek to replace in both form and functionality, including the design and development of multifingered hands and wrists hence, it becomes necessary to control large numbers of degrees of freedom dofs, required for individuated finger movements, preferably using noninvasive signals while existing control paradigms are typically used to drive a single-dof hook-based configurations, dexterous tasks such as individual finger movements would require more elaborate control schemes we show that it is possible to decode individual flexion and extension movements of each finger ten movements with greater than 90% accuracy in a transradial amputee using only noninvasive surface myoelectric signals further, comparison of decoding accuracy from a transradial amputee and able-bodied subjects shows no statistically significant difference  p &lt; 005 between these subjects these results are encouraging for the development of real-time control strategies based on the surface myoelectric signal to control dexterous prosthetic hands</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149948</th>\n",
       "      <td>real-time intelligent pattern recognition algorithm for surface emg signals electromyography emg is the study of muscle function through the inquiry of electrical signals that the muscles emanate emg signals collected from the surface of the skin surface electromyogram: semg can be used in different applications such as recognizing musculoskeletal neural based patterns intercepted for hand prosthesis movements current systems designed for controlling the prosthetic hands either have limited functions or can only be used to perform simple movements or use excessive amount of electrodes in order to achieve acceptable results in an attempt to overcome these problems we have proposed an intelligent system to recognize hand movements and have provided a user assessment routine to evaluate the correctness of executed movements</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "62466                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              applying deep artificial neural network approach to maxillofacial prostheses coloration maxillofacial prosthetic rehabilitation replaces missing structures to recover the function and aesthetics relating to facial defects or injuries deep learning is rapidly expanding with respect to applications in medical fields in this study, we apply the artificial neural network ann-based deep learning approach to coloration support for fabricating maxillofacial prostheses   \n",
       "32881                                                                   development of a shoulder disarticulation prosthesis system intuitively controlled with the trunk surface electromyogram we developed an intuitively operational shoulder disarticulation prosthesis system that can be used without long-term training the developed system consisted of four degrees of freedom joints, as well as a user adapting control system based on a machine learning technique and surface electromyogram emg of the trunk we measured the surface emg of the trunk of healthy subjects at multiple points and analyzed through principal component analysis to identify the proper emg measurement portion of the trunk, which was determined to be distributed in the chest and back additionally, evaluation experiments demonstrated the capability of four healthy subjects to grasp and move objects in the horizontal as well as the vertical directions, using our developed system controlled via the emg of the chest and back moreover, we also quantitatively confirmed the ability of a bilateral shoulder disarticulation amputee to complete the evaluation experiment similar to healthy subjects   \n",
       "39944                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                automated detection of periprosthetic joint infections and data elements using natural language processing periprosthetic joint infection pji data elements are contained in both structured and unstructured documents in electronic health records and require manual data collection the goal of this study is to develop a natural language processing nlp algorithm to replicate manual chart review for pji data elements   \n",
       "142817  decoding of individuated finger movements using surface electromyography upper limb prostheses are increasingly resembling the limbs they seek to replace in both form and functionality, including the design and development of multifingered hands and wrists hence, it becomes necessary to control large numbers of degrees of freedom dofs, required for individuated finger movements, preferably using noninvasive signals while existing control paradigms are typically used to drive a single-dof hook-based configurations, dexterous tasks such as individual finger movements would require more elaborate control schemes we show that it is possible to decode individual flexion and extension movements of each finger ten movements with greater than 90% accuracy in a transradial amputee using only noninvasive surface myoelectric signals further, comparison of decoding accuracy from a transradial amputee and able-bodied subjects shows no statistically significant difference  p < 005 between these subjects these results are encouraging for the development of real-time control strategies based on the surface myoelectric signal to control dexterous prosthetic hands   \n",
       "149948                                                                                                                                                                                                                                                                                                                                              real-time intelligent pattern recognition algorithm for surface emg signals electromyography emg is the study of muscle function through the inquiry of electrical signals that the muscles emanate emg signals collected from the surface of the skin surface electromyogram: semg can be used in different applications such as recognizing musculoskeletal neural based patterns intercepted for hand prosthesis movements current systems designed for controlling the prosthetic hands either have limited functions or can only be used to perform simple movements or use excessive amount of electrodes in order to achieve acceptable results in an attempt to overcome these problems we have proposed an intelligent system to recognize hand movements and have provided a user assessment routine to evaluate the correctness of executed movements   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "62466         0       0       0           0          0        0       0   \n",
       "32881         0       0       0           0          0        0       0   \n",
       "39944         0       0       1           0          0        0       0   \n",
       "142817        0       0       0           0          0        0       0   \n",
       "149948        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "62466            0            0         0           0        0       0   \n",
       "32881            0            0         0           0        0       0   \n",
       "39944            0            0         0           0        0       0   \n",
       "142817           0            0         0           0        0       0   \n",
       "149948           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "62466            0             0           0            0         0   \n",
       "32881            0             0           0            0         0   \n",
       "39944            0             0           0            0         0   \n",
       "142817           0             0           0            0         0   \n",
       "149948           0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "62466           0           0            0           0            0   \n",
       "32881           0           0            0           0            0   \n",
       "39944           0           0            0           0            0   \n",
       "142817          0           0            0           0            0   \n",
       "149948          0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "62466           0            0        0         0          0       0        0   \n",
       "32881           0            0        0         0          0       0        0   \n",
       "39944           0            0        0         0          0       0        0   \n",
       "142817          0            0        0         0          0       0        0   \n",
       "149948          0            0        1         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "62466          0          0        0       0         0          0        0   \n",
       "32881          0          0        0       0         0          0        0   \n",
       "39944          0          0        0       0         0          0        0   \n",
       "142817         0          0        0       0         0          0        0   \n",
       "149948         0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "62466            0         0        0        0       0           0         0   \n",
       "32881            0         0        0        0       0           0         0   \n",
       "39944            0         0        0        0       0           0         0   \n",
       "142817           0         0        0        0       0           0         0   \n",
       "149948           0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text haem_text obs_text  \\\n",
       "62466        0            0           0        0         0        0   \n",
       "32881        0            0           0        0         0        0   \n",
       "39944        0            0           0        0         0        0   \n",
       "142817       0            0           0        0         0        0   \n",
       "149948       0            0           0        0         0        0   \n",
       "\n",
       "       renal_text ackd_text paeds_text dent_text audio_text bci_text  \\\n",
       "62466           0         0          0         1          0        0   \n",
       "32881           0         0          0         0          0        0   \n",
       "39944           0         0          0         0          0        0   \n",
       "142817          0         0          0         0          0        0   \n",
       "149948          0         0          0         0          0        0   \n",
       "\n",
       "       prosth_text  \n",
       "62466            1  \n",
       "32881            1  \n",
       "39944            1  \n",
       "142817           1  \n",
       "149948           1  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['prosth_text']=='1'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b5ecf990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 34109, '1': 70})\n"
     ]
    }
   ],
   "source": [
    "## ASSISTIVE DEVICE CONTROL / assist\n",
    "\n",
    "## text\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"wheelchair\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"scooter\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"mobility device\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"assistive device\"), \"1\", \"0\")\n",
    "spec['assist_text'] = np.where(groups['text'].str.contains(\"exoskeleton\"), \"1\", \"0\")\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['assist_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bca0a02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>bci_text</th>\n",
       "      <th>prosth_text</th>\n",
       "      <th>assist_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88354</th>\n",
       "      <td>implementation of a surface electromyography-based upper extremity exoskeleton controller using learning from demonstration upper-extremity exoskeletons have demonstrated potential as augmentative, assistive, and rehabilitative devices typical control of upper-extremity exoskeletons have relied on switches, force/torque sensors, and surface electromyography semg, but these systems are usually reactionary, and/or rely on entirely hand-tuned parameters semg-based systems may be able to provide anticipatory control, since they interface directly with muscle signals, but typically require expert placement of sensors on muscle bodies we present an implementation of an adaptive semg-based exoskeleton controller that learns a mapping between muscle activation and the desired system state during interaction with a user, generating a personalized semg feature classifier to allow for anticipatory control this system is robust to novice placement of semg sensors, as well as subdermal muscle shifts we validate this method with 18 subjects using a thumb exoskeleton to complete a book-placement task this learning-from-demonstration system for exoskeleton control allows for very short training times, as well as the potential for improvement in intent recognition over time, and adaptation to physiological changes in the user, such as those due to fatigue</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65390</th>\n",
       "      <td>rnn-based on-line continuous gait phase estimation from shank-mounted imus to control ankle exoskeletons several research groups have developed and studied powered ankle exoskeletons to improve energetics of healthy subjects and the mobility of elderly subjects, or to reduce asymmetry in gaits induced by strokes to achieve optimal effect, the timing of assistive torque has been proved to be of crucial importance previous studies estimated the onset timings mostly by extrapolating the time horizon from past gait events observed with sensors such methods have inherently limited performance when subjects are not walking at steady frequencies to overcome such limitation and allow the use of exoskeletons in various scenarios in a daily life, we propose to estimate the gait phase as a continuous variable progressing over a gait cycle, hence allowing immediate response to frequency changes rather than iteratively correcting it after each cycle our method uses recurrent neural networks to estimate gait phases out of an inertial measurement unit imu every 10 ms by replacing foot sensors with an imu we can obtain rich enough information to estimate gait phase continuously as well as avoid physical damage in sensors from ground impacts our preliminary tests with 2 healthy subjects showed qualitatively positive outcomes regarding the gait phase estimation and the assistive torque control</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39334</th>\n",
       "      <td>continuous estimation of knee joint angle based on surface electromyography using a long short-term memory neural network and time-advanced feature continuous joint angle estimation based on a surface electromyography semg signal can be used to improve the man-machine coordination performance of the exoskeleton in this study, we proposed a time-advanced feature and utilized long short-term memory lstm with a root mean square rms feature and its time-advanced feature rmstaf; collectively referred to as rrtaf of semg to estimate the knee joint angle to evaluate the effect of joint angle estimation, we used root mean square error rmse and cross-correlation coefficient &lt;i&gt;ρ&lt;/i&gt; between the estimated angle and actual angle we also compared three methods ie, lstm using rms, bpnn back propagation neural network using rrtaf, and bpnn using rms with lstm using rrtaf to highlight its good performance five healthy subjects participated in the experiment and their eight muscle ie, rectus femoris rf, biceps femoris bf, semitendinosus st, gracilis gc, semimembranosus sm, sartorius sr, medial gastrocnemius mg, and tibialis anterior ta semg signals were taken as algorithm inputs moreover, the knee joint angles were used as target values the experimental results showed that, compared with lstm using rms, bpnn using rrtaf, and bpnn using rms, the average rmse values of lstm using rrtaf were respectively reduced by 857%, 4662%, and 6869%, whereas the average &lt;i&gt;ρ&lt;/i&gt; values were respectively increased by 031%, 415%, and 1835% the results demonstrated that lstm using rrtaf, which contained the time-advanced feature, had better performance for estimating the knee joint motion</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55778</th>\n",
       "      <td>prediction of plantar forces during gait using wearable sensors and deep neural networks&lt;sup&gt;&lt;/sup&gt; to enable on-time and high-fidelity lower-limb exoskeleton control, it is effective to predict the future human motion from the observed status in this research, we propose a novel method to predict future plantar force during the gait using imu and plantar sensors deep neural networks dnn are used to learn the non-linear relationship between the measured sensor data and the future plantar force data using the trained network, we can predict the plantar force not only during walking but also at the start and end of walking in the experiments, the performance of the proposed method is confirmed for different prediction time</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96518</th>\n",
       "      <td>classifying three imaginary states of the same upper extremity using time-domain features brain-computer interface bci allows collaboration between humans and machines it translates the electrical activity of the brain to understandable commands to operate a machine or a device in this study, we propose a method to improve the accuracy of a 3-class bci using electroencephalographic eeg signals this bci discriminates rest against imaginary grasps and elbow movements of the same limb this classification task is challenging because imaginary movements within the same limb have close spatial representations on the motor cortex area the proposed method extracts time-domain features and classifies them using a support vector machine svm with a radial basis kernel function rbf an average accuracy of 742% was obtained when using the proposed method on a dataset collected, prior to this study, from 12 healthy individuals this accuracy was higher than that obtained when other widely used methods, such as common spatial patterns csp, filter bank csp fbcsp, and band power methods, were used on the same dataset these results are encouraging and the proposed method could potentially be used in future applications including bci-driven robotic devices, such as a portable exoskeleton for the arm, to assist individuals with impaired upper extremity functions in performing daily tasks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101171</th>\n",
       "      <td>eeg classification for motor imagery and resting state in bci applications using multi-class adaboost extreme learning machine brain-computer interface bci systems provide an alternative communication and control approach for people with limited motor function therefore, the feature extraction and classification approach should differentiate the relative unusual state of motion intention from a common resting state in this paper, we sought a novel approach for multi-class classification in bci applications we collected electroencephalographic eeg signals registered by electrodes placed over the scalp during left hand motor imagery, right hand motor imagery, and resting state for ten healthy human subjects we proposed using the kolmogorov complexity kc for feature extraction and a multi-class adaboost classifier with extreme learning machine as base classifier for classification, in order to classify the three-class eeg samples an average classification accuracy of 795% was obtained for ten subjects, which greatly outperformed commonly used approaches thus, it is concluded that the proposed method could improve the performance for classification of motor imagery tasks for multi-class samples it could be applied in further studies to generate the control commands to initiate the movement of a robotic exoskeleton or orthosis, which finally facilitates the rehabilitation of disabled people</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55818</th>\n",
       "      <td>classification and transfer learning of eeg during a kinesthetic motor imagery task using deep convolutional neural networks the reliable classification of electroencephalography eeg signals is a crucial step towards making eeg-controlled non-invasive neuro-exoskeleton rehabilitation a practical reality eeg signals collected during motor imagery tasks have been proposed to act as a control signal for exoskeleton applications here, a deep convolutional neural network dcnn was optimized to classify a two-class kinesthetic motor imagery eeg dataset, leading to an optimized architecture consisting of four convolutional layers and three fully connected layers transfer learning, or the leveraging of data from past subjects to classify the intentions of a new subject, is important for rehabilitation as it helps to minimize the number of training sessions required from subjects who lack full motor functionality the transfer learning training paradigm investigated through this study utilized region criticality trends to reduce the number of new subject training sessions and increase the classification performance when compared against a single-subject non-transfer-learning classifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49131</th>\n",
       "      <td>characterization of forearm muscle activation in duchenne muscular dystrophy via high-density electromyography: a case study on the implications for myoelectric control duchenne muscular dystrophy dmd is a genetic disorder that results in progressive muscular degeneration although medical advances increased their life expectancy, dmd individuals are still highly dependent on caregivers hand/wrist function is central for providing independence, and robotic exoskeletons are good candidates for effectively compensating for deteriorating functionality robotic hand exoskeletons require the accurate decoding of motor intention typically via surface electromyography semg traditional low-density semg was used in the past to explore the muscular activations of individuals with dmd; however, it cannot provide high spatial resolution this study characterized, for the first time, the forearm high-density hd electromyograms of three individuals with dmd while performing seven hand/wrist-related tasks and compared them to eight healthy individuals all data available online we looked into the spatial distribution of hd-semg patterns by using principal component analysis pca and also assessed the repeatability and the amplitude distributions of muscle activity additionally, we used a machine learning approach to assess dmd individualspotentials for myocontrol our analysis showed that although participants with dmd were able to repeat similar hd-semg patterns across gestures similarly to healthy participants, a fewer number of electrodes was activated during their gestures compared to the healthy participants additionally, participants with dmd activated their muscles close to maximal contraction level 063 ± 023, whereas healthy participants had lower normalized activations 026 ± 02 lastly, participants with dmd showed on average fewer pcs 3, explaining 90% of the complete gesture space than the healthy 5 however, the ability of the dmd participants to produce repeatable hd-semg patterns was unexpectedly comparable to that of healthy participants, and the same holds true for their offline myocontrol performance, disproving our hypothesis and suggesting a clear potential for the myocontrol of wearable exoskeletons our findings present evidence for the first time on how dmd leads to progressive alterations in hand/wrist motor control in dmd individuals compared to healthy the better understanding of these alterations can lead to further developments for the intuitive and robust myoelectric control of active hand exoskeletons for individuals with dmd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60710</th>\n",
       "      <td>myoelectric control of a soft hand exoskeleton using kinematic synergies soft hand exoskeletons offer a lightweight, low-profile alternative to rigid rehabilitative robotic systems, enabling their use to restore activities of daily living adl in those with hand paresis due to stroke or other conditions the hand exoskeleton with embedded synergies hexoes is a soft cable-driven hand exoskeleton capable of independently actuating and sensing 10 degrees of freedom dof of the hand control of the 10 dof exoskeleton is dimensionally reduced using three manually defined synergies in software corresponding to thumb, index, and 3-finger flexion and extension in this paper, five healthy subjects control hexoes using a neural network which decodes synergy weights from contralateral electromyography emg activity the three synergies are manipulated in real time to grasp and lift 15 adl objects of various sizes and weights the neural networks training and validation mean squared error, object grasp time, and grasp success rate were measured for five healthy subjects the final training error of the neural network was 48 ± 18% averaged across subjects and tasks, with 83 ± 34% validation error the time to reach, grasp, and lift an object was 1115 ± 435 s on average, with an average success rate of 667% across all objects the complete system demonstrates real time use of biosignals and machine learning to allow subjects to operate kinematic synergies to grasp objects using a wearable hand exoskeleton future work and applications are further discussed, including possible design improvements and enrollment of individuals with stroke</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63793</th>\n",
       "      <td>sub-optimally solving actuator redundancy in a hybrid neuroprosthetic system with a multi-layer neural network structure functional electrical stimulation fes has recently been proposed as a supplementary torque assist in lower-limb powered exoskeletons for persons with paraplegia in the combined system, also known as a hybrid neuroprosthesis, both fes-assist and the exoskeleton act to generate lower-limb torques to achieve standing and walking functions due to this actuator redundancy, we are motivated to optimally allocate fes-assist and exoskeleton torque based on a performance index that penalizes fes overuse to minimize muscle fatigue while also minimizing regulation or tracking errors traditional optimal control approaches need a system model to optimize; however, it is often difficult to formulate a musculoskeletal model that accurately predicts muscle responses due to fes in this paper, we use a novel identification and control structure that contains a recurrent neural network rnn and several feedforward neural networks fnns the rnn is trained by supervised learning to identify the system dynamics, while the fnns are trained by a reinforcement learning method to provide sub-optimal control actions the output layer of each fnn has its unique activation functions, so that the asymmetric constraint of fes and the symmetric constraint of exoskeleton motor control input can be realized this new structure is experimentally validated on a seated human participant using a single joint hybrid neuroprosthesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121138</th>\n",
       "      <td>high accuracy decoding of user intentions using eeg to control a lower-body exoskeleton brain-machine interface bmi systems allow users to control external mechanical systems using their thoughts commonly used in literature are invasive techniques to acquire brain signals and decode users attempted motions to drive these systems eg a robotic manipulator in this work we use a lower-body exoskeleton and measure the users brain activity using non-invasive electroencephalography eeg the main focus of this study is to decode a paraplegic subjects motion intentions and provide him with the ability of walking with a lower-body exoskeleton accordingly we present our novel method of decoding with high offline evaluation accuracies around 98%, our closed loop implementation structure with considerably short on-site training time around 38 sec, and preliminary results from the real-time closed loop implementation neurorex with a paraplegic test subject</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118314</th>\n",
       "      <td>a fuzzy controller for lower limb exoskeletons during sit-to-stand and stand-to-sit movement using wearable sensors human motion is a daily and rhythmic activity the exoskeleton concept is a very positive scientific approach for human rehabilitation in case of lower limb impairment although the exoskeleton shows potential, it is not yet applied extensively in clinical rehabilitation in this research, a fuzzy based control algorithm is proposed for lower limb exoskeletons during sit-to-stand and stand-to-sit movements surface electromyograms emgs are acquired from the vastus lateralis muscle using a wearable emg sensor the resultant acceleration angle along the z-axis is determined from a kinematics sensor twenty volunteers were chosen to perform the experiments the whole experiment was accomplished in two phases in the first phase, acceleration angles and emg data were acquired from the volunteers during both sit-to-stand and stand-to-sit motions during sit-to-stand movements, the average acceleration angle at activation was 11°-48° and the emg varied from -019 mv to +019 mv on the other hand, during stand-to-sit movements, the average acceleration angle was found to be 575°-108° at the activation point and the emg varied from -032 mv to +032 mv in the second phase, a fuzzy controller was designed from the experimental data the controller was tested and validated with both offline and real time data using labview</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102371</th>\n",
       "      <td>eeg-based detection of starting and stopping during gait cycle walking is for humans an essential task in our daily life however, there is a huge and growing number of people who have this ability diminished or are not able to walk due to motor disabilities in this paper, a system to detect the start and the stop of the gait through electroencephalographic signals has been developed the system has been designed in order to be applied in the future to control a lower limb exoskeleton to help stroke or spinal cord injured patients during the gait the brain-machine interface bmi training has been optimized through a preliminary analysis using the brain information recorded during the experiments performed by three healthy subjects afterward, the system has been verified by other four healthy subjects and three patients in a real-time test in both preliminary optimization analysis and real-time tests, the results obtained are very similar the true positive rates are formula: see text and formula: see text respectively regarding the false positive per minute, the values are also very similar, decreasing from 266 in preliminary tests to 190 in real-time finally, the average latencies in the detection of the movement intentions are 794 and 798formula: see textms, preliminary and real-time tests respectively</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39967</th>\n",
       "      <td>a real-time stable-control gait switching strategy for lower-limb rehabilitation exoskeleton switching different gait according to different movements is an important direction in the study of exoskeleton robot identifying the movement intention of the wearer to control the gait planning of the exoskeleton robot can effectively improve the man-machine interaction experience after the exoskeleton this paper uses a support vector machine svm to realize wearers motion posture recognition by collecting semg signals on the human surface the moving gait of the exoskeleton is planned according to the recognition results, and the decoding intention signal controls gait switching meanwhile, the stability of the planned gait during the movement was analyzed experimental results show that the semg signal decoding human motion intentional, and control exoskeleton robot gait switching has good accuracy and real-time performance it helps patients to complete rehabilitation training more safely and quickly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107145</th>\n",
       "      <td>a neural network-based gait phase classification method using sensors equipped on lower limb exoskeleton robots an exact classification of different gait phases is essential to enable the control of exoskeleton robots and detect the intentions of users we propose a gait phase classification method based on neural networks using sensor signals from lower limb exoskeleton robots in such robots, foot sensors with force sensing registers are commonly used to classify gait phases we describe classifiers that use the orientation of each lower limb segment and the angular velocities of the joints to output the current gait phase experiments to obtain the input signals and desired outputs for the learning and validation process are conducted, and two neural network methods a multilayer perceptron and nonlinear autoregressive with external inputs narx are used to develop an optimal classifier offline and online evaluations using four criteria are used to compare the performance of the classifiers the proposed narx-based method exhibits sufficiently good performance to replace foot sensors as a means of classifying gait phases</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "88354                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   implementation of a surface electromyography-based upper extremity exoskeleton controller using learning from demonstration upper-extremity exoskeletons have demonstrated potential as augmentative, assistive, and rehabilitative devices typical control of upper-extremity exoskeletons have relied on switches, force/torque sensors, and surface electromyography semg, but these systems are usually reactionary, and/or rely on entirely hand-tuned parameters semg-based systems may be able to provide anticipatory control, since they interface directly with muscle signals, but typically require expert placement of sensors on muscle bodies we present an implementation of an adaptive semg-based exoskeleton controller that learns a mapping between muscle activation and the desired system state during interaction with a user, generating a personalized semg feature classifier to allow for anticipatory control this system is robust to novice placement of semg sensors, as well as subdermal muscle shifts we validate this method with 18 subjects using a thumb exoskeleton to complete a book-placement task this learning-from-demonstration system for exoskeleton control allows for very short training times, as well as the potential for improvement in intent recognition over time, and adaptation to physiological changes in the user, such as those due to fatigue   \n",
       "65390                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             rnn-based on-line continuous gait phase estimation from shank-mounted imus to control ankle exoskeletons several research groups have developed and studied powered ankle exoskeletons to improve energetics of healthy subjects and the mobility of elderly subjects, or to reduce asymmetry in gaits induced by strokes to achieve optimal effect, the timing of assistive torque has been proved to be of crucial importance previous studies estimated the onset timings mostly by extrapolating the time horizon from past gait events observed with sensors such methods have inherently limited performance when subjects are not walking at steady frequencies to overcome such limitation and allow the use of exoskeletons in various scenarios in a daily life, we propose to estimate the gait phase as a continuous variable progressing over a gait cycle, hence allowing immediate response to frequency changes rather than iteratively correcting it after each cycle our method uses recurrent neural networks to estimate gait phases out of an inertial measurement unit imu every 10 ms by replacing foot sensors with an imu we can obtain rich enough information to estimate gait phase continuously as well as avoid physical damage in sensors from ground impacts our preliminary tests with 2 healthy subjects showed qualitatively positive outcomes regarding the gait phase estimation and the assistive torque control   \n",
       "39334                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                continuous estimation of knee joint angle based on surface electromyography using a long short-term memory neural network and time-advanced feature continuous joint angle estimation based on a surface electromyography semg signal can be used to improve the man-machine coordination performance of the exoskeleton in this study, we proposed a time-advanced feature and utilized long short-term memory lstm with a root mean square rms feature and its time-advanced feature rmstaf; collectively referred to as rrtaf of semg to estimate the knee joint angle to evaluate the effect of joint angle estimation, we used root mean square error rmse and cross-correlation coefficient <i>ρ</i> between the estimated angle and actual angle we also compared three methods ie, lstm using rms, bpnn back propagation neural network using rrtaf, and bpnn using rms with lstm using rrtaf to highlight its good performance five healthy subjects participated in the experiment and their eight muscle ie, rectus femoris rf, biceps femoris bf, semitendinosus st, gracilis gc, semimembranosus sm, sartorius sr, medial gastrocnemius mg, and tibialis anterior ta semg signals were taken as algorithm inputs moreover, the knee joint angles were used as target values the experimental results showed that, compared with lstm using rms, bpnn using rrtaf, and bpnn using rms, the average rmse values of lstm using rrtaf were respectively reduced by 857%, 4662%, and 6869%, whereas the average <i>ρ</i> values were respectively increased by 031%, 415%, and 1835% the results demonstrated that lstm using rrtaf, which contained the time-advanced feature, had better performance for estimating the knee joint motion   \n",
       "55778                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         prediction of plantar forces during gait using wearable sensors and deep neural networks<sup></sup> to enable on-time and high-fidelity lower-limb exoskeleton control, it is effective to predict the future human motion from the observed status in this research, we propose a novel method to predict future plantar force during the gait using imu and plantar sensors deep neural networks dnn are used to learn the non-linear relationship between the measured sensor data and the future plantar force data using the trained network, we can predict the plantar force not only during walking but also at the start and end of walking in the experiments, the performance of the proposed method is confirmed for different prediction time   \n",
       "96518                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       classifying three imaginary states of the same upper extremity using time-domain features brain-computer interface bci allows collaboration between humans and machines it translates the electrical activity of the brain to understandable commands to operate a machine or a device in this study, we propose a method to improve the accuracy of a 3-class bci using electroencephalographic eeg signals this bci discriminates rest against imaginary grasps and elbow movements of the same limb this classification task is challenging because imaginary movements within the same limb have close spatial representations on the motor cortex area the proposed method extracts time-domain features and classifies them using a support vector machine svm with a radial basis kernel function rbf an average accuracy of 742% was obtained when using the proposed method on a dataset collected, prior to this study, from 12 healthy individuals this accuracy was higher than that obtained when other widely used methods, such as common spatial patterns csp, filter bank csp fbcsp, and band power methods, were used on the same dataset these results are encouraging and the proposed method could potentially be used in future applications including bci-driven robotic devices, such as a portable exoskeleton for the arm, to assist individuals with impaired upper extremity functions in performing daily tasks   \n",
       "101171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 eeg classification for motor imagery and resting state in bci applications using multi-class adaboost extreme learning machine brain-computer interface bci systems provide an alternative communication and control approach for people with limited motor function therefore, the feature extraction and classification approach should differentiate the relative unusual state of motion intention from a common resting state in this paper, we sought a novel approach for multi-class classification in bci applications we collected electroencephalographic eeg signals registered by electrodes placed over the scalp during left hand motor imagery, right hand motor imagery, and resting state for ten healthy human subjects we proposed using the kolmogorov complexity kc for feature extraction and a multi-class adaboost classifier with extreme learning machine as base classifier for classification, in order to classify the three-class eeg samples an average classification accuracy of 795% was obtained for ten subjects, which greatly outperformed commonly used approaches thus, it is concluded that the proposed method could improve the performance for classification of motor imagery tasks for multi-class samples it could be applied in further studies to generate the control commands to initiate the movement of a robotic exoskeleton or orthosis, which finally facilitates the rehabilitation of disabled people    \n",
       "55818                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          classification and transfer learning of eeg during a kinesthetic motor imagery task using deep convolutional neural networks the reliable classification of electroencephalography eeg signals is a crucial step towards making eeg-controlled non-invasive neuro-exoskeleton rehabilitation a practical reality eeg signals collected during motor imagery tasks have been proposed to act as a control signal for exoskeleton applications here, a deep convolutional neural network dcnn was optimized to classify a two-class kinesthetic motor imagery eeg dataset, leading to an optimized architecture consisting of four convolutional layers and three fully connected layers transfer learning, or the leveraging of data from past subjects to classify the intentions of a new subject, is important for rehabilitation as it helps to minimize the number of training sessions required from subjects who lack full motor functionality the transfer learning training paradigm investigated through this study utilized region criticality trends to reduce the number of new subject training sessions and increase the classification performance when compared against a single-subject non-transfer-learning classifier   \n",
       "49131   characterization of forearm muscle activation in duchenne muscular dystrophy via high-density electromyography: a case study on the implications for myoelectric control duchenne muscular dystrophy dmd is a genetic disorder that results in progressive muscular degeneration although medical advances increased their life expectancy, dmd individuals are still highly dependent on caregivers hand/wrist function is central for providing independence, and robotic exoskeletons are good candidates for effectively compensating for deteriorating functionality robotic hand exoskeletons require the accurate decoding of motor intention typically via surface electromyography semg traditional low-density semg was used in the past to explore the muscular activations of individuals with dmd; however, it cannot provide high spatial resolution this study characterized, for the first time, the forearm high-density hd electromyograms of three individuals with dmd while performing seven hand/wrist-related tasks and compared them to eight healthy individuals all data available online we looked into the spatial distribution of hd-semg patterns by using principal component analysis pca and also assessed the repeatability and the amplitude distributions of muscle activity additionally, we used a machine learning approach to assess dmd individualspotentials for myocontrol our analysis showed that although participants with dmd were able to repeat similar hd-semg patterns across gestures similarly to healthy participants, a fewer number of electrodes was activated during their gestures compared to the healthy participants additionally, participants with dmd activated their muscles close to maximal contraction level 063 ± 023, whereas healthy participants had lower normalized activations 026 ± 02 lastly, participants with dmd showed on average fewer pcs 3, explaining 90% of the complete gesture space than the healthy 5 however, the ability of the dmd participants to produce repeatable hd-semg patterns was unexpectedly comparable to that of healthy participants, and the same holds true for their offline myocontrol performance, disproving our hypothesis and suggesting a clear potential for the myocontrol of wearable exoskeletons our findings present evidence for the first time on how dmd leads to progressive alterations in hand/wrist motor control in dmd individuals compared to healthy the better understanding of these alterations can lead to further developments for the intuitive and robust myoelectric control of active hand exoskeletons for individuals with dmd   \n",
       "60710                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            myoelectric control of a soft hand exoskeleton using kinematic synergies soft hand exoskeletons offer a lightweight, low-profile alternative to rigid rehabilitative robotic systems, enabling their use to restore activities of daily living adl in those with hand paresis due to stroke or other conditions the hand exoskeleton with embedded synergies hexoes is a soft cable-driven hand exoskeleton capable of independently actuating and sensing 10 degrees of freedom dof of the hand control of the 10 dof exoskeleton is dimensionally reduced using three manually defined synergies in software corresponding to thumb, index, and 3-finger flexion and extension in this paper, five healthy subjects control hexoes using a neural network which decodes synergy weights from contralateral electromyography emg activity the three synergies are manipulated in real time to grasp and lift 15 adl objects of various sizes and weights the neural networks training and validation mean squared error, object grasp time, and grasp success rate were measured for five healthy subjects the final training error of the neural network was 48 ± 18% averaged across subjects and tasks, with 83 ± 34% validation error the time to reach, grasp, and lift an object was 1115 ± 435 s on average, with an average success rate of 667% across all objects the complete system demonstrates real time use of biosignals and machine learning to allow subjects to operate kinematic synergies to grasp objects using a wearable hand exoskeleton future work and applications are further discussed, including possible design improvements and enrollment of individuals with stroke   \n",
       "63793                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      sub-optimally solving actuator redundancy in a hybrid neuroprosthetic system with a multi-layer neural network structure functional electrical stimulation fes has recently been proposed as a supplementary torque assist in lower-limb powered exoskeletons for persons with paraplegia in the combined system, also known as a hybrid neuroprosthesis, both fes-assist and the exoskeleton act to generate lower-limb torques to achieve standing and walking functions due to this actuator redundancy, we are motivated to optimally allocate fes-assist and exoskeleton torque based on a performance index that penalizes fes overuse to minimize muscle fatigue while also minimizing regulation or tracking errors traditional optimal control approaches need a system model to optimize; however, it is often difficult to formulate a musculoskeletal model that accurately predicts muscle responses due to fes in this paper, we use a novel identification and control structure that contains a recurrent neural network rnn and several feedforward neural networks fnns the rnn is trained by supervised learning to identify the system dynamics, while the fnns are trained by a reinforcement learning method to provide sub-optimal control actions the output layer of each fnn has its unique activation functions, so that the asymmetric constraint of fes and the symmetric constraint of exoskeleton motor control input can be realized this new structure is experimentally validated on a seated human participant using a single joint hybrid neuroprosthesis   \n",
       "121138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      high accuracy decoding of user intentions using eeg to control a lower-body exoskeleton brain-machine interface bmi systems allow users to control external mechanical systems using their thoughts commonly used in literature are invasive techniques to acquire brain signals and decode users attempted motions to drive these systems eg a robotic manipulator in this work we use a lower-body exoskeleton and measure the users brain activity using non-invasive electroencephalography eeg the main focus of this study is to decode a paraplegic subjects motion intentions and provide him with the ability of walking with a lower-body exoskeleton accordingly we present our novel method of decoding with high offline evaluation accuracies around 98%, our closed loop implementation structure with considerably short on-site training time around 38 sec, and preliminary results from the real-time closed loop implementation neurorex with a paraplegic test subject    \n",
       "118314                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     a fuzzy controller for lower limb exoskeletons during sit-to-stand and stand-to-sit movement using wearable sensors human motion is a daily and rhythmic activity the exoskeleton concept is a very positive scientific approach for human rehabilitation in case of lower limb impairment although the exoskeleton shows potential, it is not yet applied extensively in clinical rehabilitation in this research, a fuzzy based control algorithm is proposed for lower limb exoskeletons during sit-to-stand and stand-to-sit movements surface electromyograms emgs are acquired from the vastus lateralis muscle using a wearable emg sensor the resultant acceleration angle along the z-axis is determined from a kinematics sensor twenty volunteers were chosen to perform the experiments the whole experiment was accomplished in two phases in the first phase, acceleration angles and emg data were acquired from the volunteers during both sit-to-stand and stand-to-sit motions during sit-to-stand movements, the average acceleration angle at activation was 11°-48° and the emg varied from -019 mv to +019 mv on the other hand, during stand-to-sit movements, the average acceleration angle was found to be 575°-108° at the activation point and the emg varied from -032 mv to +032 mv in the second phase, a fuzzy controller was designed from the experimental data the controller was tested and validated with both offline and real time data using labview    \n",
       "102371                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        eeg-based detection of starting and stopping during gait cycle walking is for humans an essential task in our daily life however, there is a huge and growing number of people who have this ability diminished or are not able to walk due to motor disabilities in this paper, a system to detect the start and the stop of the gait through electroencephalographic signals has been developed the system has been designed in order to be applied in the future to control a lower limb exoskeleton to help stroke or spinal cord injured patients during the gait the brain-machine interface bmi training has been optimized through a preliminary analysis using the brain information recorded during the experiments performed by three healthy subjects afterward, the system has been verified by other four healthy subjects and three patients in a real-time test in both preliminary optimization analysis and real-time tests, the results obtained are very similar the true positive rates are formula: see text and formula: see text respectively regarding the false positive per minute, the values are also very similar, decreasing from 266 in preliminary tests to 190 in real-time finally, the average latencies in the detection of the movement intentions are 794 and 798formula: see textms, preliminary and real-time tests respectively    \n",
       "39967                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     a real-time stable-control gait switching strategy for lower-limb rehabilitation exoskeleton switching different gait according to different movements is an important direction in the study of exoskeleton robot identifying the movement intention of the wearer to control the gait planning of the exoskeleton robot can effectively improve the man-machine interaction experience after the exoskeleton this paper uses a support vector machine svm to realize wearers motion posture recognition by collecting semg signals on the human surface the moving gait of the exoskeleton is planned according to the recognition results, and the decoding intention signal controls gait switching meanwhile, the stability of the planned gait during the movement was analyzed experimental results show that the semg signal decoding human motion intentional, and control exoskeleton robot gait switching has good accuracy and real-time performance it helps patients to complete rehabilitation training more safely and quickly   \n",
       "107145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   a neural network-based gait phase classification method using sensors equipped on lower limb exoskeleton robots an exact classification of different gait phases is essential to enable the control of exoskeleton robots and detect the intentions of users we propose a gait phase classification method based on neural networks using sensor signals from lower limb exoskeleton robots in such robots, foot sensors with force sensing registers are commonly used to classify gait phases we describe classifiers that use the orientation of each lower limb segment and the angular velocities of the joints to output the current gait phase experiments to obtain the input signals and desired outputs for the learning and validation process are conducted, and two neural network methods a multilayer perceptron and nonlinear autoregressive with external inputs narx are used to develop an optimal classifier offline and online evaluations using four criteria are used to compare the performance of the classifiers the proposed narx-based method exhibits sufficiently good performance to replace foot sensors as a means of classifying gait phases    \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "88354         0       0       0           0          0        0       0   \n",
       "65390         0       0       0           0          0        0       0   \n",
       "39334         0       0       0           0          0        0       0   \n",
       "55778         0       0       0           0          0        0       0   \n",
       "96518         0       0       0           0          0        0       0   \n",
       "101171        0       0       0           0          0        0       0   \n",
       "55818         0       0       0           0          0        0       0   \n",
       "49131         0       0       1           0          0        0       0   \n",
       "60710         0       0       0           0          0        0       0   \n",
       "63793         0       0       0           0          0        0       0   \n",
       "121138        0       0       0           0          0        0       0   \n",
       "118314        0       0       0           0          0        0       0   \n",
       "102371        0       0       0           0          0        0       0   \n",
       "39967         0       0       0           0          0        0       0   \n",
       "107145        0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "88354            0            0         0           0        0       0   \n",
       "65390            0            0         0           0        0       0   \n",
       "39334            0            0         0           0        0       0   \n",
       "55778            0            0         0           0        0       0   \n",
       "96518            0            0         0           0        0       0   \n",
       "101171           0            0         0           0        0       0   \n",
       "55818            0            0         0           0        0       0   \n",
       "49131            0            0         0           0        0       0   \n",
       "60710            0            0         0           0        0       0   \n",
       "63793            0            0         0           0        0       0   \n",
       "121138           0            0         0           0        0       0   \n",
       "118314           0            0         0           0        0       0   \n",
       "102371           0            0         0           0        0       0   \n",
       "39967            0            0         0           0        0       0   \n",
       "107145           0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "88354            0             0           0            0         0   \n",
       "65390            0             0           0            0         0   \n",
       "39334            0             0           0            0         0   \n",
       "55778            0             0           0            0         0   \n",
       "96518            0             0           0            0         0   \n",
       "101171           0             0           0            0         0   \n",
       "55818            0             0           0            0         0   \n",
       "49131            0             0           0            0         0   \n",
       "60710            0             0           0            0         0   \n",
       "63793            0             0           0            0         0   \n",
       "121138           0             0           0            0         0   \n",
       "118314           0             0           0            0         0   \n",
       "102371           0             0           0            0         0   \n",
       "39967            0             0           0            0         0   \n",
       "107145           0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "88354           0           0            0           0            0   \n",
       "65390           0           0            0           0            0   \n",
       "39334           0           0            0           0            0   \n",
       "55778           0           0            0           0            0   \n",
       "96518           0           0            0           0            0   \n",
       "101171          0           0            0           0            0   \n",
       "55818           0           0            0           0            0   \n",
       "49131           0           0            0           0            0   \n",
       "60710           0           0            0           0            0   \n",
       "63793           0           0            0           0            0   \n",
       "121138          0           0            0           0            0   \n",
       "118314          0           0            0           0            0   \n",
       "102371          0           0            0           0            0   \n",
       "39967           0           0            0           0            0   \n",
       "107145          0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "88354           0            0        0         0          0       0        0   \n",
       "65390           0            0        0         0          0       0        0   \n",
       "39334           0            0        0         0          0       1        0   \n",
       "55778           0            0        0         0          0       0        0   \n",
       "96518           0            0        0         0          0       0        0   \n",
       "101171          0            0        0         0          0       0        0   \n",
       "55818           0            0        0         0          0       0        0   \n",
       "49131           0            0        0         0          0       0        0   \n",
       "60710           0            0        0         0          0       0        0   \n",
       "63793           0            0        1         0          0       0        0   \n",
       "121138          0            0        0         0          0       0        0   \n",
       "118314          0            0        0         0          0       0        0   \n",
       "102371          0            0        0         0          0       0        0   \n",
       "39967           0            0        0         0          0       0        0   \n",
       "107145          0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "88354          0          0        0       0         0          0        0   \n",
       "65390          0          0        0       0         0          0        0   \n",
       "39334          0          0        0       0         0          0        0   \n",
       "55778          0          0        0       0         0          0        0   \n",
       "96518          0          0        0       0         0          1        0   \n",
       "101171         0          0        0       0         0          1        0   \n",
       "55818          0          0        0       0         0          1        0   \n",
       "49131          0          0        0       0         0          0        0   \n",
       "60710          0          0        0       0         0          0        0   \n",
       "63793          0          0        0       0         0          1        0   \n",
       "121138         0          0        0       0         0          1        0   \n",
       "118314         0          0        0       0         0          0        0   \n",
       "102371         0          0        0       0         0          1        1   \n",
       "39967          0          0        0       0         0          0        0   \n",
       "107145         0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "88354            0         0        0        0       0           0         0   \n",
       "65390            0         0        0        0       0           0         0   \n",
       "39334            0         0        0        0       0           0         0   \n",
       "55778            0         0        0        0       0           0         0   \n",
       "96518            0         0        0        0       0           0         0   \n",
       "101171           0         0        0        0       0           0         0   \n",
       "55818            0         0        0        0       0           0         0   \n",
       "49131            0         0        0        0       0           0         0   \n",
       "60710            0         0        0        0       0           0         0   \n",
       "63793            0         0        0        0       0           0         0   \n",
       "121138           0         0        0        0       0           0         0   \n",
       "118314           0         0        0        0       0           0         0   \n",
       "102371           0         0        0        0       0           0         0   \n",
       "39967            0         0        0        0       0           0         0   \n",
       "107145           0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text haem_text obs_text  \\\n",
       "88354        0            0           0        0         0        0   \n",
       "65390        0            0           0        0         0        0   \n",
       "39334        0            0           0        0         0        0   \n",
       "55778        0            0           0        0         0        0   \n",
       "96518        0            0           0        0         0        0   \n",
       "101171       0            0           0        0         0        0   \n",
       "55818        0            0           0        0         0        0   \n",
       "49131        0            0           0        0         0        0   \n",
       "60710        0            0           0        0         0        0   \n",
       "63793        0            0           0        0         0        0   \n",
       "121138       0            0           0        0         0        0   \n",
       "118314       0            0           0        0         0        0   \n",
       "102371       0            0           0        0         0        0   \n",
       "39967        0            0           0        0         0        0   \n",
       "107145       0            0           0        0         0        0   \n",
       "\n",
       "       renal_text ackd_text paeds_text dent_text audio_text bci_text  \\\n",
       "88354           0         0          0         0          0        0   \n",
       "65390           0         0          0         0          0        0   \n",
       "39334           0         0          0         0          0        0   \n",
       "55778           0         0          0         0          0        0   \n",
       "96518           0         0          0         0          0        0   \n",
       "101171          0         0          0         0          0        0   \n",
       "55818           0         0          0         0          0        0   \n",
       "49131           0         0          0         0          0        0   \n",
       "60710           0         0          0         0          0        0   \n",
       "63793           0         0          0         0          0        0   \n",
       "121138          0         0          0         0          0        0   \n",
       "118314          0         0          0         0          0        0   \n",
       "102371          0         0          0         0          0        0   \n",
       "39967           0         0          0         0          0        0   \n",
       "107145          0         0          0         0          0        0   \n",
       "\n",
       "       prosth_text assist_text  \n",
       "88354            0           1  \n",
       "65390            0           1  \n",
       "39334            0           1  \n",
       "55778            0           1  \n",
       "96518            0           1  \n",
       "101171           0           1  \n",
       "55818            0           1  \n",
       "49131            0           1  \n",
       "60710            0           1  \n",
       "63793            1           1  \n",
       "121138           0           1  \n",
       "118314           0           1  \n",
       "102371           0           1  \n",
       "39967            0           1  \n",
       "107145           0           1  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['assist_text']=='1'].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2496863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text counts:\n",
      "Counter({'0': 33728, '1': 451})\n"
     ]
    }
   ],
   "source": [
    "## HOME ACTIVITY / active\n",
    "\n",
    "## text\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activity monitor\"), \"1\", \"0\")\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activity detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activities monitor\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activities detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"home environ\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"fall detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"fall monitor\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls detect\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls monitor\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"daily activit\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"activity classif\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"daily living\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"fall prevent\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls in home\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"falls at home\"), \"1\", spec['activity_text'])\n",
    "spec['activity_text'] = np.where(groups['text'].str.contains(\"home sensor\"), \"1\", spec['activity_text'])\n",
    "\n",
    "print('text counts:')\n",
    "print(Counter(spec['activity_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5c0adcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>icu_text</th>\n",
       "      <th>ed_text</th>\n",
       "      <th>id_text</th>\n",
       "      <th>sepsis_text</th>\n",
       "      <th>cov19_text</th>\n",
       "      <th>hiv_text</th>\n",
       "      <th>tb_text</th>\n",
       "      <th>tropic_text</th>\n",
       "      <th>malaria_text</th>\n",
       "      <th>derm_text</th>\n",
       "      <th>dermca_text</th>\n",
       "      <th>onc_text</th>\n",
       "      <th>rx_text</th>\n",
       "      <th>breast_text</th>\n",
       "      <th>breastca_text</th>\n",
       "      <th>lungca_text</th>\n",
       "      <th>brainca_text</th>\n",
       "      <th>gica_text</th>\n",
       "      <th>hepca_text</th>\n",
       "      <th>prosca_text</th>\n",
       "      <th>renalca_text</th>\n",
       "      <th>gynonc_text</th>\n",
       "      <th>haemonc_text</th>\n",
       "      <th>psych_text</th>\n",
       "      <th>suicide_text</th>\n",
       "      <th>msk_text</th>\n",
       "      <th>frac_text</th>\n",
       "      <th>rheum_text</th>\n",
       "      <th>gi_text</th>\n",
       "      <th>hep_text</th>\n",
       "      <th>resp_text</th>\n",
       "      <th>pneum_text</th>\n",
       "      <th>osa_text</th>\n",
       "      <th>pe_text</th>\n",
       "      <th>pubh_text</th>\n",
       "      <th>neuro_text</th>\n",
       "      <th>cva_text</th>\n",
       "      <th>epilep_text</th>\n",
       "      <th>alzh_text</th>\n",
       "      <th>cvs_text</th>\n",
       "      <th>ihd_text</th>\n",
       "      <th>hf_text</th>\n",
       "      <th>arrhyt_text</th>\n",
       "      <th>endo_text</th>\n",
       "      <th>dm_text</th>\n",
       "      <th>insulin_text</th>\n",
       "      <th>retina_text</th>\n",
       "      <th>eye_text</th>\n",
       "      <th>haem_text</th>\n",
       "      <th>obs_text</th>\n",
       "      <th>renal_text</th>\n",
       "      <th>ackd_text</th>\n",
       "      <th>paeds_text</th>\n",
       "      <th>dent_text</th>\n",
       "      <th>audio_text</th>\n",
       "      <th>bci_text</th>\n",
       "      <th>prosth_text</th>\n",
       "      <th>assist_text</th>\n",
       "      <th>activity_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97256</th>\n",
       "      <td>training classifiers with shadow features for sensor-based human activity recognition in this paper, a novel training/testing process for building/using a classification model based on human activity recognition har is proposed traditionally, har has been accomplished by a classifier that learns the activities of a person by training with skeletal data obtained from a motion sensor, such as microsoft kinect these skeletal data are the spatial coordinates x, y, z of different parts of the human body the numeric information forms time series, temporal records of movement sequences that can be used for training a classifier in addition to the spatial features that describe current positions in the skeletal data, new features called shadow featuresare used to improve the supervised learning efficacy of the classifier shadow features are inferred from the dynamics of body movements, and thereby modelling the underlying momentum of the performed activities they provide extra dimensions of information for characterising activities in the classification process, and thereby significantly improve the classification accuracy two cases of har are tested using a classification model trained with shadow features: one is by using wearable sensor and the other is by a kinect-based remote sensor our experiments can demonstrate the advantages of the new method, which will have an impact on human activity detection research</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97061</th>\n",
       "      <td>principal component analysis can decrease neural networks performance for incipient falls detection: a preliminary study with hands and feet accelerations fall-related accidents constitute a major problem for elderly people and a burden to the health-care national system it is therefore important to design devices eg, accelerometers and machine learning algorithms able to recognize incipient falls as quickly and reliably as possible blind source separation bss methods are often used as a preprocessing step before classification, however the effects of bss on classification performance are not well understood the aim of this work is to preliminarily characterize the effect that two methods, namely principal and independent component analysis pca and ica and their combined use have on the performance of a neural network in detecting incipient falls we used the feet and arms 3d kinematics of subjects while managing unexpected perturbations during walking results show that pca needs to be used carefully as depending on the initial dataset, the pca might lump variance together thus impairing the performance of an artificial neural networks ann classifier the use of pca with 85% residual variance threshold significantly decreased the classifier performance, which was restored with a subsequent ica pca + ica the results suggest that bss techniques, though linear, might have an adverse effect on nonlinear classifiers such as ann that might be dependent on the initial dataset redundancy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71539</th>\n",
       "      <td>accelerometer-based human fall detection using convolutional neural networks human falls are a global public health issue resulting in over 373 million severe injuries and 646,000 deaths yearly falls result in direct financial cost to health systems and indirectly to society productivity unsurprisingly, human fall detection and prevention are a major focus of health research in this article, we consider deep learning for fall detection in an iot and fog computing environment we propose a convolutional neural network composed of three convolutional layers, two maxpool, and three fully-connected layers as our deep learning model we evaluate its performance using three open data sets and against extant research our approach for resolving dimensionality and modelling simplicity issues is outlined accuracy, precision, sensitivity, specificity, and the matthews correlation coefficient are used to evaluate performance the best results are achieved when using data augmentation during the training process the paper concludes with a discussion of challenges and future directions for research in this domain</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110383</th>\n",
       "      <td>a personalized health-monitoring system for elderly by combining rules and case-based reasoning health-monitoring system for elderly in home environment is a promising solution to provide efficient medical services that increasingly interest by the researchers within this area it is often more challenging when the system is self-served and functioning as personalized provision this paper proposed a personalized self-served health-monitoring system for elderly in home environment by combining general rules with a case-based reasoning approach here, the system generates feedback, recommendation and alarm in a personalized manner based on elderlys medical information and health parameters such as blood pressure, blood glucose, weight, activity, pulse, etc a set of general rules has used to classify individual health parameters the case-based reasoning approach is used to combine all different health parameters, which generates an overall classification of health condition according to the evaluation result considering 323 cases and k=2 ie, top 2 most similar retrieved cases, the sensitivity, specificity and overall accuracy are achieved as 90%, 97% and 96% respectively the preliminary result of the system is acceptable since the feedback; recommendation and alarm messages are personalized and differ from the general messages thus, this approach could be possibly adapted for other situations in personalized elderly monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20222</th>\n",
       "      <td>early detection of freezing of gait during walking using inertial measurement unit and plantar pressure distribution data freezing of gait fog is a sudden and highly disruptive gait dysfunction that appears in mid to late-stage parkinsons disease pd and can lead to falling and injury a system that predicts freezing before it occurs or detects freezing immediately after onset would generate an opportunity for fog prevention or mitigation and thus enhance safe mobility and quality of life this research used accelerometer, gyroscope, and plantar pressure sensors to extract 861 features from walking data collected from 11 people with fog minimum-redundancy maximum-relevance and relief-f feature selection were performed prior to training boosted ensembles of decision trees the binary classification models identified total-fog or no fog states, wherein the total-fog class included data windows from 2 s before the fog onset until the end of the fog episode three feature sets were compared: plantar pressure, inertial measurement unit imu, and both plantar pressure and imu features the plantar-pressure-only model had the greatest sensitivity and the imu-only model had the greatest specificity the best overall model used the combination of plantar pressure and imu features, achieving 764% sensitivity and 862% specificity next, the total-fog class components were evaluated individually ie, pre-fog windows, freeze windows, transition windows between pre-fog and freeze the best model detected windows that contained both pre-fog and fog data with 852% sensitivity, which is equivalent to detecting fog less than 1 s after the freeze began windows of fog data were detected with 934% sensitivity the imu and plantar pressure feature-based model slightly outperformed models that used data from a single sensor type the model achieved early detection by identifying the transition from pre-fog to fog while maintaining excellent fog detection performance 934% sensitivity therefore, if used as part of an intelligent, real-time fog identification and cueing system, even if the pre-fog state were missed, the model would perform well as a freeze detection and cueing system that could improve the mobility and independence of people with pd during their daily activities</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19961</th>\n",
       "      <td>deep artificial neural network based on environmental sound data for the generation of a children activity classification model children activity recognition car is a subject for which numerous works have been developed in recent years, most of them focused on monitoring and safety commonly, these works use as data source different types of sensors that can interfere with the natural behavior of children, since these sensors are embedded in their clothes this article proposes the use of environmental sound data for the creation of a children activity classification model, through the development of a deep artificial neural network ann initially, the ann architecture is proposed, specifying its parameters and defining the necessary values for the creation of the classification model the ann is trained and tested in two ways: using a 70-30 approach 70% of the data for training and 30% for testing and with a k-fold cross-validation approach according to the results obtained in the two validation processes 70-30 splitting and k-fold cross validation, the ann with the proposed architecture achieves an accuracy of 9451% and 9419%, respectively, which allows to conclude that the developed model using the ann and its proposed architecture achieves significant accuracy in the children activity classification by analyzing environmental sound</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79566</th>\n",
       "      <td>automatic timed up-and-go sub-task segmentation for parkinsons disease patients using video-based activity classification the timed up-and-go tug test has been widely accepted as a standard assessment for measuring the basic functional mobility of patients with parkinsons disease several basic mobility sub-tasks sit, sit-to-stand, walk, turn, walk-back, and sit-back are included in a tug test it has been shown that the time costs of these sub-tasks are useful clinical parameters for the assessment of parkinsons disease several automatic methods have been proposed to segment and time these sub-tasks in a tug test however, these methods usually require either well-controlled environments for the tug video recording or information from special devices, such as wearable inertial sensors, ambient sensors, or depth cameras in this paper, an automatic tug sub-task segmentation method using video-based activity classification is proposed and validated in a study with 24 parkinsons disease patients videos used in this paper are recorded in semi-controlled environments with various backgrounds the state-of-the-art deep learning-base 2-d human pose estimation technologies are used for feature extraction a support vector machine and a long short-term memory network are then used for the activity classification and the subtask segmentation our method can be used to automatically acquire clinical parameters for the assessment of parkinsons disease using tug videos-only, leading to the possibility of remote monitoring of the patientscondition</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137278</th>\n",
       "      <td>pattern mining of multichannel semg for tremor classification tremor is defined as the involuntary rhythmic or quasi-rhythmic oscillation of a body part, resulting from alternating or simultaneous contractions of antagonistic muscle groups while tremor may be physiological, those who have disabling pathological tremors find that performing typical activities for daily living to be physically challenging and emotionally draining detecting the presence of tremor and its proper identification are crucial in prescribing the appropriate therapy to lessen its deleterious physical, emotional, psychological, and social impact while diagnosis relies heavily on clinical evaluation, pattern analysis of surface electromyogram semg signals can be a useful diagnostic aid for an objective identification of tremor types using semg system attached to several parts of the patients body while performing several tasks, this research aims to develop a classifier system that automates the process of tremor types recognition finding the optimal model and its corresponding parameters is not a straightforward process the resulting workflow, however, provides valuable information in understanding the interplay and impact of the different features and their parameters to the behavior and performance of the classifier system the resulting model analysis helps identify the necessary locations for the placement of semg electrodes and relevant features that have significant impact in the process of classification these information can help clinicians in streamlining the process of diagnosis without sacrificing its accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39775</th>\n",
       "      <td>mobile sensor based human activity recognition: distinguishing of challenging activities by applying long short-term memory deep learning modified by residual network concept automated recognition of daily human tasks is a novel method for continuous monitoring of the health of elderly people nowadays mobile devices ie smartphone and smartwatch are equipped with a variety of sensors, therefore activity classification algorithms have become as useful, low-cost, and non-invasive diagnostic modality to implement as mobile software the aim of this article is to introduce a new deep learning structure for recognizing challenging ie similar human activities based on signals which have been recorded by sensors mounted on mobile devices in the proposed structure, the residual network concept is engaged as a new substructure inside the main proposed structure this part is responsible to address the problem of accuracy saturation in convolutional neural networks, thanks to its ability in jump over some layers which leads to reducing vanishing gradients effect therefore the accuracy of the classification of several activities is increased by using the proposed structure performance of the proposed method is evaluated on real life recorded signals and is compared with existing techniques in two different scenarios the proposed structure is applied on two well-known human activity datasets that have been prepared in university of fordham the first dataset contains the recorded signals which arise from six different activities including walking, jogging, upstairs, downstairs, sitting, and standing the second dataset also contains walking, jogging, stairs, sitting, standing, eating soup, eating sandwich, and eating chips in the first scenario, the performance of the proposed structures is compared with deep learning schemes the obtained results show that the proposed method may improve the recognition rate at least 5% for the first dataset against its own family alternatives in distinguishing challenging activities ie downstairs and upstairs for the second data set similar improvements is obtained for some challenging activities ie eating sandwich and eating chips these superiorities even reach to at least 28% when the capability of the proposed method in recognizing downstairs and upstairs is compared to its non-family methods for the first dataset increasing the recognition rate of the proposed method for challenging activities ie downstairs and upstairs, eating sandwich and eating chips in parallel with its acceptable performance for other non-challenging activities shows its effectiveness in mobile sensor-based health monitoring systems</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63856</th>\n",
       "      <td>neural muscle activation detection: a deep learning approach using surface electromyography the timing of muscles activation which is a key parameter in determining plenty of medical conditions can be greatly assessed by the surface emg signal which inherently carries an immense amount of information many techniques for measuring muscle activity detection exist in the literature however, due to the complex nature of the emg signal as well as the interference from other muscles that is observed during the measurement of the emg signal, the accuracy of these techniques is compromised in this paper, we introduce the neural muscle activation detection nmad framework that detects the muscle activation based on deep learning the main motivation behind using deep learning is to allow the neural network to detect based on the appropriate signal features instead of depending on certain assumptions not only the presented approach significantly improves the accuracy of timing detection, but because of the training nature, it can adapt to operate under different levels of interference and signal-to-noise ratio</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>a machine learning-based fall risk assessment model for inpatients falls are one of the most common accidents among inpatients and may result in extended hospitalization and increased medical costs constructing a highly accurate fall prediction model could effectively reduce the rate of patient falls, further reducing unnecessary medical costs and patient injury this study applied data mining techniques on a hospitals electronic medical records database comprising a nursing information system to construct inpatient-fall-prediction models for use during various stages of inpatient care the inpatient data were collected from 15 inpatient wards to develop timely and effective fall prediction models for inpatients, we retrieved the data of multiple-time assessment variables at four points during hospitalization this study used various supervised machine learning algorithms to build classification models four supervised learning and two classifier ensemble techniques were selected for model development the results indicated that bagging+rf classifiers yielded optimal prediction performance at all four points during hospitalization this study suggests that nursing personnel should be aware of patientsrisk factors based on comprehensive fall risk assessment and provide patients with individualized fall prevention interventions to reduce inpatient fall rates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87861</th>\n",
       "      <td>an activity recognition framework deploying the random forest classifier and a single optical heart rate monitoring and triaxial accelerometerwrist-band wrist-worn sensors have better compliance for activity monitoring compared to hip, waist, ankle or chest positions however, wrist-worn activity monitoring is challenging due to the wide degree of freedom for the hand movements, as well as similarity of hand movements in different activities such as varying intensities of cycling to strengthen the ability of wrist-worn sensors in detecting human activities more accurately, motion signals can be complemented by physiological signals such as optical heart rate hr based on photoplethysmography in this paper, an activity monitoring framework using an optical hr sensor and a triaxial wrist-worn accelerometer is presented we investigated a range of daily life activities including sitting, standing, household activities and stationary cycling with two intensities a random forest rf classifier was exploited to detect these activities based on the wrist motions and optical hr the highest overall accuracy of 896 ± 39% was achieved with a forest of a size of 64 trees and 13-s signal segments with 90% overlap removing the hr-derived features decreased the classification accuracy of high-intensity cycling by almost 7%, but did not affect the classification accuracies of other activities a feature reduction utilizing the feature importance scores of rf was also carried out and resulted in a shrunken feature set of only 21 features the overall accuracy of the classification utilizing the shrunken feature set was 894 ± 42%, which is almost equivalent to the above-mentioned peak overall accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113163</th>\n",
       "      <td>modulation of grasping force in prosthetic hands using neural network-based predictive control this chapter describes the implementation of a neural network-based predictive control system for driving a prosthetic hand nonlinearities associated with the electromechanical aspects of prosthetic devices present great challenges for precise control of this type of device model-based controllers may overcome this issue moreover, given the complexity of these kinds of electromechanical systems, neural network-based modeling arises as a good fit for modeling the fingersdynamics the results of simulations mimicking potential situations encountered during activities of daily living demonstrate the feasibility of this technique</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15329</th>\n",
       "      <td>ensemble machine learning classification of daily living abilities among older people with hiv clinically relevant methods to identify individuals at risk for impaired daily living abilities secondary to neurocognitive impairment adls remain elusive this is especially true for complex clinical conditions such as hiv-associated neurocognitive disorders hand the aim of this study was to identify novel and modifiable factors that have potential to improve diagnostic accuracy of adl risk, with the long-term goal of guiding future interventions to minimize adl disruption</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94596</th>\n",
       "      <td>a combined one-class svm and template-matching approach for user-aided human fall detection by means of floor acoustic features the primary cause of injury-related death for the elders is represented by falls the scientific community devoted them particular attention, since injuries can be limited by an early detection of the event the solution proposed in this paper is based on a combined one-class svm ocsvm and template-matching classifier that discriminate human falls from nonfalls in a semisupervised framework acoustic signals are captured by means of a floor acoustic sensor; then mel-frequency cepstral coefficients and gaussian mean supervectors gmss are extracted for the fall/nonfall discrimination here we propose a single-sensor two-stage user-aided approach: in the first stage, the ocsvm detects abnormal acoustic events in the second, the template-matching classifier produces the final decision exploiting a set of template gmss related to the events marked as false positives by the user the performance of the algorithm has been evaluated on a corpus containing human falls and nonfall sounds compared to the ocsvm only approach, the proposed algorithm improves the performance by 1014% in clean conditions and 484% in noisy conditions compared to popescu and mahnot 2009 the performance improvement is 1996% in clean conditions and 808% in noisy conditions</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  \\\n",
       "97256                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               training classifiers with shadow features for sensor-based human activity recognition in this paper, a novel training/testing process for building/using a classification model based on human activity recognition har is proposed traditionally, har has been accomplished by a classifier that learns the activities of a person by training with skeletal data obtained from a motion sensor, such as microsoft kinect these skeletal data are the spatial coordinates x, y, z of different parts of the human body the numeric information forms time series, temporal records of movement sequences that can be used for training a classifier in addition to the spatial features that describe current positions in the skeletal data, new features called shadow featuresare used to improve the supervised learning efficacy of the classifier shadow features are inferred from the dynamics of body movements, and thereby modelling the underlying momentum of the performed activities they provide extra dimensions of information for characterising activities in the classification process, and thereby significantly improve the classification accuracy two cases of har are tested using a classification model trained with shadow features: one is by using wearable sensor and the other is by a kinect-based remote sensor our experiments can demonstrate the advantages of the new method, which will have an impact on human activity detection research   \n",
       "97061                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      principal component analysis can decrease neural networks performance for incipient falls detection: a preliminary study with hands and feet accelerations fall-related accidents constitute a major problem for elderly people and a burden to the health-care national system it is therefore important to design devices eg, accelerometers and machine learning algorithms able to recognize incipient falls as quickly and reliably as possible blind source separation bss methods are often used as a preprocessing step before classification, however the effects of bss on classification performance are not well understood the aim of this work is to preliminarily characterize the effect that two methods, namely principal and independent component analysis pca and ica and their combined use have on the performance of a neural network in detecting incipient falls we used the feet and arms 3d kinematics of subjects while managing unexpected perturbations during walking results show that pca needs to be used carefully as depending on the initial dataset, the pca might lump variance together thus impairing the performance of an artificial neural networks ann classifier the use of pca with 85% residual variance threshold significantly decreased the classifier performance, which was restored with a subsequent ica pca + ica the results suggest that bss techniques, though linear, might have an adverse effect on nonlinear classifiers such as ann that might be dependent on the initial dataset redundancy   \n",
       "71539                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           accelerometer-based human fall detection using convolutional neural networks human falls are a global public health issue resulting in over 373 million severe injuries and 646,000 deaths yearly falls result in direct financial cost to health systems and indirectly to society productivity unsurprisingly, human fall detection and prevention are a major focus of health research in this article, we consider deep learning for fall detection in an iot and fog computing environment we propose a convolutional neural network composed of three convolutional layers, two maxpool, and three fully-connected layers as our deep learning model we evaluate its performance using three open data sets and against extant research our approach for resolving dimensionality and modelling simplicity issues is outlined accuracy, precision, sensitivity, specificity, and the matthews correlation coefficient are used to evaluate performance the best results are achieved when using data augmentation during the training process the paper concludes with a discussion of challenges and future directions for research in this domain   \n",
       "110383                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             a personalized health-monitoring system for elderly by combining rules and case-based reasoning health-monitoring system for elderly in home environment is a promising solution to provide efficient medical services that increasingly interest by the researchers within this area it is often more challenging when the system is self-served and functioning as personalized provision this paper proposed a personalized self-served health-monitoring system for elderly in home environment by combining general rules with a case-based reasoning approach here, the system generates feedback, recommendation and alarm in a personalized manner based on elderlys medical information and health parameters such as blood pressure, blood glucose, weight, activity, pulse, etc a set of general rules has used to classify individual health parameters the case-based reasoning approach is used to combine all different health parameters, which generates an overall classification of health condition according to the evaluation result considering 323 cases and k=2 ie, top 2 most similar retrieved cases, the sensitivity, specificity and overall accuracy are achieved as 90%, 97% and 96% respectively the preliminary result of the system is acceptable since the feedback; recommendation and alarm messages are personalized and differ from the general messages thus, this approach could be possibly adapted for other situations in personalized elderly monitoring    \n",
       "20222                                                                                                                                                                                                                                                                                                                                                                                                            early detection of freezing of gait during walking using inertial measurement unit and plantar pressure distribution data freezing of gait fog is a sudden and highly disruptive gait dysfunction that appears in mid to late-stage parkinsons disease pd and can lead to falling and injury a system that predicts freezing before it occurs or detects freezing immediately after onset would generate an opportunity for fog prevention or mitigation and thus enhance safe mobility and quality of life this research used accelerometer, gyroscope, and plantar pressure sensors to extract 861 features from walking data collected from 11 people with fog minimum-redundancy maximum-relevance and relief-f feature selection were performed prior to training boosted ensembles of decision trees the binary classification models identified total-fog or no fog states, wherein the total-fog class included data windows from 2 s before the fog onset until the end of the fog episode three feature sets were compared: plantar pressure, inertial measurement unit imu, and both plantar pressure and imu features the plantar-pressure-only model had the greatest sensitivity and the imu-only model had the greatest specificity the best overall model used the combination of plantar pressure and imu features, achieving 764% sensitivity and 862% specificity next, the total-fog class components were evaluated individually ie, pre-fog windows, freeze windows, transition windows between pre-fog and freeze the best model detected windows that contained both pre-fog and fog data with 852% sensitivity, which is equivalent to detecting fog less than 1 s after the freeze began windows of fog data were detected with 934% sensitivity the imu and plantar pressure feature-based model slightly outperformed models that used data from a single sensor type the model achieved early detection by identifying the transition from pre-fog to fog while maintaining excellent fog detection performance 934% sensitivity therefore, if used as part of an intelligent, real-time fog identification and cueing system, even if the pre-fog state were missed, the model would perform well as a freeze detection and cueing system that could improve the mobility and independence of people with pd during their daily activities   \n",
       "19961                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           deep artificial neural network based on environmental sound data for the generation of a children activity classification model children activity recognition car is a subject for which numerous works have been developed in recent years, most of them focused on monitoring and safety commonly, these works use as data source different types of sensors that can interfere with the natural behavior of children, since these sensors are embedded in their clothes this article proposes the use of environmental sound data for the creation of a children activity classification model, through the development of a deep artificial neural network ann initially, the ann architecture is proposed, specifying its parameters and defining the necessary values for the creation of the classification model the ann is trained and tested in two ways: using a 70-30 approach 70% of the data for training and 30% for testing and with a k-fold cross-validation approach according to the results obtained in the two validation processes 70-30 splitting and k-fold cross validation, the ann with the proposed architecture achieves an accuracy of 9451% and 9419%, respectively, which allows to conclude that the developed model using the ann and its proposed architecture achieves significant accuracy in the children activity classification by analyzing environmental sound   \n",
       "79566                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   automatic timed up-and-go sub-task segmentation for parkinsons disease patients using video-based activity classification the timed up-and-go tug test has been widely accepted as a standard assessment for measuring the basic functional mobility of patients with parkinsons disease several basic mobility sub-tasks sit, sit-to-stand, walk, turn, walk-back, and sit-back are included in a tug test it has been shown that the time costs of these sub-tasks are useful clinical parameters for the assessment of parkinsons disease several automatic methods have been proposed to segment and time these sub-tasks in a tug test however, these methods usually require either well-controlled environments for the tug video recording or information from special devices, such as wearable inertial sensors, ambient sensors, or depth cameras in this paper, an automatic tug sub-task segmentation method using video-based activity classification is proposed and validated in a study with 24 parkinsons disease patients videos used in this paper are recorded in semi-controlled environments with various backgrounds the state-of-the-art deep learning-base 2-d human pose estimation technologies are used for feature extraction a support vector machine and a long short-term memory network are then used for the activity classification and the subtask segmentation our method can be used to automatically acquire clinical parameters for the assessment of parkinsons disease using tug videos-only, leading to the possibility of remote monitoring of the patientscondition   \n",
       "137278                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                pattern mining of multichannel semg for tremor classification tremor is defined as the involuntary rhythmic or quasi-rhythmic oscillation of a body part, resulting from alternating or simultaneous contractions of antagonistic muscle groups while tremor may be physiological, those who have disabling pathological tremors find that performing typical activities for daily living to be physically challenging and emotionally draining detecting the presence of tremor and its proper identification are crucial in prescribing the appropriate therapy to lessen its deleterious physical, emotional, psychological, and social impact while diagnosis relies heavily on clinical evaluation, pattern analysis of surface electromyogram semg signals can be a useful diagnostic aid for an objective identification of tremor types using semg system attached to several parts of the patients body while performing several tasks, this research aims to develop a classifier system that automates the process of tremor types recognition finding the optimal model and its corresponding parameters is not a straightforward process the resulting workflow, however, provides valuable information in understanding the interplay and impact of the different features and their parameters to the behavior and performance of the classifier system the resulting model analysis helps identify the necessary locations for the placement of semg electrodes and relevant features that have significant impact in the process of classification these information can help clinicians in streamlining the process of diagnosis without sacrificing its accuracy   \n",
       "39775   mobile sensor based human activity recognition: distinguishing of challenging activities by applying long short-term memory deep learning modified by residual network concept automated recognition of daily human tasks is a novel method for continuous monitoring of the health of elderly people nowadays mobile devices ie smartphone and smartwatch are equipped with a variety of sensors, therefore activity classification algorithms have become as useful, low-cost, and non-invasive diagnostic modality to implement as mobile software the aim of this article is to introduce a new deep learning structure for recognizing challenging ie similar human activities based on signals which have been recorded by sensors mounted on mobile devices in the proposed structure, the residual network concept is engaged as a new substructure inside the main proposed structure this part is responsible to address the problem of accuracy saturation in convolutional neural networks, thanks to its ability in jump over some layers which leads to reducing vanishing gradients effect therefore the accuracy of the classification of several activities is increased by using the proposed structure performance of the proposed method is evaluated on real life recorded signals and is compared with existing techniques in two different scenarios the proposed structure is applied on two well-known human activity datasets that have been prepared in university of fordham the first dataset contains the recorded signals which arise from six different activities including walking, jogging, upstairs, downstairs, sitting, and standing the second dataset also contains walking, jogging, stairs, sitting, standing, eating soup, eating sandwich, and eating chips in the first scenario, the performance of the proposed structures is compared with deep learning schemes the obtained results show that the proposed method may improve the recognition rate at least 5% for the first dataset against its own family alternatives in distinguishing challenging activities ie downstairs and upstairs for the second data set similar improvements is obtained for some challenging activities ie eating sandwich and eating chips these superiorities even reach to at least 28% when the capability of the proposed method in recognizing downstairs and upstairs is compared to its non-family methods for the first dataset increasing the recognition rate of the proposed method for challenging activities ie downstairs and upstairs, eating sandwich and eating chips in parallel with its acceptable performance for other non-challenging activities shows its effectiveness in mobile sensor-based health monitoring systems   \n",
       "63856                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         neural muscle activation detection: a deep learning approach using surface electromyography the timing of muscles activation which is a key parameter in determining plenty of medical conditions can be greatly assessed by the surface emg signal which inherently carries an immense amount of information many techniques for measuring muscle activity detection exist in the literature however, due to the complex nature of the emg signal as well as the interference from other muscles that is observed during the measurement of the emg signal, the accuracy of these techniques is compromised in this paper, we introduce the neural muscle activation detection nmad framework that detects the muscle activation based on deep learning the main motivation behind using deep learning is to allow the neural network to detect based on the appropriate signal features instead of depending on certain assumptions not only the presented approach significantly improves the accuracy of timing detection, but because of the training nature, it can adapt to operate under different levels of interference and signal-to-noise ratio   \n",
       "6707                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         a machine learning-based fall risk assessment model for inpatients falls are one of the most common accidents among inpatients and may result in extended hospitalization and increased medical costs constructing a highly accurate fall prediction model could effectively reduce the rate of patient falls, further reducing unnecessary medical costs and patient injury this study applied data mining techniques on a hospitals electronic medical records database comprising a nursing information system to construct inpatient-fall-prediction models for use during various stages of inpatient care the inpatient data were collected from 15 inpatient wards to develop timely and effective fall prediction models for inpatients, we retrieved the data of multiple-time assessment variables at four points during hospitalization this study used various supervised machine learning algorithms to build classification models four supervised learning and two classifier ensemble techniques were selected for model development the results indicated that bagging+rf classifiers yielded optimal prediction performance at all four points during hospitalization this study suggests that nursing personnel should be aware of patientsrisk factors based on comprehensive fall risk assessment and provide patients with individualized fall prevention interventions to reduce inpatient fall rates   \n",
       "87861                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          an activity recognition framework deploying the random forest classifier and a single optical heart rate monitoring and triaxial accelerometerwrist-band wrist-worn sensors have better compliance for activity monitoring compared to hip, waist, ankle or chest positions however, wrist-worn activity monitoring is challenging due to the wide degree of freedom for the hand movements, as well as similarity of hand movements in different activities such as varying intensities of cycling to strengthen the ability of wrist-worn sensors in detecting human activities more accurately, motion signals can be complemented by physiological signals such as optical heart rate hr based on photoplethysmography in this paper, an activity monitoring framework using an optical hr sensor and a triaxial wrist-worn accelerometer is presented we investigated a range of daily life activities including sitting, standing, household activities and stationary cycling with two intensities a random forest rf classifier was exploited to detect these activities based on the wrist motions and optical hr the highest overall accuracy of 896 ± 39% was achieved with a forest of a size of 64 trees and 13-s signal segments with 90% overlap removing the hr-derived features decreased the classification accuracy of high-intensity cycling by almost 7%, but did not affect the classification accuracies of other activities a feature reduction utilizing the feature importance scores of rf was also carried out and resulted in a shrunken feature set of only 21 features the overall accuracy of the classification utilizing the shrunken feature set was 894 ± 42%, which is almost equivalent to the above-mentioned peak overall accuracy   \n",
       "113163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           modulation of grasping force in prosthetic hands using neural network-based predictive control this chapter describes the implementation of a neural network-based predictive control system for driving a prosthetic hand nonlinearities associated with the electromechanical aspects of prosthetic devices present great challenges for precise control of this type of device model-based controllers may overcome this issue moreover, given the complexity of these kinds of electromechanical systems, neural network-based modeling arises as a good fit for modeling the fingersdynamics the results of simulations mimicking potential situations encountered during activities of daily living demonstrate the feasibility of this technique    \n",
       "15329                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ensemble machine learning classification of daily living abilities among older people with hiv clinically relevant methods to identify individuals at risk for impaired daily living abilities secondary to neurocognitive impairment adls remain elusive this is especially true for complex clinical conditions such as hiv-associated neurocognitive disorders hand the aim of this study was to identify novel and modifiable factors that have potential to improve diagnostic accuracy of adl risk, with the long-term goal of guiding future interventions to minimize adl disruption   \n",
       "94596                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                a combined one-class svm and template-matching approach for user-aided human fall detection by means of floor acoustic features the primary cause of injury-related death for the elders is represented by falls the scientific community devoted them particular attention, since injuries can be limited by an early detection of the event the solution proposed in this paper is based on a combined one-class svm ocsvm and template-matching classifier that discriminate human falls from nonfalls in a semisupervised framework acoustic signals are captured by means of a floor acoustic sensor; then mel-frequency cepstral coefficients and gaussian mean supervectors gmss are extracted for the fall/nonfall discrimination here we propose a single-sensor two-stage user-aided approach: in the first stage, the ocsvm detects abnormal acoustic events in the second, the template-matching classifier produces the final decision exploiting a set of template gmss related to the events marked as false positives by the user the performance of the algorithm has been evaluated on a corpus containing human falls and nonfall sounds compared to the ocsvm only approach, the proposed algorithm improves the performance by 1014% in clean conditions and 484% in noisy conditions compared to popescu and mahnot 2009 the performance improvement is 1996% in clean conditions and 808% in noisy conditions   \n",
       "\n",
       "       icu_text ed_text id_text sepsis_text cov19_text hiv_text tb_text  \\\n",
       "97256         0       0       0           0          0        0       0   \n",
       "97061         0       0       0           0          0        0       0   \n",
       "71539         0       0       0           0          0        0       0   \n",
       "110383        0       0       0           0          0        0       0   \n",
       "20222         0       0       0           0          0        0       0   \n",
       "19961         0       0       0           0          0        0       0   \n",
       "79566         0       0       0           0          0        0       0   \n",
       "137278        0       0       0           0          0        0       0   \n",
       "39775         0       0       0           0          0        0       0   \n",
       "63856         0       0       0           0          0        0       0   \n",
       "6707          0       0       0           0          0        0       0   \n",
       "87861         0       0       0           0          0        0       0   \n",
       "113163        0       0       0           0          0        0       0   \n",
       "15329         0       0       1           0          0        1       0   \n",
       "94596         0       0       0           0          0        0       0   \n",
       "\n",
       "       tropic_text malaria_text derm_text dermca_text onc_text rx_text  \\\n",
       "97256            0            0         0           0        0       0   \n",
       "97061            0            0         0           0        0       0   \n",
       "71539            0            0         0           0        0       0   \n",
       "110383           0            0         0           0        0       0   \n",
       "20222            0            0         0           0        0       0   \n",
       "19961            0            0         0           0        0       0   \n",
       "79566            0            0         0           0        0       0   \n",
       "137278           0            0         0           0        0       0   \n",
       "39775            0            0         0           0        0       0   \n",
       "63856            0            0         0           0        0       0   \n",
       "6707             0            0         0           0        0       0   \n",
       "87861            0            0         0           0        0       0   \n",
       "113163           0            0         0           0        0       0   \n",
       "15329            0            0         0           0        0       0   \n",
       "94596            0            0         0           0        0       0   \n",
       "\n",
       "       breast_text breastca_text lungca_text brainca_text gica_text  \\\n",
       "97256            0             0           0            0         0   \n",
       "97061            0             0           0            0         0   \n",
       "71539            0             0           0            0         0   \n",
       "110383           0             0           0            0         0   \n",
       "20222            0             0           0            0         0   \n",
       "19961            0             0           0            0         0   \n",
       "79566            0             0           0            0         0   \n",
       "137278           0             0           0            0         0   \n",
       "39775            0             0           0            0         0   \n",
       "63856            0             0           0            0         0   \n",
       "6707             0             0           0            0         0   \n",
       "87861            0             0           0            0         0   \n",
       "113163           0             0           0            0         0   \n",
       "15329            0             0           0            0         0   \n",
       "94596            0             0           0            0         0   \n",
       "\n",
       "       hepca_text prosca_text renalca_text gynonc_text haemonc_text  \\\n",
       "97256           0           0            0           0            0   \n",
       "97061           0           0            0           0            0   \n",
       "71539           0           0            0           0            0   \n",
       "110383          0           0            0           0            0   \n",
       "20222           0           0            0           0            0   \n",
       "19961           0           0            0           0            0   \n",
       "79566           0           0            0           0            0   \n",
       "137278          0           0            0           0            0   \n",
       "39775           0           0            0           0            0   \n",
       "63856           0           0            0           0            0   \n",
       "6707            0           0            0           0            0   \n",
       "87861           0           0            0           0            0   \n",
       "113163          0           0            0           0            0   \n",
       "15329           0           0            0           0            0   \n",
       "94596           0           0            0           0            0   \n",
       "\n",
       "       psych_text suicide_text msk_text frac_text rheum_text gi_text hep_text  \\\n",
       "97256           0            0        0         0          0       0        0   \n",
       "97061           0            0        0         0          0       0        0   \n",
       "71539           0            0        0         0          0       0        0   \n",
       "110383          0            0        0         0          0       0        0   \n",
       "20222           0            0        0         0          0       0        0   \n",
       "19961           0            0        0         0          0       0        0   \n",
       "79566           0            0        0         0          0       0        0   \n",
       "137278          1            0        0         0          0       0        0   \n",
       "39775           0            0        0         0          0       0        0   \n",
       "63856           0            0        0         0          0       0        0   \n",
       "6707            0            0        0         0          0       0        0   \n",
       "87861           0            0        0         0          0       0        0   \n",
       "113163          0            0        0         0          0       0        0   \n",
       "15329           0            0        0         0          0       0        0   \n",
       "94596           0            0        0         0          0       0        0   \n",
       "\n",
       "       resp_text pneum_text osa_text pe_text pubh_text neuro_text cva_text  \\\n",
       "97256          0          0        0       0         0          0        0   \n",
       "97061          0          0        0       0         0          0        0   \n",
       "71539          0          0        0       0         1          0        0   \n",
       "110383         0          0        0       0         0          0        0   \n",
       "20222          0          0        0       0         0          1        0   \n",
       "19961          0          0        0       0         0          0        0   \n",
       "79566          0          0        0       0         0          1        0   \n",
       "137278         0          0        0       0         0          0        0   \n",
       "39775          0          0        0       0         0          0        0   \n",
       "63856          0          0        0       0         0          0        0   \n",
       "6707           0          0        0       0         0          0        0   \n",
       "87861          0          0        0       0         0          0        0   \n",
       "113163         0          0        0       0         0          0        0   \n",
       "15329          0          0        0       0         0          1        0   \n",
       "94596          0          0        0       0         0          0        0   \n",
       "\n",
       "       epilep_text alzh_text cvs_text ihd_text hf_text arrhyt_text endo_text  \\\n",
       "97256            0         0        0        0       0           0         0   \n",
       "97061            0         0        0        0       0           0         0   \n",
       "71539            0         0        0        0       0           0         0   \n",
       "110383           0         0        0        0       0           0         0   \n",
       "20222            0         0        0        0       0           0         0   \n",
       "19961            0         0        0        0       0           0         0   \n",
       "79566            0         0        0        0       0           0         0   \n",
       "137278           0         0        0        0       0           0         0   \n",
       "39775            0         0        0        0       0           0         0   \n",
       "63856            0         0        0        0       0           0         0   \n",
       "6707             0         0        0        0       0           0         0   \n",
       "87861            0         0        0        0       0           0         0   \n",
       "113163           0         0        0        0       0           0         0   \n",
       "15329            0         1        0        0       0           0         0   \n",
       "94596            0         0        0        0       0           0         0   \n",
       "\n",
       "       dm_text insulin_text retina_text eye_text haem_text obs_text  \\\n",
       "97256        0            0           0        0         0        0   \n",
       "97061        0            0           0        0         0        0   \n",
       "71539        0            0           0        0         0        0   \n",
       "110383       0            0           0        0         0        0   \n",
       "20222        0            0           0        0         0        0   \n",
       "19961        0            0           0        0         0        0   \n",
       "79566        0            0           0        0         0        0   \n",
       "137278       0            0           0        0         0        0   \n",
       "39775        0            0           0        0         0        0   \n",
       "63856        0            0           0        0         0        0   \n",
       "6707         0            0           0        0         0        0   \n",
       "87861        0            0           0        0         0        0   \n",
       "113163       0            0           0        0         0        0   \n",
       "15329        0            0           0        0         0        0   \n",
       "94596        0            0           0        0         0        0   \n",
       "\n",
       "       renal_text ackd_text paeds_text dent_text audio_text bci_text  \\\n",
       "97256           0         0          0         0          0        0   \n",
       "97061           0         0          0         0          0        0   \n",
       "71539           0         0          0         0          0        0   \n",
       "110383          0         0          0         0          0        0   \n",
       "20222           0         0          0         0          0        0   \n",
       "19961           0         0          1         0          0        0   \n",
       "79566           0         0          0         0          0        0   \n",
       "137278          0         0          0         0          0        0   \n",
       "39775           0         0          0         0          0        0   \n",
       "63856           0         0          0         0          0        0   \n",
       "6707            0         0          0         0          0        0   \n",
       "87861           0         0          0         0          0        0   \n",
       "113163          0         0          0         0          0        0   \n",
       "15329           0         0          0         0          0        0   \n",
       "94596           0         0          0         0          0        0   \n",
       "\n",
       "       prosth_text assist_text activity_text  \n",
       "97256            0           0             1  \n",
       "97061            0           0             1  \n",
       "71539            0           0             1  \n",
       "110383           0           0             1  \n",
       "20222            0           0             1  \n",
       "19961            0           0             1  \n",
       "79566            0           0             1  \n",
       "137278           0           0             1  \n",
       "39775            0           0             1  \n",
       "63856            0           0             1  \n",
       "6707             0           0             1  \n",
       "87861            0           0             1  \n",
       "113163           1           0             1  \n",
       "15329            0           0             1  \n",
       "94596            0           0             1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec[spec['activity_text']=='1'].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a0883546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_haem'] = np.where(spec['haem_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_obs'] = np.where(spec['obs_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_renal'] = np.where(spec['renal_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_ackd'] = np.where(spec['ackd_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['spec_pubh'] = np.where(spec['pubh_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_bci'] = np.where(spec['bci_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_prosth'] = np.where(spec['prosth_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_assist'] = np.where(spec['assist_text'].str.contains(\"1\"), \"1\", \"0\")\n",
      "C:\\Users\\JOEZ~1\\AppData\\Local\\Temp/ipykernel_17188/3498537448.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  labelled['subspec_activity'] = np.where(spec['activity_text'].str.contains(\"1\"), \"1\", \"0\")\n"
     ]
    }
   ],
   "source": [
    "## combine\n",
    "\n",
    "labelled['subspec_icu'] = np.where(spec['icu_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_ed'] = np.where(spec['ed_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_paeds'] = np.where(spec['paeds_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_dent'] = np.where(spec['dent_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_audio'] = np.where(spec['audio_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_id'] = np.where(spec['id_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_sepsis'] = np.where(spec['sepsis_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_hiv'] = np.where(spec['hiv_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_cov19'] = np.where(spec['cov19_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_tb'] = np.where(spec['tb_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_malaria'] = np.where(spec['malaria_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_tropic'] = np.where(spec['tropic_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_derm'] = np.where(spec['derm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_dermca'] = np.where(spec['dermca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_onc'] = np.where(spec['onc_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_rx'] = np.where(spec['rx_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_lungca'] = np.where(spec['lungca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_brainca'] = np.where(spec['brainca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_gica'] = np.where(spec['gica_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_hepca'] = np.where(spec['hepca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_prosca'] = np.where(spec['prosca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_gynonc'] = np.where(spec['gynonc_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_renalca'] = np.where(spec['renalca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_haemonc'] = np.where(spec['haemonc_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_breast'] = np.where(spec['breast_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_breastca'] = np.where(spec['breastca_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_psych'] = np.where(spec['psych_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_suicide'] = np.where(spec['suicide_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_msk'] = np.where(spec['msk_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_frac'] = np.where(spec['frac_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_rheum'] = np.where(spec['rheum_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_gi'] = np.where(spec['gi_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_hep'] = np.where(spec['hep_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_resp'] = np.where(spec['resp_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_pneum'] = np.where(spec['pneum_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_osa'] = np.where(spec['osa_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_pe'] = np.where(spec['pe_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_neuro'] = np.where(spec['neuro_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_epilep'] = np.where(spec['epilep_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_cva'] = np.where(spec['cva_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_alzh'] = np.where(spec['alzh_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_cvs'] = np.where(spec['cvs_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_ihd'] = np.where(spec['ihd_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_hf'] = np.where(spec['hf_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_arrhyt'] =  np.where(spec['arrhyt_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_endo'] = np.where(spec['endo_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_dm'] = np.where(spec['dm_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_insulin'] = np.where(spec['insulin_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_eye'] = np.where(spec['eye_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_retina'] = np.where(spec['retina_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_haem'] = np.where(spec['haem_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_obs'] = np.where(spec['obs_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_renal'] = np.where(spec['renal_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_ackd'] = np.where(spec['ackd_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['spec_pubh'] = np.where(spec['pubh_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_bci'] = np.where(spec['bci_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_prosth'] = np.where(spec['prosth_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_assist'] = np.where(spec['assist_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "labelled['subspec_activity'] = np.where(spec['activity_text'].str.contains(\"1\"), \"1\", \"0\")\n",
    "\n",
    "#spec.to_csv('output/spec_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1656064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why NER?\n",
    "# non specific e.g. TB could be in the middle of a ward. NER recognises context\n",
    "# words separate by unspecified distance -> lung and adenocarcinoma\n",
    "## too many possible specific terms for subconditions e.g. lung adenocarcinoma, NSCLC -> adenocarcinoma of the lung\n",
    "\n",
    "## Combination of general terms in main text\n",
    "## NER for specific terms\n",
    "\n",
    "## What are the most used **use-cases**\n",
    "## Can we find what the prediction target is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42368f94",
   "metadata": {},
   "source": [
    "## Other Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2cff7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lmic_list = ['afghanistan', 'albania', 'algeria', 'angola', 'antigua', 'barbuda', 'argentina', 'armenia', 'china',\n",
    "#             'azerbaijan', 'bangladesh', 'belarus', 'belize', 'benin', 'bhutan', 'bolivia', 'bosnia', 'herzegovina', \n",
    "#             'botswana', 'brazil', 'burkina', 'faso', 'burundi', 'verde', 'cambodia', 'cameroon', 'africa', 'chad', \n",
    "#             'colombia', 'comoros', 'congo', 'costa rica', 'ivoire', 'cuba', 'djibouti', 'dominica', 'dominica', \n",
    "#             'ecuador', 'egypt', 'salvador', 'guinea', 'eritrea', 'eswatini', 'ethiopia', 'fiji', 'gabon', 'gambia', \n",
    "#             'georgia', 'ghana', 'grenada', 'guatemala', 'guinea', 'guyana', 'haiti', 'honduras', 'india', \n",
    "#             'indonesia', 'iran', 'iraq', 'jamaica', 'jordan', 'kazakhstan', 'kenya', 'kiribati', 'dpr', 'north korea', \n",
    "#             'kosovo', 'kyrgyzstan', 'lao', 'lebanon', 'lesotho', 'liberia', 'libya', 'macedonia', 'madagascar', 'malawi', \n",
    "#             'malaysia', 'maldives', 'mali', 'marshall', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', \n",
    "#             'mongolia', 'montenegro', 'montserrat', 'morocco', 'mozambique', 'myanmar', 'namibia', 'nauru', 'nepal', \n",
    "#             'nicaragua', 'niger', 'nigeria', 'niue', 'pakistan', 'palau', 'panama', 'papua', 'paraguay', 'peru', \n",
    "#             'philippines', 'rwanda', 'helena', 'samoa', 'príncipe', 'senegal', 'serbia', 'sierra leone', 'solomon', \n",
    "#             'somalia', 'south africa', 'sudan', 'sri lanka', 'saint lucia', 'saint vincent', 'grenadines', 'sudan', \n",
    "#             'suriname', 'syria', 'tajikistan', 'tanzania', 'thailand', 'timor', 'togo', 'tokelau', 'tonga', 'tunisia', \n",
    "#             'turkey', 'turkmenistan', 'tuvalu', 'uganda', 'ukraine', 'uzbekistan', 'vanuatu', 'venezuela', 'vietnam', \n",
    "#             'wallis', 'west bank', 'gaza', 'palestine', 'yemen', 'zambia', 'zimbabwe', 'low-income', 'middle-income', \n",
    "#             'lmic', 'scarce', 'resource limited', 'resource-limited']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0966b",
   "metadata": {},
   "source": [
    "## Final Tagged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "176252a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34179\n"
     ]
    }
   ],
   "source": [
    "#all_tagged = pd.concat([algo, feat, spec], axis=1)\n",
    "#\n",
    "print(len(labelled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6c1fc669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34179 entries, 1 to 172538\n",
      "Data columns (total 109 columns):\n",
      " #    Column               Dtype \n",
      "---   ------               ----- \n",
      " 0    pmid                 string\n",
      " 1    doi                  string\n",
      " 2    title                string\n",
      " 3    abstract             string\n",
      " 4    article_date         string\n",
      " 5    pubmed_date          string\n",
      " 6    article_type         string\n",
      " 7    lang                 string\n",
      " 8    journal              string\n",
      " 9    journal_short        string\n",
      " 10   journal_country      string\n",
      " 11   authors              string\n",
      " 12   author_affils        string\n",
      " 13   keywords             string\n",
      " 14   mesh_terms           string\n",
      " 15   references_pmids     string\n",
      " 16   feature              string\n",
      " 17   include              string\n",
      " 18   mature               string\n",
      " 19   algo_neural_net      object\n",
      " 20   algo_support_vector  object\n",
      " 21   algo_regression      object\n",
      " 22   algo_decision_tree   object\n",
      " 23   algo_discriminant    object\n",
      " 24   algo_naive_bayes     object\n",
      " 25   algo_transfer        object\n",
      " 26   algo_federated       object\n",
      " 27   algo_k_nearest       object\n",
      " 28   algo_unsupervised    object\n",
      " 29   feat_xr              object\n",
      " 30   feat_ct              object\n",
      " 31   feat_mri             object\n",
      " 32   feat_eeg             object\n",
      " 33   feat_ecg             object\n",
      " 34   feat_emg             object\n",
      " 35   feat_us              object\n",
      " 36   feat_echo            object\n",
      " 37   feat_histo           object\n",
      " 38   feat_oct             object\n",
      " 39   feat_mamm            object\n",
      " 40   feat_endoscop        object\n",
      " 41   feat_derm            object\n",
      " 42   feat_gene            object\n",
      " 43   feat_bio             object\n",
      " 44   feat_nlp             object\n",
      " 45   feat_ehr             object\n",
      " 46   feat_sensor          object\n",
      " 47   feat_phone           object\n",
      " 48   feat_prom            object\n",
      " 49   feat_sound           object\n",
      " 50   subspec_icu          object\n",
      " 51   subspec_ed           object\n",
      " 52   spec_paeds           object\n",
      " 53   spec_dent            object\n",
      " 54   spec_audio           object\n",
      " 55   spec_id              object\n",
      " 56   subspec_sepsis       object\n",
      " 57   subspec_hiv          object\n",
      " 58   subspec_cov19        object\n",
      " 59   subspec_tb           object\n",
      " 60   subspec_malaria      object\n",
      " 61   subspec_tropic       object\n",
      " 62   spec_derm            object\n",
      " 63   subspec_dermca       object\n",
      " 64   spec_onc             object\n",
      " 65   subspec_rx           object\n",
      " 66   subspec_lungca       object\n",
      " 67   subspec_brainca      object\n",
      " 68   subspec_gica         object\n",
      " 69   subspec_hepca        object\n",
      " 70   subspec_prosca       object\n",
      " 71   subspec_gynonc       object\n",
      " 72   subspec_renalca      object\n",
      " 73   subspec_haemonc      object\n",
      " 74   subspec_breast       object\n",
      " 75   subspec_breastca     object\n",
      " 76   spec_psych           object\n",
      " 77   subspec_suicide      object\n",
      " 78   spec_msk             object\n",
      " 79   subspec_frac         object\n",
      " 80   spec_rheum           object\n",
      " 81   spec_gi              object\n",
      " 82   spec_hep             object\n",
      " 83   spec_resp            object\n",
      " 84   subspec_pneum        object\n",
      " 85   subspec_osa          object\n",
      " 86   subspec_pe           object\n",
      " 87   spec_neuro           object\n",
      " 88   subspec_epilep       object\n",
      " 89   subspec_cva          object\n",
      " 90   subspec_alzh         object\n",
      " 91   spec_cvs             object\n",
      " 92   subspec_ihd          object\n",
      " 93   subspec_hf           object\n",
      " 94   subspec_arrhyt       object\n",
      " 95   spec_endo            object\n",
      " 96   spec_dm              object\n",
      " 97   subspec_insulin      object\n",
      " 98   spec_eye             object\n",
      " 99   subspec_retina       object\n",
      " 100  spec_haem            object\n",
      " 101  spec_obs             object\n",
      " 102  spec_renal           object\n",
      " 103  subspec_ackd         object\n",
      " 104  spec_pubh            object\n",
      " 105  subspec_bci          object\n",
      " 106  subspec_prosth       object\n",
      " 107  subspec_assist       object\n",
      " 108  subspec_activity     object\n",
      "dtypes: object(90), string(19)\n",
      "memory usage: 28.7+ MB\n"
     ]
    }
   ],
   "source": [
    "labelled.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b6183aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>article_date</th>\n",
       "      <th>pubmed_date</th>\n",
       "      <th>article_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>journal</th>\n",
       "      <th>journal_short</th>\n",
       "      <th>journal_country</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_affils</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>references_pmids</th>\n",
       "      <th>feature</th>\n",
       "      <th>include</th>\n",
       "      <th>mature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "      <th>feat_xr</th>\n",
       "      <th>feat_ct</th>\n",
       "      <th>feat_mri</th>\n",
       "      <th>feat_eeg</th>\n",
       "      <th>feat_ecg</th>\n",
       "      <th>feat_emg</th>\n",
       "      <th>feat_us</th>\n",
       "      <th>feat_echo</th>\n",
       "      <th>feat_histo</th>\n",
       "      <th>feat_oct</th>\n",
       "      <th>feat_mamm</th>\n",
       "      <th>feat_endoscop</th>\n",
       "      <th>feat_derm</th>\n",
       "      <th>feat_gene</th>\n",
       "      <th>feat_bio</th>\n",
       "      <th>feat_nlp</th>\n",
       "      <th>feat_ehr</th>\n",
       "      <th>feat_sensor</th>\n",
       "      <th>feat_phone</th>\n",
       "      <th>feat_prom</th>\n",
       "      <th>feat_sound</th>\n",
       "      <th>subspec_icu</th>\n",
       "      <th>subspec_ed</th>\n",
       "      <th>spec_paeds</th>\n",
       "      <th>spec_dent</th>\n",
       "      <th>spec_audio</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>subspec_sepsis</th>\n",
       "      <th>subspec_hiv</th>\n",
       "      <th>subspec_cov19</th>\n",
       "      <th>subspec_tb</th>\n",
       "      <th>subspec_malaria</th>\n",
       "      <th>subspec_tropic</th>\n",
       "      <th>spec_derm</th>\n",
       "      <th>subspec_dermca</th>\n",
       "      <th>spec_onc</th>\n",
       "      <th>subspec_rx</th>\n",
       "      <th>subspec_lungca</th>\n",
       "      <th>subspec_brainca</th>\n",
       "      <th>subspec_gica</th>\n",
       "      <th>subspec_hepca</th>\n",
       "      <th>subspec_prosca</th>\n",
       "      <th>subspec_gynonc</th>\n",
       "      <th>subspec_renalca</th>\n",
       "      <th>subspec_haemonc</th>\n",
       "      <th>subspec_breast</th>\n",
       "      <th>subspec_breastca</th>\n",
       "      <th>spec_psych</th>\n",
       "      <th>subspec_suicide</th>\n",
       "      <th>spec_msk</th>\n",
       "      <th>subspec_frac</th>\n",
       "      <th>spec_rheum</th>\n",
       "      <th>spec_gi</th>\n",
       "      <th>spec_hep</th>\n",
       "      <th>spec_resp</th>\n",
       "      <th>subspec_pneum</th>\n",
       "      <th>subspec_osa</th>\n",
       "      <th>subspec_pe</th>\n",
       "      <th>spec_neuro</th>\n",
       "      <th>subspec_epilep</th>\n",
       "      <th>subspec_cva</th>\n",
       "      <th>subspec_alzh</th>\n",
       "      <th>spec_cvs</th>\n",
       "      <th>subspec_ihd</th>\n",
       "      <th>subspec_hf</th>\n",
       "      <th>subspec_arrhyt</th>\n",
       "      <th>spec_endo</th>\n",
       "      <th>spec_dm</th>\n",
       "      <th>subspec_insulin</th>\n",
       "      <th>spec_eye</th>\n",
       "      <th>subspec_retina</th>\n",
       "      <th>spec_haem</th>\n",
       "      <th>spec_obs</th>\n",
       "      <th>spec_renal</th>\n",
       "      <th>subspec_ackd</th>\n",
       "      <th>spec_pubh</th>\n",
       "      <th>subspec_bci</th>\n",
       "      <th>subspec_prosth</th>\n",
       "      <th>subspec_assist</th>\n",
       "      <th>subspec_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34688173</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104924</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.</td>\n",
       "      <td>Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']</td>\n",
       "      <td>['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']</td>\n",
       "      <td>['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688172</td>\n",
       "      <td>10.1016/j.compbiomed.2021.104927</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.</td>\n",
       "      <td>The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computers in biology and medicine</td>\n",
       "      <td>Comput Biol Med</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']</td>\n",
       "      <td>['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']</td>\n",
       "      <td>['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34687858</td>\n",
       "      <td>10.1016/j.neuroimage.2021.118652</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States.</td>\n",
       "      <td>Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>NeuroImage</td>\n",
       "      <td>Neuroimage</td>\n",
       "      <td>United States</td>\n",
       "      <td>['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']</td>\n",
       "      <td>['Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology &amp; Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']</td>\n",
       "      <td>['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34687853</td>\n",
       "      <td>10.1016/j.mri.2021.10.024</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.</td>\n",
       "      <td>To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']</td>\n",
       "      <td>['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']</td>\n",
       "      <td>['Endometrial cancer', 'Radiomic machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34687850</td>\n",
       "      <td>10.1016/j.mri.2021.10.023</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.</td>\n",
       "      <td>Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>2021-10-20</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Magnetic resonance imaging</td>\n",
       "      <td>Magn Reson Imaging</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']</td>\n",
       "      <td>['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']</td>\n",
       "      <td>['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34687347</td>\n",
       "      <td>10.1007/s00330-021-08284-z</td>\n",
       "      <td>Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements.</td>\n",
       "      <td>To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.</td>\n",
       "      <td>2021-10-23</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>European radiology</td>\n",
       "      <td>Eur Radiol</td>\n",
       "      <td>Germany</td>\n",
       "      <td>['Rühling Sebastian', 'Navarro Fernando', 'Sekuboyina Anjany', 'El Husseini Malek', 'Baum Thomas', 'Menze Bjoern', 'Braren Rickmer', 'Zimmer Claus', 'Kirschke Jan S']</td>\n",
       "      <td>['Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Informatics, Technical University of Munich, Munich, Germany.', 'Department of Diagnostic and Interventional Radiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany. jan.kirschke@tum.de.']</td>\n",
       "      <td>['Bone density', 'Machine learning', 'Multidetector computed tomography', 'Osteoporosis', 'Screening']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements. To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>34686914</td>\n",
       "      <td>10.1007/s00467-021-05321-3</td>\n",
       "      <td>Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves.</td>\n",
       "      <td>Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.</td>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Pediatric nephrology (Berlin, Germany)</td>\n",
       "      <td>Pediatr Nephrol</td>\n",
       "      <td>Germany</td>\n",
       "      <td>['Kwong Jethro Cc', 'Khondker Adree', 'Kim Jin Kyu', 'Chua Michael', 'Keefe Daniel T', 'Dos Santos Joana', 'Skreta Marta', 'Erdman Lauren', \"D'Souza Neeta\", 'Selman Antoine Fermin', 'Weaver John', 'Weiss Dana A', 'Long Christopher', 'Tasian Gregory', 'Teoh Chia Wei', 'Rickard Mandy', 'Lorenzo Armando J']</td>\n",
       "      <td>['Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", 'Division of Nephrology, Hospital for Sick Children, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada. armando.lorenzo@sickkids.ca.']</td>\n",
       "      <td>['Catheterization', 'Chronic kidney disease', 'Dialysis', 'Machine learning', 'Posterior urethral valve', 'Transplant']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves. Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>34686815</td>\n",
       "      <td>10.1038/s41415-021-3526-6</td>\n",
       "      <td>The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software.</td>\n",
       "      <td>Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p &lt;0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.</td>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>British dental journal</td>\n",
       "      <td>Br Dent J</td>\n",
       "      <td>England</td>\n",
       "      <td>['Devlin Hugh', 'Williams Tomos', 'Graham Jim', 'Ashley Martin']</td>\n",
       "      <td>['Professor of Restorative Dentistry, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Director, Manchester Imaging Ltd, UK.', 'Honorary Research Assistant, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Software Manager, Manchester Imaging Ltd, UK. tomos.williams@manchester.ac.uk.', 'Director, Manchester Imaging Ltd, UK; Honorary Reader, Division of Informatics, Imaging and Data Sciences, School of Health Sciences, University of Manchester, UK.', 'Consultant and MAHSC Honorary Professor in Restorative Dentistry and Oral Health, University Dental Hospital of Manchester, Manchester University NHS Foundation Trust, UK.']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software. Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p &lt;0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>34686646</td>\n",
       "      <td>10.1097/CMR.0000000000000774</td>\n",
       "      <td>Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas.</td>\n",
       "      <td>To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.</td>\n",
       "      <td>2021-10-21</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Melanoma research</td>\n",
       "      <td>Melanoma Res</td>\n",
       "      <td>England</td>\n",
       "      <td>['Moglia Andrea', 'Cerri Amilcare', 'Moglia Alessandra', 'Berchiolli Raffaella', 'Ferrari Mauro', 'Betti Roberto']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas. To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>34686573</td>\n",
       "      <td>10.1136/neurintsurg-2021-017976</td>\n",
       "      <td>Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population.</td>\n",
       "      <td>Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.</td>\n",
       "      <td>2021-10-22</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>eng</td>\n",
       "      <td>Journal of neurointerventional surgery</td>\n",
       "      <td>J Neurointerv Surg</td>\n",
       "      <td>England</td>\n",
       "      <td>['Salimi Ashkezari Seyedeh Fatemeh', 'Mut Fernando', 'Slawski Martin', 'Cheng Boyle', 'Yu Alexander K', 'White Tim G', 'Woo Henry H', 'Koch Matthew J', 'Amin-Hanjani Sepideh', 'Charbel Fady T', 'Rezai Jahromi Behnam', 'Niemelä Mika', 'Koivisto Timo', 'Frosen Juhana', 'Tobe Yasutaka', 'Maiti Spandan', 'Robertson Anne M', 'Cebral Juan R']</td>\n",
       "      <td>['Department of Bioengineering, George Mason University, Fairfax, Virginia, USA ssalimia@gmu.edu.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.', 'Department of Statistics, George Mason University, Fairfax, Virginia, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Neurosurgery Research Group, Biomedicum Helsinki, University of Helsinki, Helsinki, Uusimaa, Finland.', 'Department of Neurosurgery, Töölö Hospital, University of Helsinki, Helsinki, Finland.', 'Department of Neurosurgery, Kuopio University Hospital, Kuopio, Pohjois-Savo, Finland.', 'Department of Neurosurgery, Tampere University Hospital, Tampere, Finland.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.']</td>\n",
       "      <td>['aneurysm', 'blood flow', 'hemorrhage', 'statistics']</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population. Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pmid                               doi  \\\n",
       "1   34688173  10.1016/j.compbiomed.2021.104924   \n",
       "2   34688172  10.1016/j.compbiomed.2021.104927   \n",
       "8   34687858  10.1016/j.neuroimage.2021.118652   \n",
       "9   34687853         10.1016/j.mri.2021.10.024   \n",
       "10  34687850         10.1016/j.mri.2021.10.023   \n",
       "14  34687347        10.1007/s00330-021-08284-z   \n",
       "21  34686914        10.1007/s00467-021-05321-3   \n",
       "25  34686815         10.1038/s41415-021-3526-6   \n",
       "29  34686646      10.1097/CMR.0000000000000774   \n",
       "31  34686573   10.1136/neurintsurg-2021-017976   \n",
       "\n",
       "                                                                                                                                                                                               title  \\\n",
       "1                                                                              A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists.   \n",
       "2                                                                                              A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.   \n",
       "8                                                                                                                                        Causal Decoding of Individual Cortical Excitability States.   \n",
       "9                    Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion.   \n",
       "10                                                        MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme.   \n",
       "14                                       Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements.   \n",
       "21                                    Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves.   \n",
       "25  The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software.   \n",
       "29                                        Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas.   \n",
       "31                                  Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                 The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8                                                                         Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.   \n",
       "25                                                                                                                                         Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p <0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.   \n",
       "29  To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.   \n",
       "\n",
       "   article_date pubmed_date     article_type lang  \\\n",
       "1    2021-10-06  2021-10-24  Journal Article  eng   \n",
       "2    2021-10-11  2021-10-24  Journal Article  eng   \n",
       "8    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "9    2021-10-20  2021-10-24  Journal Article  eng   \n",
       "10   2021-10-20  2021-10-24  Journal Article  eng   \n",
       "14   2021-10-23  2021-10-24  Journal Article  eng   \n",
       "21   2021-10-22  2021-10-24  Journal Article  eng   \n",
       "25   2021-10-22  2021-10-24  Journal Article  eng   \n",
       "29   2021-10-21  2021-10-24  Journal Article  eng   \n",
       "31   2021-10-22  2021-10-24  Journal Article  eng   \n",
       "\n",
       "                                   journal       journal_short  \\\n",
       "1        Computers in biology and medicine     Comput Biol Med   \n",
       "2        Computers in biology and medicine     Comput Biol Med   \n",
       "8                               NeuroImage          Neuroimage   \n",
       "9               Magnetic resonance imaging  Magn Reson Imaging   \n",
       "10              Magnetic resonance imaging  Magn Reson Imaging   \n",
       "14                      European radiology          Eur Radiol   \n",
       "21  Pediatric nephrology (Berlin, Germany)     Pediatr Nephrol   \n",
       "25                  British dental journal           Br Dent J   \n",
       "29                       Melanoma research        Melanoma Res   \n",
       "31  Journal of neurointerventional surgery  J Neurointerv Surg   \n",
       "\n",
       "   journal_country  \\\n",
       "1    United States   \n",
       "2    United States   \n",
       "8    United States   \n",
       "9      Netherlands   \n",
       "10     Netherlands   \n",
       "14         Germany   \n",
       "21         Germany   \n",
       "25         England   \n",
       "29         England   \n",
       "31         England   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                               authors  \\\n",
       "1                                                                                                                                                                                                                             ['Yang Yiguang', 'Wang Juncheng', 'Xie Fengying', 'Liu Jie', 'Shu Chang', 'Wang Yukun', 'Zheng Yushan', 'Zhang Haopeng']   \n",
       "2                                                                                                                                                                                                                                                                                  ['Azouji Neda', 'Sami Ashkan', 'Taheri Mohammad', 'Müller Henning']   \n",
       "8                                                                                                                                                                                                                                                                             ['Metsomaa J', 'Belardinelli P', 'Ermolova M', 'Ziemann U', 'Zrenner C']   \n",
       "9                                                                                                                                   ['Otani Satoshi', 'Himoto Yuki', 'Nishio Mizuho', 'Fujimoto Koji', 'Moribata Yusaku', 'Yakami Masahiro', 'Kurata Yasuhisa', 'Hamanishi Junzo', 'Ueda Akihiko', 'Minamiguchi Sachiko', 'Mandai Masaki', 'Kido Aki']   \n",
       "10                                                                                                                                                                                                                                                                          ['Jajroudi Mahdie', 'Enferadi Milad', 'Homayoun Amir Azar', 'Reiazi Reza']   \n",
       "14                                                                                                                                                                              ['Rühling Sebastian', 'Navarro Fernando', 'Sekuboyina Anjany', 'El Husseini Malek', 'Baum Thomas', 'Menze Bjoern', 'Braren Rickmer', 'Zimmer Claus', 'Kirschke Jan S']   \n",
       "21                                   ['Kwong Jethro Cc', 'Khondker Adree', 'Kim Jin Kyu', 'Chua Michael', 'Keefe Daniel T', 'Dos Santos Joana', 'Skreta Marta', 'Erdman Lauren', \"D'Souza Neeta\", 'Selman Antoine Fermin', 'Weaver John', 'Weiss Dana A', 'Long Christopher', 'Tasian Gregory', 'Teoh Chia Wei', 'Rickard Mandy', 'Lorenzo Armando J']   \n",
       "25                                                                                                                                                                                                                                                                                    ['Devlin Hugh', 'Williams Tomos', 'Graham Jim', 'Ashley Martin']   \n",
       "29                                                                                                                                                                                                                                  ['Moglia Andrea', 'Cerri Amilcare', 'Moglia Alessandra', 'Berchiolli Raffaella', 'Ferrari Mauro', 'Betti Roberto']   \n",
       "31  ['Salimi Ashkezari Seyedeh Fatemeh', 'Mut Fernando', 'Slawski Martin', 'Cheng Boyle', 'Yu Alexander K', 'White Tim G', 'Woo Henry H', 'Koch Matthew J', 'Amin-Hanjani Sepideh', 'Charbel Fady T', 'Rezai Jahromi Behnam', 'Niemelä Mika', 'Koivisto Timo', 'Frosen Juhana', 'Tobe Yasutaka', 'Maiti Spandan', 'Robertson Anne M', 'Cebral Juan R']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         author_affils  \\\n",
       "1   ['Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China. Electronic address: xfy_73@buaa.edu.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China. Electronic address: Liujie04672@pumch.cn.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Department of Dermatology, State Key Laboratory of Complex Severe and Rare Diseases, Peking Union Medical College Hospital, Chinese Academy of Medical Science and Peking Union Medical College, National Clinical Research Center for Dermatologic and Immunologic Diseases, Beijing, 100730, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.', 'Image Processing Center, School of Astronautics, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing, 100191, China.']   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: azouji@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: sami@shirazu.ac.ir.', 'Department of Computer Science and Engineering and IT, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran. Electronic address: motaheri@shirazu.ac.ir.', 'Department of Business Information Systems University of Applied Sciences Western Switzerland, Sierre (HES SO), Switzerland. Electronic address: henning.mueller@hevs.ch.']   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; CIMeC, Center for Mind-Brain Sciences, University of Trento, Italy.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen. Electronic address: ulf.ziemann@uni-tuebingen.de.', 'Department of Neurology & Stroke, University of Tübingen, Tübingen, Germany; Hertie Institute for Clinical Brain Research, University of Tübingen; Temerty Centre for Therapeutic Brain Intervention, Centre for Addiction and Mental Health, and Department of Psychiatry, University of Toronto, Toronto, ON, Canada.']   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ['Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan. Electronic address: yhimoto@kuhp.kyoto-u.ac.jp.', 'Department of Diagnostic Imaging and Nuclear Medicine, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Real World Data Research and Developmentx, Graduate School of Medicine, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan; Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Preemptive Medicine and Lifestyle-related Disease Research Center, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Pathology, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Gynecology and Obstetrics, Kyoto University, Kyoto 606-8507, Japan.', 'Department of Diagnostic Radiology and Nuclear Medicine, Kyoto University Hospital, Kyoto 606-8507, Japan.']   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ['Pharmaceutical Research Center, Mashhad University of Medical Sciences, Mashhad, Iran. Electronic address: Jajroudimh991@mums.ac.ir.', 'Research Center for Nuclear Medicine, Shariati Hospital, Tehran University of Medical Sciences, Tehran, Iran.', 'Sina Trauma Research Center, Tehran University of Medical Sciences, Tehran, Iran.', 'Radiation Medicine Program, Princess Margaret Cancer Centre, University Health Network, Toronto, Ontario, Canada. Electronic address: reza.reiazi@uhnresearch.ca.']   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ['Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Informatics, Technical University of Munich, Munich, Germany.', 'Department of Diagnostic and Interventional Radiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany.', 'Department of Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Ismaninger Str 22, 81675, Munich, Germany. jan.kirschke@tum.de.']   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                            ['Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', 'Centre for Computational Medicine, The Hospital for Sick Children, Toronto, ON, Canada.', \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", \"Division of Urology, Children's Hospital of Philadelphia, Philadelphia, PA, USA.\", 'Division of Nephrology, Hospital for Sick Children, Toronto, ON, Canada.', 'Division of Urology, Department of Surgery, Hospital for Sick Children, 555 University Avenue, Toronto, ON, M5G 1X8, Canada.', 'Division of Urology, Department of Surgery, University of Toronto, Toronto, ON, Canada. armando.lorenzo@sickkids.ca.']   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ['Professor of Restorative Dentistry, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Director, Manchester Imaging Ltd, UK.', 'Honorary Research Assistant, Division of Dentistry, School of Medical Sciences, University of Manchester, UK; Software Manager, Manchester Imaging Ltd, UK. tomos.williams@manchester.ac.uk.', 'Director, Manchester Imaging Ltd, UK; Honorary Reader, Division of Informatics, Imaging and Data Sciences, School of Health Sciences, University of Manchester, UK.', 'Consultant and MAHSC Honorary Professor in Restorative Dentistry and Oral Health, University Dental Hospital of Manchester, Manchester University NHS Foundation Trust, UK.']   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <NA>   \n",
       "31                                                                                                                                                                                                                                                                                                                                     ['Department of Bioengineering, George Mason University, Fairfax, Virginia, USA ssalimia@gmu.edu.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.', 'Department of Statistics, George Mason University, Fairfax, Virginia, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Allegheny General Hospital, Pittsburgh, Pennsylvania, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Manhasset, New York, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Department of Neurosurgery, University of Illinois at Chicago, Chicago, Illinois, USA.', 'Neurosurgery Research Group, Biomedicum Helsinki, University of Helsinki, Helsinki, Uusimaa, Finland.', 'Department of Neurosurgery, Töölö Hospital, University of Helsinki, Helsinki, Finland.', 'Department of Neurosurgery, Kuopio University Hospital, Kuopio, Pohjois-Savo, Finland.', 'Department of Neurosurgery, Tampere University Hospital, Tampere, Finland.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Mechanical Engineering and Material Science, University of Pittsburgh, Pittsburgh, Pennsylvania, USA.', 'Department of Bioengineering, George Mason University, Fairfax, Virginia, USA.']   \n",
       "\n",
       "                                                                                                                         keywords  \\\n",
       "1           ['Convolutional neural networks', 'Deep-learning', 'Dermoscopic images', 'Papulosquamous skin diseases', 'Psoriasis']   \n",
       "2   ['COVID-19', 'Computer-aided diagnosis (CAD)', 'Deep feature extraction', 'Large margin classifier', 'MERS', 'SARS', 'X-ray']   \n",
       "8                                             ['EEG', 'TMS', 'brain state', 'classification', 'excitability', 'machine learning']   \n",
       "9                                                                             ['Endometrial cancer', 'Radiomic machine learning']   \n",
       "10                              ['Biomarker', 'Clinical features', 'Glioblastoma multiforme', 'MRI features', 'Machine learning']   \n",
       "14                         ['Bone density', 'Machine learning', 'Multidetector computed tomography', 'Osteoporosis', 'Screening']   \n",
       "21        ['Catheterization', 'Chronic kidney disease', 'Dialysis', 'Machine learning', 'Posterior urethral valve', 'Transplant']   \n",
       "25                                                                                                                           <NA>   \n",
       "29                                                                                                                           <NA>   \n",
       "31                                                                         ['aneurysm', 'blood flow', 'hemorrhage', 'statistics']   \n",
       "\n",
       "   mesh_terms references_pmids  \\\n",
       "1        <NA>             <NA>   \n",
       "2        <NA>             <NA>   \n",
       "8        <NA>             <NA>   \n",
       "9        <NA>             <NA>   \n",
       "10       <NA>             <NA>   \n",
       "14       <NA>             <NA>   \n",
       "21       <NA>             <NA>   \n",
       "25       <NA>             <NA>   \n",
       "29       <NA>             <NA>   \n",
       "31       <NA>             <NA>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  feature  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                      A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8                                                                                                                                                                        Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Radiomic machine learning for pretreatment assessment of prognostic risk factors for endometrial cancer and its effects on radiologists' decisions of deep myometrial invasion. To evaluate radiomic machine learning (ML) classifiers based on multiparametric magnetic resonance images (MRI) in pretreatment assessment of endometrial cancer (EC) risk factors and to examine effects on radiologists' interpretation of deep myometrial invasion (dMI).   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     MRI-based machine learning for determining quantitative and qualitative characteristics affecting the survival of glioblastoma multiforme. Our current study aims to consider the image biomarkers extracted from the MRI images for exploring their effects on glioblastoma multiforme (GBM) patients' survival. Determining its biomarker helps better manage the disease and evaluate treatments. It has been proven that imaging features could be used as a biomarker. The purpose of this study is to investigate the features in MRI and clinical features as the biomarker association of survival of GBM.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Automated detection of the contrast phase in MDCT by an artificial neural network improves the accuracy of opportunistic bone mineral density measurements. To determine the accuracy of an artificial neural network (ANN) for fully automated detection of the presence and phase of iodinated contrast agent in routine abdominal multidetector computed tomography (MDCT) scans and evaluate the effect of contrast correction for osteoporosis screening.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Posterior Urethral Valves Outcomes Prediction (PUVOP): a machine learning tool to predict clinically relevant outcomes in boys with posterior urethral valves. Early kidney and anatomic features may be predictive of future progression and need for additional procedures in patients with posterior urethral valve (PUV). The objective of this study was to use machine learning (ML) to predict clinically relevant outcomes in these patients.   \n",
       "25                                                                                                   The ADEPT study: a comparative study of dentists' ability to detect enamel-only proximal caries in bitewing radiographs with and without the use of AssistDent artificial intelligence software. Introduction Reversal of enamel-only proximal caries by non-invasive treatments is important in preventive dentistry. However, detecting such caries using bitewing radiography is difficult and the subtle patterns are often missed by dental practitioners.Aims To investigate whether the ability of dentists to detect enamel-only proximal caries is enhanced by the use of AssistDent artificial intelligence (AI) software.Materials and methods In the ADEPT (AssistDent Enamel-only Proximal caries assessmenT) study, 23 dentists were randomly divided into a control arm, without AI assistance, and an experimental arm, in which AI assistance provided on-screen prompts indicating potential enamel-only proximal caries. All participants analysed a set of 24 bitewings in which an expert panel had previously identified 65 enamel-only carious lesions and 241 healthy proximal surfaces.Results The control group found 44.3% of the caries, whereas the experimental group found 75.8%. The experimental group incorrectly identified caries in 14.6% of the healthy surfaces compared to 3.7% in the control group. The increase in sensitivity of 71% and decrease in specificity of 11% are statistically significant (p <0.01).Conclusions AssistDent AI software significantly improves dentists' ability to detect enamel-only proximal caries and could be considered as a tool to support preventive dentistry in general practice.   \n",
       "29  Machine learning for the identification of decision boundaries during the transition from radial to vertical growth phase superficial spreading melanomas. To compute threshold values for the diameter of superficial spreading melanomas (SSMs) at which the radial growth phase (RGP) evolves into an invasive vertical growth phase (VGP). We examined reports from 1995 to 2019 of 834 primary SSMs. All the patients underwent complete surgical removal of the tumor and the diagnosis was confirmed after histologic examination. Machine learning was used to compute the thresholds. For invasive non-naevus-associated SSMs, a threshold for the diameter was found at 13.2 mm (n = 634). For the lower limb (n = 209) the threshold was at 9.8 mm, whereas for the upper limb (n = 117) at 14.1 mm. For the back (n = 106) and the trunk (n = 173), the threshold was at 16.2 mm and 17.1 mm, respectively. When considering non-naevus-associated and naevus-associated SSMs together (n = 834) a threshold for the diameter was found at 16.8 mm. For the lower limb (n = 248) the threshold was at 11.7 mm, whereas for the upper limb (n = 146) at 16.4 mm. For the back (n = 170) and the trunk (n = 236), the threshold was at 18.6 mm and 14.1 mm, respectively. Thresholds for various anatomic locations and for each gender were defined. They were based on the diameter of the melanoma and computed to suggest a transition from RGP to VGP. The transition from a radial to a more invasive vertical phase is detected by an increase of tumor size with a numeric cutoff. Besides the anamnestic, clinical and dermatoscopic findings, our proposed approach may have practical relevance in vivo during clinical presurgical inspections.   \n",
       "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Prediction of bleb formation in intracranial aneurysms using machine learning models based on aneurysm hemodynamics, geometry, location, and patient population. Bleb presence in intracranial aneurysms (IAs) is a known indication of instability and vulnerability.   \n",
       "\n",
       "   include mature algo_neural_net algo_support_vector algo_regression  \\\n",
       "1      1.0    1.0               1                   0               0   \n",
       "2      1.0    0.0               0                   0               0   \n",
       "8      1.0    0.0               0                   0               0   \n",
       "9      1.0    1.0               0                   0               0   \n",
       "10     1.0    0.0               0                   0               0   \n",
       "14     1.0    0.0               1                   0               0   \n",
       "21     1.0    0.0               0                   0               0   \n",
       "25     1.0    1.0               0                   0               0   \n",
       "29     1.0    0.0               0                   0               0   \n",
       "31     1.0    0.0               0                   0               0   \n",
       "\n",
       "   algo_decision_tree algo_discriminant algo_naive_bayes algo_transfer  \\\n",
       "1                   0                 0                0             0   \n",
       "2                   0                 0                0             1   \n",
       "8                   0                 0                0             0   \n",
       "9                   0                 0                0             0   \n",
       "10                  0                 0                0             0   \n",
       "14                  0                 0                0             0   \n",
       "21                  0                 0                0             0   \n",
       "25                  0                 0                0             0   \n",
       "29                  0                 0                0             0   \n",
       "31                  0                 0                0             0   \n",
       "\n",
       "   algo_federated algo_k_nearest algo_unsupervised feat_xr feat_ct feat_mri  \\\n",
       "1               0              0                 0       0       0        0   \n",
       "2               0              0                 0       1       0        0   \n",
       "8               0              0                 0       0       0        0   \n",
       "9               0              0                 0       0       0        1   \n",
       "10              0              0                 0       0       0        1   \n",
       "14              0              0                 0       0       1        0   \n",
       "21              0              0                 0       0       0        0   \n",
       "25              0              0                 0       1       0        0   \n",
       "29              0              0                 0       0       0        0   \n",
       "31              0              0                 0       0       0        0   \n",
       "\n",
       "   feat_eeg feat_ecg feat_emg feat_us feat_echo feat_histo feat_oct feat_mamm  \\\n",
       "1         0        0        0       0         0          0        0         0   \n",
       "2         0        0        0       0         0          0        0         0   \n",
       "8         1        0        0       0         0          0        0         0   \n",
       "9         0        0        0       0         0          0        0         0   \n",
       "10        0        0        0       0         0          0        0         0   \n",
       "14        0        0        0       0         0          0        0         0   \n",
       "21        0        0        0       0         0          0        0         0   \n",
       "25        0        0        0       0         0          0        0         0   \n",
       "29        0        0        0       0         0          1        0         0   \n",
       "31        0        0        0       0         0          0        0         0   \n",
       "\n",
       "   feat_endoscop feat_derm feat_gene feat_bio feat_nlp feat_ehr feat_sensor  \\\n",
       "1              0         1         0        0        0        0           0   \n",
       "2              0         0         0        0        0        0           0   \n",
       "8              0         0         0        0        0        0           0   \n",
       "9              0         0         0        0        0        0           0   \n",
       "10             0         0         0        1        0        0           0   \n",
       "14             0         0         0        0        0        0           0   \n",
       "21             0         0         0        0        0        0           0   \n",
       "25             0         0         0        0        0        0           0   \n",
       "29             0         0         0        0        0        0           0   \n",
       "31             0         0         0        0        0        0           0   \n",
       "\n",
       "   feat_phone feat_prom feat_sound subspec_icu subspec_ed spec_paeds  \\\n",
       "1           0         0          0           0          0          0   \n",
       "2           0         0          0           0          0          0   \n",
       "8           0         0          0           0          0          0   \n",
       "9           0         0          0           0          0          0   \n",
       "10          0         0          0           0          0          0   \n",
       "14          0         0          0           0          0          0   \n",
       "21          0         0          0           0          0          0   \n",
       "25          0         0          0           0          0          0   \n",
       "29          0         0          0           0          0          0   \n",
       "31          0         0          0           0          0          0   \n",
       "\n",
       "   spec_dent spec_audio spec_id subspec_sepsis subspec_hiv subspec_cov19  \\\n",
       "1          0          0       0              0           0             0   \n",
       "2          0          0       1              0           0             1   \n",
       "8          0          0       0              0           0             0   \n",
       "9          0          0       0              0           0             0   \n",
       "10         0          0       0              0           0             0   \n",
       "14         0          0       0              0           0             0   \n",
       "21         0          0       0              0           0             0   \n",
       "25         1          0       0              0           0             0   \n",
       "29         0          0       0              0           0             0   \n",
       "31         0          0       0              0           0             0   \n",
       "\n",
       "   subspec_tb subspec_malaria subspec_tropic spec_derm subspec_dermca  \\\n",
       "1           0               0              0         1              0   \n",
       "2           0               0              0         0              0   \n",
       "8           0               0              0         0              0   \n",
       "9           0               0              0         0              0   \n",
       "10          0               0              0         0              0   \n",
       "14          0               0              0         0              0   \n",
       "21          0               0              0         0              0   \n",
       "25          0               0              0         0              0   \n",
       "29          0               0              0         1              1   \n",
       "31          0               0              0         0              0   \n",
       "\n",
       "   spec_onc subspec_rx subspec_lungca subspec_brainca subspec_gica  \\\n",
       "1         0          0              0               0            0   \n",
       "2         0          0              0               0            0   \n",
       "8         0          0              0               0            0   \n",
       "9         0          0              0               0            0   \n",
       "10        1          0              0               1            0   \n",
       "14        0          0              0               0            0   \n",
       "21        0          0              0               0            0   \n",
       "25        0          0              0               0            0   \n",
       "29        1          0              0               0            0   \n",
       "31        0          0              0               0            0   \n",
       "\n",
       "   subspec_hepca subspec_prosca subspec_gynonc subspec_renalca  \\\n",
       "1              0              0              0               0   \n",
       "2              0              0              0               0   \n",
       "8              0              0              0               0   \n",
       "9              0              0              0               0   \n",
       "10             0              0              0               0   \n",
       "14             0              0              0               0   \n",
       "21             0              0              0               0   \n",
       "25             0              0              0               0   \n",
       "29             0              0              0               0   \n",
       "31             0              0              0               0   \n",
       "\n",
       "   subspec_haemonc subspec_breast subspec_breastca spec_psych subspec_suicide  \\\n",
       "1                0              0                0          1               0   \n",
       "2                0              0                0          0               0   \n",
       "8                0              0                0          0               0   \n",
       "9                0              0                0          0               0   \n",
       "10               0              0                0          0               0   \n",
       "14               0              0                0          0               0   \n",
       "21               0              0                0          0               0   \n",
       "25               0              0                0          0               0   \n",
       "29               0              0                0          0               0   \n",
       "31               0              0                0          0               0   \n",
       "\n",
       "   spec_msk subspec_frac spec_rheum spec_gi spec_hep spec_resp subspec_pneum  \\\n",
       "1         0            0          0       0        0         0             0   \n",
       "2         0            0          0       0        0         1             1   \n",
       "8         0            0          0       0        0         0             0   \n",
       "9         0            0          0       0        0         0             0   \n",
       "10        0            0          0       0        0         0             0   \n",
       "14        0            0          0       0        0         0             0   \n",
       "21        0            0          0       0        0         0             0   \n",
       "25        0            0          0       0        0         0             0   \n",
       "29        0            0          0       0        0         0             0   \n",
       "31        0            0          0       0        0         0             0   \n",
       "\n",
       "   subspec_osa subspec_pe spec_neuro subspec_epilep subspec_cva subspec_alzh  \\\n",
       "1            0          0          0              0           0            0   \n",
       "2            0          0          0              0           0            0   \n",
       "8            0          0          1              0           0            0   \n",
       "9            0          0          0              0           0            0   \n",
       "10           0          0          1              0           0            0   \n",
       "14           0          0          0              0           0            0   \n",
       "21           0          0          0              0           0            0   \n",
       "25           0          0          0              0           0            0   \n",
       "29           0          0          0              0           0            0   \n",
       "31           0          0          0              0           0            0   \n",
       "\n",
       "   spec_cvs subspec_ihd subspec_hf subspec_arrhyt spec_endo spec_dm  \\\n",
       "1         0           0          0              0         0       0   \n",
       "2         0           0          0              0         0       0   \n",
       "8         0           0          0              0         0       0   \n",
       "9         0           0          0              0         0       0   \n",
       "10        0           0          0              0         0       0   \n",
       "14        0           0          0              0         0       0   \n",
       "21        0           0          0              0         0       0   \n",
       "25        0           0          0              0         0       0   \n",
       "29        0           0          0              0         0       0   \n",
       "31        0           0          0              0         0       0   \n",
       "\n",
       "   subspec_insulin spec_eye subspec_retina spec_haem spec_obs spec_renal  \\\n",
       "1                0        0              0         0        0          0   \n",
       "2                0        0              0         0        0          0   \n",
       "8                0        0              0         0        0          0   \n",
       "9                0        0              0         0        0          0   \n",
       "10               0        0              0         0        0          0   \n",
       "14               0        0              0         0        0          0   \n",
       "21               0        0              0         0        0          1   \n",
       "25               0        0              0         0        0          0   \n",
       "29               0        0              0         0        0          0   \n",
       "31               0        0              0         0        0          0   \n",
       "\n",
       "   subspec_ackd spec_pubh subspec_bci subspec_prosth subspec_assist  \\\n",
       "1             0         0           0              0              0   \n",
       "2             0         0           0              0              0   \n",
       "8             0         0           0              0              0   \n",
       "9             0         0           0              0              0   \n",
       "10            0         0           0              0              0   \n",
       "14            0         0           0              0              0   \n",
       "21            0         0           0              0              0   \n",
       "25            0         0           0              0              0   \n",
       "29            0         0           0              0              0   \n",
       "31            0         0           0              0              0   \n",
       "\n",
       "   subspec_activity  \n",
       "1                 0  \n",
       "2                 0  \n",
       "8                 0  \n",
       "9                 0  \n",
       "10                0  \n",
       "14                0  \n",
       "21                0  \n",
       "25                0  \n",
       "29                0  \n",
       "31                0  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4deaf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_ner = all_tagged[['text', \n",
    "#                        'algo_neural_net', 'algo_support_vector', 'algo_regression', 'algo_decision_tree', \n",
    "#                       'algo_discriminant', 'algo_naive_bayes', 'algo_transfer', 'algo_federated', 'algo_k_nearest',\n",
    "#                       'algo_unsupervised',\n",
    "#                        'feat_imaging', 'feat_xr', 'feat_ct', 'feat_mri', 'feat_eeg', 'feat_ecg',\n",
    "#                       'feat_us', 'feat_echo', 'feat_histo', 'feat_oct', 'feat_mamm', 'feat_endoscop', 'feat_derm',\n",
    "#                       'feat_gene', 'feat_bio', 'feat_nlp', 'feat_ehr', 'feat_sensor', 'feat_phone', \n",
    "#                        'subspec_icu', 'subspec_ed', 'spec_id', 'subspec_sepsis', 'subspec_hiv', 'subspec_cov19', 'subspec_tb',\n",
    "#                       'subspec_malaria', 'spec_derm', 'subspec_dermca', 'spec_onc', 'subspec_rx', 'subspec_gynonc', \n",
    "#                       'subspec_lungca', 'subspec_brainca', 'subspec_gica', 'subspec_hepca', 'subspec_prosca',\n",
    "#                       'subspec_renalca', 'subspec_haemonc', 'subspec_breast', 'spec_psych','subspec_suicide', 'spec_msk', \n",
    "#                        'subspec_frac', 'spec_rheum', 'spec_gi', 'spec_hep', 'spec_resp', 'subspec_pneum',\n",
    "#                        'spec_neuro', 'subspec_epilep', 'subspec_cva', 'subspec_alzh', 'spec_cvs', 'subspec_ihd', 'subspec_hf', \n",
    "#                       'spec_endo', 'subspec_dm', 'spec_eye', 'subspec_retina', 'spec_haem', 'spec_obs', 'spec_renal', \n",
    "#                        'subspec_ackd', 'spec_paeds', 'spec_dent',  'spec_audio', 'spec_pubh', 'subspec_bci',\n",
    "#                       'subspec_prosth', 'subspec_assist','subspec_activity', 'subspec_arrhyt', 'countries', 'lmic_flag']].copy()\n",
    "#\n",
    "#final_ner.to_csv('output/final_ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a915ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled.to_csv('data/char_labelled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41263789",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "74d770bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_eval = labelled.drop(['doi', 'title', 'abstract', 'article_date', 'pubmed_date', 'article_type', 'lang', 'journal', 'journal_short',\n",
    "                         'journal_country', 'authors', 'author_affils', 'keywords', 'mesh_terms', 'references_pmids', 'include', 'mature'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f786f0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>feature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "      <th>feat_xr</th>\n",
       "      <th>feat_ct</th>\n",
       "      <th>feat_mri</th>\n",
       "      <th>feat_eeg</th>\n",
       "      <th>feat_ecg</th>\n",
       "      <th>feat_emg</th>\n",
       "      <th>feat_us</th>\n",
       "      <th>feat_echo</th>\n",
       "      <th>feat_histo</th>\n",
       "      <th>feat_oct</th>\n",
       "      <th>feat_mamm</th>\n",
       "      <th>feat_endoscop</th>\n",
       "      <th>feat_derm</th>\n",
       "      <th>feat_gene</th>\n",
       "      <th>feat_bio</th>\n",
       "      <th>feat_nlp</th>\n",
       "      <th>feat_ehr</th>\n",
       "      <th>feat_sensor</th>\n",
       "      <th>feat_phone</th>\n",
       "      <th>feat_prom</th>\n",
       "      <th>feat_sound</th>\n",
       "      <th>subspec_icu</th>\n",
       "      <th>subspec_ed</th>\n",
       "      <th>spec_paeds</th>\n",
       "      <th>spec_dent</th>\n",
       "      <th>spec_audio</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>subspec_sepsis</th>\n",
       "      <th>subspec_hiv</th>\n",
       "      <th>subspec_cov19</th>\n",
       "      <th>subspec_tb</th>\n",
       "      <th>subspec_malaria</th>\n",
       "      <th>subspec_tropic</th>\n",
       "      <th>spec_derm</th>\n",
       "      <th>subspec_dermca</th>\n",
       "      <th>spec_onc</th>\n",
       "      <th>subspec_rx</th>\n",
       "      <th>subspec_lungca</th>\n",
       "      <th>subspec_brainca</th>\n",
       "      <th>subspec_gica</th>\n",
       "      <th>subspec_hepca</th>\n",
       "      <th>subspec_prosca</th>\n",
       "      <th>subspec_gynonc</th>\n",
       "      <th>subspec_renalca</th>\n",
       "      <th>subspec_haemonc</th>\n",
       "      <th>subspec_breast</th>\n",
       "      <th>subspec_breastca</th>\n",
       "      <th>spec_psych</th>\n",
       "      <th>subspec_suicide</th>\n",
       "      <th>spec_msk</th>\n",
       "      <th>subspec_frac</th>\n",
       "      <th>spec_rheum</th>\n",
       "      <th>spec_gi</th>\n",
       "      <th>spec_hep</th>\n",
       "      <th>spec_resp</th>\n",
       "      <th>subspec_pneum</th>\n",
       "      <th>subspec_osa</th>\n",
       "      <th>subspec_pe</th>\n",
       "      <th>spec_neuro</th>\n",
       "      <th>subspec_epilep</th>\n",
       "      <th>subspec_cva</th>\n",
       "      <th>subspec_alzh</th>\n",
       "      <th>spec_cvs</th>\n",
       "      <th>subspec_ihd</th>\n",
       "      <th>subspec_hf</th>\n",
       "      <th>subspec_arrhyt</th>\n",
       "      <th>spec_endo</th>\n",
       "      <th>spec_dm</th>\n",
       "      <th>subspec_insulin</th>\n",
       "      <th>spec_eye</th>\n",
       "      <th>subspec_retina</th>\n",
       "      <th>spec_haem</th>\n",
       "      <th>spec_obs</th>\n",
       "      <th>spec_renal</th>\n",
       "      <th>subspec_ackd</th>\n",
       "      <th>spec_pubh</th>\n",
       "      <th>subspec_bci</th>\n",
       "      <th>subspec_prosth</th>\n",
       "      <th>subspec_assist</th>\n",
       "      <th>subspec_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34688173</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688172</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34687858</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid  \\\n",
       "1  34688173   \n",
       "2  34688172   \n",
       "8  34687858   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            feature  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8  Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "\n",
       "  algo_neural_net algo_support_vector algo_regression algo_decision_tree  \\\n",
       "1               1                   0               0                  0   \n",
       "2               0                   0               0                  0   \n",
       "8               0                   0               0                  0   \n",
       "\n",
       "  algo_discriminant algo_naive_bayes algo_transfer algo_federated  \\\n",
       "1                 0                0             0              0   \n",
       "2                 0                0             1              0   \n",
       "8                 0                0             0              0   \n",
       "\n",
       "  algo_k_nearest algo_unsupervised feat_xr feat_ct feat_mri feat_eeg feat_ecg  \\\n",
       "1              0                 0       0       0        0        0        0   \n",
       "2              0                 0       1       0        0        0        0   \n",
       "8              0                 0       0       0        0        1        0   \n",
       "\n",
       "  feat_emg feat_us feat_echo feat_histo feat_oct feat_mamm feat_endoscop  \\\n",
       "1        0       0         0          0        0         0             0   \n",
       "2        0       0         0          0        0         0             0   \n",
       "8        0       0         0          0        0         0             0   \n",
       "\n",
       "  feat_derm feat_gene feat_bio feat_nlp feat_ehr feat_sensor feat_phone  \\\n",
       "1         1         0        0        0        0           0          0   \n",
       "2         0         0        0        0        0           0          0   \n",
       "8         0         0        0        0        0           0          0   \n",
       "\n",
       "  feat_prom feat_sound subspec_icu subspec_ed spec_paeds spec_dent spec_audio  \\\n",
       "1         0          0           0          0          0         0          0   \n",
       "2         0          0           0          0          0         0          0   \n",
       "8         0          0           0          0          0         0          0   \n",
       "\n",
       "  spec_id subspec_sepsis subspec_hiv subspec_cov19 subspec_tb subspec_malaria  \\\n",
       "1       0              0           0             0          0               0   \n",
       "2       1              0           0             1          0               0   \n",
       "8       0              0           0             0          0               0   \n",
       "\n",
       "  subspec_tropic spec_derm subspec_dermca spec_onc subspec_rx subspec_lungca  \\\n",
       "1              0         1              0        0          0              0   \n",
       "2              0         0              0        0          0              0   \n",
       "8              0         0              0        0          0              0   \n",
       "\n",
       "  subspec_brainca subspec_gica subspec_hepca subspec_prosca subspec_gynonc  \\\n",
       "1               0            0             0              0              0   \n",
       "2               0            0             0              0              0   \n",
       "8               0            0             0              0              0   \n",
       "\n",
       "  subspec_renalca subspec_haemonc subspec_breast subspec_breastca spec_psych  \\\n",
       "1               0               0              0                0          1   \n",
       "2               0               0              0                0          0   \n",
       "8               0               0              0                0          0   \n",
       "\n",
       "  subspec_suicide spec_msk subspec_frac spec_rheum spec_gi spec_hep spec_resp  \\\n",
       "1               0        0            0          0       0        0         0   \n",
       "2               0        0            0          0       0        0         1   \n",
       "8               0        0            0          0       0        0         0   \n",
       "\n",
       "  subspec_pneum subspec_osa subspec_pe spec_neuro subspec_epilep subspec_cva  \\\n",
       "1             0           0          0          0              0           0   \n",
       "2             1           0          0          0              0           0   \n",
       "8             0           0          0          1              0           0   \n",
       "\n",
       "  subspec_alzh spec_cvs subspec_ihd subspec_hf subspec_arrhyt spec_endo  \\\n",
       "1            0        0           0          0              0         0   \n",
       "2            0        0           0          0              0         0   \n",
       "8            0        0           0          0              0         0   \n",
       "\n",
       "  spec_dm subspec_insulin spec_eye subspec_retina spec_haem spec_obs  \\\n",
       "1       0               0        0              0         0        0   \n",
       "2       0               0        0              0         0        0   \n",
       "8       0               0        0              0         0        0   \n",
       "\n",
       "  spec_renal subspec_ackd spec_pubh subspec_bci subspec_prosth subspec_assist  \\\n",
       "1          0            0         0           0              0              0   \n",
       "2          0            0         0           0              0              0   \n",
       "8          0            0         0           0              0              0   \n",
       "\n",
       "  subspec_activity  \n",
       "1                0  \n",
       "2                0  \n",
       "8                0  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_eval.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9576fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdata = ner_eval.apply(lambda s: [s.name if v == \"1\" else np.nan for v in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b26572aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>feature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "      <th>feat_xr</th>\n",
       "      <th>feat_ct</th>\n",
       "      <th>feat_mri</th>\n",
       "      <th>feat_eeg</th>\n",
       "      <th>feat_ecg</th>\n",
       "      <th>feat_emg</th>\n",
       "      <th>feat_us</th>\n",
       "      <th>feat_echo</th>\n",
       "      <th>feat_histo</th>\n",
       "      <th>feat_oct</th>\n",
       "      <th>feat_mamm</th>\n",
       "      <th>feat_endoscop</th>\n",
       "      <th>feat_derm</th>\n",
       "      <th>feat_gene</th>\n",
       "      <th>feat_bio</th>\n",
       "      <th>feat_nlp</th>\n",
       "      <th>feat_ehr</th>\n",
       "      <th>feat_sensor</th>\n",
       "      <th>feat_phone</th>\n",
       "      <th>feat_prom</th>\n",
       "      <th>feat_sound</th>\n",
       "      <th>subspec_icu</th>\n",
       "      <th>subspec_ed</th>\n",
       "      <th>spec_paeds</th>\n",
       "      <th>spec_dent</th>\n",
       "      <th>spec_audio</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>subspec_sepsis</th>\n",
       "      <th>subspec_hiv</th>\n",
       "      <th>subspec_cov19</th>\n",
       "      <th>subspec_tb</th>\n",
       "      <th>subspec_malaria</th>\n",
       "      <th>subspec_tropic</th>\n",
       "      <th>spec_derm</th>\n",
       "      <th>subspec_dermca</th>\n",
       "      <th>spec_onc</th>\n",
       "      <th>subspec_rx</th>\n",
       "      <th>subspec_lungca</th>\n",
       "      <th>subspec_brainca</th>\n",
       "      <th>subspec_gica</th>\n",
       "      <th>subspec_hepca</th>\n",
       "      <th>subspec_prosca</th>\n",
       "      <th>subspec_gynonc</th>\n",
       "      <th>subspec_renalca</th>\n",
       "      <th>subspec_haemonc</th>\n",
       "      <th>subspec_breast</th>\n",
       "      <th>subspec_breastca</th>\n",
       "      <th>spec_psych</th>\n",
       "      <th>subspec_suicide</th>\n",
       "      <th>spec_msk</th>\n",
       "      <th>subspec_frac</th>\n",
       "      <th>spec_rheum</th>\n",
       "      <th>spec_gi</th>\n",
       "      <th>spec_hep</th>\n",
       "      <th>spec_resp</th>\n",
       "      <th>subspec_pneum</th>\n",
       "      <th>subspec_osa</th>\n",
       "      <th>subspec_pe</th>\n",
       "      <th>spec_neuro</th>\n",
       "      <th>subspec_epilep</th>\n",
       "      <th>subspec_cva</th>\n",
       "      <th>subspec_alzh</th>\n",
       "      <th>spec_cvs</th>\n",
       "      <th>subspec_ihd</th>\n",
       "      <th>subspec_hf</th>\n",
       "      <th>subspec_arrhyt</th>\n",
       "      <th>spec_endo</th>\n",
       "      <th>spec_dm</th>\n",
       "      <th>subspec_insulin</th>\n",
       "      <th>spec_eye</th>\n",
       "      <th>subspec_retina</th>\n",
       "      <th>spec_haem</th>\n",
       "      <th>spec_obs</th>\n",
       "      <th>spec_renal</th>\n",
       "      <th>subspec_ackd</th>\n",
       "      <th>spec_pubh</th>\n",
       "      <th>subspec_bci</th>\n",
       "      <th>subspec_prosth</th>\n",
       "      <th>subspec_assist</th>\n",
       "      <th>subspec_activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algo_neural_net</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feat_derm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_derm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_psych</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algo_transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feat_xr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_id</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>subspec_cov19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_resp</td>\n",
       "      <td>subspec_pneum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feat_eeg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spec_neuro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmid  feature  algo_neural_net algo_support_vector algo_regression  \\\n",
       "1   NaN      NaN  algo_neural_net                 NaN             NaN   \n",
       "2   NaN      NaN              NaN                 NaN             NaN   \n",
       "8   NaN      NaN              NaN                 NaN             NaN   \n",
       "\n",
       "  algo_decision_tree algo_discriminant algo_naive_bayes  algo_transfer  \\\n",
       "1                NaN               NaN              NaN            NaN   \n",
       "2                NaN               NaN              NaN  algo_transfer   \n",
       "8                NaN               NaN              NaN            NaN   \n",
       "\n",
       "  algo_federated algo_k_nearest algo_unsupervised  feat_xr feat_ct feat_mri  \\\n",
       "1            NaN            NaN               NaN      NaN     NaN      NaN   \n",
       "2            NaN            NaN               NaN  feat_xr     NaN      NaN   \n",
       "8            NaN            NaN               NaN      NaN     NaN      NaN   \n",
       "\n",
       "   feat_eeg feat_ecg feat_emg feat_us feat_echo feat_histo feat_oct feat_mamm  \\\n",
       "1       NaN      NaN      NaN     NaN       NaN        NaN      NaN       NaN   \n",
       "2       NaN      NaN      NaN     NaN       NaN        NaN      NaN       NaN   \n",
       "8  feat_eeg      NaN      NaN     NaN       NaN        NaN      NaN       NaN   \n",
       "\n",
       "  feat_endoscop  feat_derm feat_gene feat_bio feat_nlp feat_ehr feat_sensor  \\\n",
       "1           NaN  feat_derm       NaN      NaN      NaN      NaN         NaN   \n",
       "2           NaN        NaN       NaN      NaN      NaN      NaN         NaN   \n",
       "8           NaN        NaN       NaN      NaN      NaN      NaN         NaN   \n",
       "\n",
       "  feat_phone feat_prom feat_sound subspec_icu subspec_ed spec_paeds spec_dent  \\\n",
       "1        NaN       NaN        NaN         NaN        NaN        NaN       NaN   \n",
       "2        NaN       NaN        NaN         NaN        NaN        NaN       NaN   \n",
       "8        NaN       NaN        NaN         NaN        NaN        NaN       NaN   \n",
       "\n",
       "  spec_audio  spec_id subspec_sepsis subspec_hiv  subspec_cov19 subspec_tb  \\\n",
       "1        NaN      NaN            NaN         NaN            NaN        NaN   \n",
       "2        NaN  spec_id            NaN         NaN  subspec_cov19        NaN   \n",
       "8        NaN      NaN            NaN         NaN            NaN        NaN   \n",
       "\n",
       "  subspec_malaria subspec_tropic  spec_derm subspec_dermca spec_onc  \\\n",
       "1             NaN            NaN  spec_derm            NaN      NaN   \n",
       "2             NaN            NaN        NaN            NaN      NaN   \n",
       "8             NaN            NaN        NaN            NaN      NaN   \n",
       "\n",
       "  subspec_rx subspec_lungca subspec_brainca subspec_gica subspec_hepca  \\\n",
       "1        NaN            NaN             NaN          NaN           NaN   \n",
       "2        NaN            NaN             NaN          NaN           NaN   \n",
       "8        NaN            NaN             NaN          NaN           NaN   \n",
       "\n",
       "  subspec_prosca subspec_gynonc subspec_renalca subspec_haemonc  \\\n",
       "1            NaN            NaN             NaN             NaN   \n",
       "2            NaN            NaN             NaN             NaN   \n",
       "8            NaN            NaN             NaN             NaN   \n",
       "\n",
       "  subspec_breast subspec_breastca  spec_psych subspec_suicide spec_msk  \\\n",
       "1            NaN              NaN  spec_psych             NaN      NaN   \n",
       "2            NaN              NaN         NaN             NaN      NaN   \n",
       "8            NaN              NaN         NaN             NaN      NaN   \n",
       "\n",
       "  subspec_frac spec_rheum spec_gi spec_hep  spec_resp  subspec_pneum  \\\n",
       "1          NaN        NaN     NaN      NaN        NaN            NaN   \n",
       "2          NaN        NaN     NaN      NaN  spec_resp  subspec_pneum   \n",
       "8          NaN        NaN     NaN      NaN        NaN            NaN   \n",
       "\n",
       "  subspec_osa subspec_pe  spec_neuro subspec_epilep subspec_cva subspec_alzh  \\\n",
       "1         NaN        NaN         NaN            NaN         NaN          NaN   \n",
       "2         NaN        NaN         NaN            NaN         NaN          NaN   \n",
       "8         NaN        NaN  spec_neuro            NaN         NaN          NaN   \n",
       "\n",
       "  spec_cvs subspec_ihd subspec_hf subspec_arrhyt spec_endo spec_dm  \\\n",
       "1      NaN         NaN        NaN            NaN       NaN     NaN   \n",
       "2      NaN         NaN        NaN            NaN       NaN     NaN   \n",
       "8      NaN         NaN        NaN            NaN       NaN     NaN   \n",
       "\n",
       "  subspec_insulin spec_eye subspec_retina spec_haem spec_obs spec_renal  \\\n",
       "1             NaN      NaN            NaN       NaN      NaN        NaN   \n",
       "2             NaN      NaN            NaN       NaN      NaN        NaN   \n",
       "8             NaN      NaN            NaN       NaN      NaN        NaN   \n",
       "\n",
       "  subspec_ackd spec_pubh subspec_bci subspec_prosth subspec_assist  \\\n",
       "1          NaN       NaN         NaN            NaN            NaN   \n",
       "2          NaN       NaN         NaN            NaN            NaN   \n",
       "8          NaN       NaN         NaN            NaN            NaN   \n",
       "\n",
       "  subspec_activity  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "8              NaN  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerdata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ec367934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_eval['result'] = nerdata.apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0129058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>feature</th>\n",
       "      <th>algo_neural_net</th>\n",
       "      <th>algo_support_vector</th>\n",
       "      <th>algo_regression</th>\n",
       "      <th>algo_decision_tree</th>\n",
       "      <th>algo_discriminant</th>\n",
       "      <th>algo_naive_bayes</th>\n",
       "      <th>algo_transfer</th>\n",
       "      <th>algo_federated</th>\n",
       "      <th>algo_k_nearest</th>\n",
       "      <th>algo_unsupervised</th>\n",
       "      <th>feat_xr</th>\n",
       "      <th>feat_ct</th>\n",
       "      <th>feat_mri</th>\n",
       "      <th>feat_eeg</th>\n",
       "      <th>feat_ecg</th>\n",
       "      <th>feat_emg</th>\n",
       "      <th>feat_us</th>\n",
       "      <th>feat_echo</th>\n",
       "      <th>feat_histo</th>\n",
       "      <th>feat_oct</th>\n",
       "      <th>feat_mamm</th>\n",
       "      <th>feat_endoscop</th>\n",
       "      <th>feat_derm</th>\n",
       "      <th>feat_gene</th>\n",
       "      <th>feat_bio</th>\n",
       "      <th>feat_nlp</th>\n",
       "      <th>feat_ehr</th>\n",
       "      <th>feat_sensor</th>\n",
       "      <th>feat_phone</th>\n",
       "      <th>feat_prom</th>\n",
       "      <th>feat_sound</th>\n",
       "      <th>subspec_icu</th>\n",
       "      <th>subspec_ed</th>\n",
       "      <th>spec_paeds</th>\n",
       "      <th>spec_dent</th>\n",
       "      <th>spec_audio</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>subspec_sepsis</th>\n",
       "      <th>subspec_hiv</th>\n",
       "      <th>subspec_cov19</th>\n",
       "      <th>subspec_tb</th>\n",
       "      <th>subspec_malaria</th>\n",
       "      <th>subspec_tropic</th>\n",
       "      <th>spec_derm</th>\n",
       "      <th>subspec_dermca</th>\n",
       "      <th>spec_onc</th>\n",
       "      <th>subspec_rx</th>\n",
       "      <th>subspec_lungca</th>\n",
       "      <th>subspec_brainca</th>\n",
       "      <th>subspec_gica</th>\n",
       "      <th>subspec_hepca</th>\n",
       "      <th>subspec_prosca</th>\n",
       "      <th>subspec_gynonc</th>\n",
       "      <th>subspec_renalca</th>\n",
       "      <th>subspec_haemonc</th>\n",
       "      <th>subspec_breast</th>\n",
       "      <th>subspec_breastca</th>\n",
       "      <th>spec_psych</th>\n",
       "      <th>subspec_suicide</th>\n",
       "      <th>spec_msk</th>\n",
       "      <th>subspec_frac</th>\n",
       "      <th>spec_rheum</th>\n",
       "      <th>spec_gi</th>\n",
       "      <th>spec_hep</th>\n",
       "      <th>spec_resp</th>\n",
       "      <th>subspec_pneum</th>\n",
       "      <th>subspec_osa</th>\n",
       "      <th>subspec_pe</th>\n",
       "      <th>spec_neuro</th>\n",
       "      <th>subspec_epilep</th>\n",
       "      <th>subspec_cva</th>\n",
       "      <th>subspec_alzh</th>\n",
       "      <th>spec_cvs</th>\n",
       "      <th>subspec_ihd</th>\n",
       "      <th>subspec_hf</th>\n",
       "      <th>subspec_arrhyt</th>\n",
       "      <th>spec_endo</th>\n",
       "      <th>spec_dm</th>\n",
       "      <th>subspec_insulin</th>\n",
       "      <th>spec_eye</th>\n",
       "      <th>subspec_retina</th>\n",
       "      <th>spec_haem</th>\n",
       "      <th>spec_obs</th>\n",
       "      <th>spec_renal</th>\n",
       "      <th>subspec_ackd</th>\n",
       "      <th>spec_pubh</th>\n",
       "      <th>subspec_bci</th>\n",
       "      <th>subspec_prosth</th>\n",
       "      <th>subspec_assist</th>\n",
       "      <th>subspec_activity</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34688173</td>\n",
       "      <td>A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>algo_neural_net,feat_derm,spec_derm,spec_psych,algo_neural_net,feat_derm,spec_derm,spec_psych</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34688172</td>\n",
       "      <td>A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>algo_transfer,feat_xr,spec_id,subspec_cov19,spec_resp,subspec_pneum,algo_transfer,feat_xr,spec_id,subspec_cov19,spec_resp,subspec_pneum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34687858</td>\n",
       "      <td>Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>feat_eeg,spec_neuro,feat_eeg,spec_neuro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid  \\\n",
       "1  34688173   \n",
       "2  34688172   \n",
       "8  34687858   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            feature  \\\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists. Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis.   \n",
       "2                                                                                                                                                                                                                                                                                                A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19. The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.   \n",
       "8  Causal Decoding of Individual Cortical Excitability States. Brain responsiveness to stimulation fluctuates with rapidly shifting cortical excitability state, as reflected by oscillations in the electroencephalogram (EEG). For example, the amplitude of motor-evoked potentials (MEPs) elicited by transcranial magnetic stimulation (TMS) of motor cortex changes from trial to trial. To date, individual estimation of the cortical processes leading to this excitability fluctuation has not been possible. Here, we propose a data-driven method to derive individually optimized EEG classifiers in healthy humans using a supervised learning approach that relates pre-TMS EEG activity dynamics to MEP amplitude. Our approach enables considering multiple brain regions and frequency bands, without defining them a priori, whose compound phase-pattern information determines the excitability. The individualized classifier leads to an increased classification accuracy of cortical excitability states from 57% to 67% when compared to μ-oscillation phase extracted by standard fixed spatial filters. Results show that, for the used TMS protocol, excitability fluctuates predominantly in the μ-oscillation range, and relevant cortical areas cluster around the stimulated motor cortex, but between subjects there is variability in relevant power spectra, phases, and cortical regions. This novel decoding method allows causal investigation of the cortical excitability state, which is critical also for individualizing therapeutic brain stimulation.   \n",
       "\n",
       "  algo_neural_net algo_support_vector algo_regression algo_decision_tree  \\\n",
       "1               1                   0               0                  0   \n",
       "2               0                   0               0                  0   \n",
       "8               0                   0               0                  0   \n",
       "\n",
       "  algo_discriminant algo_naive_bayes algo_transfer algo_federated  \\\n",
       "1                 0                0             0              0   \n",
       "2                 0                0             1              0   \n",
       "8                 0                0             0              0   \n",
       "\n",
       "  algo_k_nearest algo_unsupervised feat_xr feat_ct feat_mri feat_eeg feat_ecg  \\\n",
       "1              0                 0       0       0        0        0        0   \n",
       "2              0                 0       1       0        0        0        0   \n",
       "8              0                 0       0       0        0        1        0   \n",
       "\n",
       "  feat_emg feat_us feat_echo feat_histo feat_oct feat_mamm feat_endoscop  \\\n",
       "1        0       0         0          0        0         0             0   \n",
       "2        0       0         0          0        0         0             0   \n",
       "8        0       0         0          0        0         0             0   \n",
       "\n",
       "  feat_derm feat_gene feat_bio feat_nlp feat_ehr feat_sensor feat_phone  \\\n",
       "1         1         0        0        0        0           0          0   \n",
       "2         0         0        0        0        0           0          0   \n",
       "8         0         0        0        0        0           0          0   \n",
       "\n",
       "  feat_prom feat_sound subspec_icu subspec_ed spec_paeds spec_dent spec_audio  \\\n",
       "1         0          0           0          0          0         0          0   \n",
       "2         0          0           0          0          0         0          0   \n",
       "8         0          0           0          0          0         0          0   \n",
       "\n",
       "  spec_id subspec_sepsis subspec_hiv subspec_cov19 subspec_tb subspec_malaria  \\\n",
       "1       0              0           0             0          0               0   \n",
       "2       1              0           0             1          0               0   \n",
       "8       0              0           0             0          0               0   \n",
       "\n",
       "  subspec_tropic spec_derm subspec_dermca spec_onc subspec_rx subspec_lungca  \\\n",
       "1              0         1              0        0          0              0   \n",
       "2              0         0              0        0          0              0   \n",
       "8              0         0              0        0          0              0   \n",
       "\n",
       "  subspec_brainca subspec_gica subspec_hepca subspec_prosca subspec_gynonc  \\\n",
       "1               0            0             0              0              0   \n",
       "2               0            0             0              0              0   \n",
       "8               0            0             0              0              0   \n",
       "\n",
       "  subspec_renalca subspec_haemonc subspec_breast subspec_breastca spec_psych  \\\n",
       "1               0               0              0                0          1   \n",
       "2               0               0              0                0          0   \n",
       "8               0               0              0                0          0   \n",
       "\n",
       "  subspec_suicide spec_msk subspec_frac spec_rheum spec_gi spec_hep spec_resp  \\\n",
       "1               0        0            0          0       0        0         0   \n",
       "2               0        0            0          0       0        0         1   \n",
       "8               0        0            0          0       0        0         0   \n",
       "\n",
       "  subspec_pneum subspec_osa subspec_pe spec_neuro subspec_epilep subspec_cva  \\\n",
       "1             0           0          0          0              0           0   \n",
       "2             1           0          0          0              0           0   \n",
       "8             0           0          0          1              0           0   \n",
       "\n",
       "  subspec_alzh spec_cvs subspec_ihd subspec_hf subspec_arrhyt spec_endo  \\\n",
       "1            0        0           0          0              0         0   \n",
       "2            0        0           0          0              0         0   \n",
       "8            0        0           0          0              0         0   \n",
       "\n",
       "  spec_dm subspec_insulin spec_eye subspec_retina spec_haem spec_obs  \\\n",
       "1       0               0        0              0         0        0   \n",
       "2       0               0        0              0         0        0   \n",
       "8       0               0        0              0         0        0   \n",
       "\n",
       "  spec_renal subspec_ackd spec_pubh subspec_bci subspec_prosth subspec_assist  \\\n",
       "1          0            0         0           0              0              0   \n",
       "2          0            0         0           0              0              0   \n",
       "8          0            0         0           0              0              0   \n",
       "\n",
       "  subspec_activity  \\\n",
       "1                0   \n",
       "2                0   \n",
       "8                0   \n",
       "\n",
       "                                                                                                                                    result  \n",
       "1                                            algo_neural_net,feat_derm,spec_derm,spec_psych,algo_neural_net,feat_derm,spec_derm,spec_psych  \n",
       "2  algo_transfer,feat_xr,spec_id,subspec_cov19,spec_resp,subspec_pneum,algo_transfer,feat_xr,spec_id,subspec_cov19,spec_resp,subspec_pneum  \n",
       "8                                                                                                  feat_eeg,spec_neuro,feat_eeg,spec_neuro  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_eval.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fd418773",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_eval.to_csv('data/char_labelled_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79062737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
